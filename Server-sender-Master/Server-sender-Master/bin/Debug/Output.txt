samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

5.2.2. Data Fusion

In general, the classification of fused data can yield better results than the classification over
single data sources [59]. A promising direction of investigation is given by the nexus between energy
consumption, water consumption, and human presence in a house (also gas metering could be an
additional data source). In an extreme example, a 50% classification between laundry and gardening
could be better disambiguated by the analysis of instant energy consumption given that one of the
two activities uses energy and water at the same time. An example of the nexus between energy and
water is presented in Reference {60} In that paper, the authors leverage electricity non-intrusive load
monitoring (NILM) to acquire water disaggregation as a set of water/ energy correlated states.

5.2.3. Working at Scale

Applying standard rates derived from sample studies is misleading because of the high variability
in water use from one customer to another, even among customers with a similar infrastructure and
social-economic pro?le. The model extracted from a single house?s data is limited, and does not
leverage the information hidden in the broader population. What we consider parameters for a single
house (pipe size, extension of parcel, number of rooms, habits of tenants, ?le.) could be considered
as independent variables in a broader model, comprising a full set of properties in a city or region.
The collection of massive datasets for an entire city or region is, nowadays, technically feasible and
possible to maintain in the long ternL Hence, an interesting research question is to build and evaluate
large-scale models.

6. Conclusions

Non-intrusive water disaggregation is a valuable approach for estimating fixture-specific water
consumption, while keeping installation costs affordable, and, at the same time, the underlying
complexity of processing remains manageable. We have presented a review of water disaggregation
methods that make use of either mono?modal sensing or multi?modal sensing (e.g., combining different
variables, such as water flow, pressure, etc). The result of our review can be summarized in the
following conclusions:

5. Discussion on Issues and Future Challenges

Water usage disaggregation is the equivalent of non-intrusive electricity load monitoring, applied
in the water domain, but with an important difference: While electricity outlets can be monitored with
non-invasive, out-of-the-box meters, water fixtures are, in general, unpowered and more difficult to
wire to a data communication infrastructure. This entails battery-operated instrumentation and, in
turn, constrained communication capabilities. Moreover, when dealing with supervised classifiers, a
necessary step is fitting the model with labelled data In the case of water, this may require special
purpose sensors, plumbing, and battery-operated equipment to be installed. Unfortunately, in real
houses, it is not viable to install a flow switch in every fixture or a Closed Circuit Television (CCTV)
camera in every room just to fit the classification model because plumbing is expensive and invasive.
Any viable approach should then comply with the principle of minimal installation requirements, and,
further, any sensors or equipment installed should already be an off-the-shelf product with a high
degree of acceptance among the general public. In summary, we can identify three requirements for
instrumenting a house with sensors:

. High acceptance (design, shape, part of shopping trends, identification of a user need)
. Low cost to buy and install
. Minimal or zero maintenance

All the works described in this survey challenges the previous state-of?the-art against classification
accuracy and are thus built on some hi-tech lab-level setup that requires continuous manual
intervention to ensure a reliable collection and processing of water data and ground truth.
To summarize:

Flow traces analysis [34] requires a data logger to be installed and then data should be manually
collected every 14 days, the data collected are then manually analysed and added to a database. It
seems a feasible solution to analyse a given period of time, but is not practical to perform online
disaggregation. HydroSense [38] reaches an accuracy of 8 ?u, but needs at least two days of ground
truth collectiono Their current approach trains the language model using data from the home where it

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

unnecessary amount of radio pollution, which is, in turn, is blamed as a potential cancer cause. The
second one is with respect to violations of citizens? rights. Detractors accuse governments of being
driven by the interest of suppliers and that the smart metering roadmap has been laid out without
public consultation, in violation of the spirit of shared consensus and democracy [54]. The last concern,
and probably the one with a proven impact, is that of privacy. There are many examples of how
high-resolution metering could be used to identify personal habits and retrieve personal information.
Notable proof of this concept is shown in Reference [55], where TV programs actually watched by
home occupants is inferred by correlating features such as the luminosity of scenes to high-resolution
energy consumption data. An approach to increase acceptance of industrial-level smart and cognitive
meters a viable solution is twofold:

- Give control to end users (they must be able to switch on/ off the metering; to set up the resolution;
to control the amount of radio messaging inside the property, etc.)

. Locally process most of the data and locally reveal the insights needed by end users to monitor
and improve their water demand. Powerful insight, such as usage disaggregation, could occur
in-home rather than being inferred remotely. This allows to send, to the supplier, only the
strictly-necessary data for operation (for instance daily average consumption over a week).

However, the above-mentioned approach does not take into full account the detailed needs of
water suppliers, as the focus is mainly on user privacy. Thus, as explained in Reference [56], developing
a context-specific framework for assessing how the collection and processing of detailed water usage
impacts the users privacy, and identifying a set of best practices to mitigate the impact is of paramount
importance. We expect that this issue will be addressed as soon as smart metering and cognitive
metering become ubiquitously available for the adaptive management of urban water resources.

52, A Few Promising Research Directions towards Real World Adoption

In this section, some promising directions of further investigation are described. The general
rationale is not to encourage competition in classification techniques to achieve 100% accuracy, but
rather to bridge the gaps for water disaggregation to become a viable tool in real world environments.

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

OPPORTUNITY dataset

The OPPORTUNITY dataset contains data from four subjects, performing six di??erent
runs each of: ADLlfADLS and Drill. In the Drill run, subject must act in a predeterr
mined activity sequence and, as for ADLlfADLS, there is no restriction on the order
and number of activities, For each subject, there is information from three types of
sensors: bodyrworn sensors, object sensors and ambient sensors The onebody sensors
include 7 multiesensor inertial measurement units with another 12 3D acceleration sen?
sors: 145 signals in total, Since only body?worn sensors are concerned in the evaluation
section of the original paper [31], the data from object and ambient sensors are trun?
cated in the following experiments. In terms of activities or classes, this damsel has 3
ditterent sets: 4 types of locomotion (highrlevel activities); 17 types of gesture (midelevel
actions); and lowelevel actions to objects (which is ignored in this work),

Experiments on the OPPORTUNITY dataset

We retrain and evaluate our system using the same experimental setting as in the origi?
nal paper [31]: using ADLz and ADL3 from one subject as the testing set and use Drill,
ADLI, ADL4 and ADLs from the same subject as the training set. We conduct experi?
ments in this con?guration for all four subjects and in the two tasks: highelevel locos
motion (Table 7) and midelevel gestures (Table 8). The ?rst column shows the different
proposed systems, and the best systems are remarked with bold font.

For me highelevel locomotion task (Table 7), ?le system proposed in this paper obtains
?ue best results for all subjects when the Null class is not considered (the 4 last columns).
When including the Null class (the 4 ?rst columns), we obtain the best results for all
subjects except 83.

For the mid?level gesture task (Table 3), the system proposed in this paper obtains
the best results for all subiects except 54 when the Null class is included (the 4 ?rst

Table 7 Experimental results on the OPPORTUNITY dataset (hlgh-level locomntlnn classl-
?cauon)

itallrvaluer imitate the best remit; in this expevlment

columns), ln conclusion, the proposed sysmm is also a competitive solution for home
care monitoring applications,

Conclusions
This paper has proposed a HAR system for classifying 33 di?'erent physical activities
composed of two main modules: feature extraction and activity recognition modules.

The first contribution has been an analysis of several feature extraction strategies:
timerbased and frequencysbased. The timerbased features have provided betmr results
compared to the frequencyrbased ones, This paper has also evaluated several normalis
zation methods for reducing the degradation produced when training and testing with
different users. Thanks to the new feature extraction module and the normalization
strategy, the system has shown strong robustness when facing me Nullractivity and dif
ferent placement scenarios, two vital aspects for real applications.

Regarding the type ofsensor, the magnetometer signals have provided better discrimir
nation capability, The best results have been obtained when combining the information
from all the sensors. ln this case, the improvement is significant. The main experiments
have been done on a public available dataset, REALDISP Activity Recognition dataset.
Final results have exhibited that the proposed system largely improves me performance
compared to previous works on the same damset [24]. Under the best configuration, the
accuracy reaches 99.196 and Frmeasule 0.991.

The proposed system luas been also evaluated with another public dataset (OPPORr
TUNITY dataset) demonstrating competitive results (compared to previous work [31])
in two main tasks for home care monitoring: highrlevel locomotion and midrlevel gess
ture classi?catlon,

 

for training the system is very small (2 out of the 3 subjects recorded in this scenario).
In order to analyze the in?uence of the amount of data, we repeat the same experiments
but using the ideal?p|acement data for training the system. Although there is a mismatch
in the conditions, the amount of available data for training would increase a lot (from 2
to 16 subjects). The experiments are shown in Table 6. In the ?Train Set" and ?Test Set"
columns, we have also included the number of subjects considered for training and mstr
ing the system.

The results show that when being tmined with ideal datasets and tested with mutual
damsets, the system reaches a very good accuracy though the training and testing sets
come from diti'erent placement scenarios. For example, for mutual4, the accuracy goes
from 87.9 to 99.096 (the ?rst row). These results support the hypothesis that the amount
of data for training is an important factor in the system performance.

with the idea of cross?dataset experiment, we go further on me idealrplacement and
sel?placement scenarios (the last row in Table 6). As Table 6 shows, there is not a signi??
cant di?'erence on the accuracy when testing with sel?placement dataset and training
with ideal or self placement (99.1 vs. 98,936, di?'erence lower than the con?dence interval
0.596). In this case, the amount of data available in ideal?placement and sel?placement
scenarios is me same.

System analysis in a new domain: home care monitoring

In the introduction, we commented two main applications of HAR: physical exercise
monitoring and home care monitoring. The REALDISP dataset is focused on the ?rst
one: physical exercise monitoring, In order to verify the viability of the proposed system
in a home care monitoring application, we have evaluated the best system con?guration
with another dataset: the OPPORTUNITY dataset for HAR from wearable, object, and
ambient sensors [30]. The recordings include daily morning activities: getting up from
the bed, preparing and having breakfast (a coffee and a salami sandwich) and clean?
ing the kitchen latter. This dataset is a very popular l-[AR dataset on thls research field.
There is no constraining on the location or body posture in any ofthe scripted activities.

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

number of sensors and devices used in these studies were significantly high, our proposal is focused
on answering the question whether it is possible to obtain similar or better results regarding occupancy
detection using fewer resources. Even more, our proposal uses a non~intrusive ground truth strategy
to avoid jeopardising the privacy and security of the occupants in the area of interest.

This research uses as inspiration some of the ideas proposed in the works discussed, in particular,
the gathering of data from different sources to move forward to a complete analysis of the data collected
using a ML approach. Our goal is to detect occupancy in indoor environments by preprocessing the
datasets collected betore applying ML to a binary and mul?eclass problem. Additionally we design our
solution focused on two main requirements: the first one is to try to take advantage of cheap/affordable
devices commonly deployed in Smart Environment, and the second one is to guarantee that the privacy
of the occupants in the area under study would not be compromised, Thus, after the discussion of
the works in this research area, our proposal uses environmental features via the combination of data
gathered from different sensors. This solution is presented in the following section.

3, occupancy Detection in Indoor Environments

As discussed in the previous section, occupancy detection could be used to trigger some actuation
mechanisms in Smart Environments in order to improve resource usage and user experience, among
other factors, An important issue that must be considered is the preservation of privacy of the data
collected and analysed. Additionally, it would be desirable to take advantage of the infrastructure
available in the surroundings to avoid incurring in extra expenses, while allowing the scalability of the
solution. Considering these factors, this research is focused on a nonintrusive and inexpensive solution
for occupancy estimation that ensures occupants privacy wlule taking advantage of the technological
infrastructure already available in common Smart Environments including Smart Buildings and Smart
Homes. From the analysis performed on Section 2, and to comply with the previously established
requirements, our occupancy detection solution is focused on environmental data.

A scene analysis approach is used in this research to extract the features of interest for indoor
scenarios to proceed and then toest-imate the occupancy in the area using the gathered data [24]. The
scene analysis method does not rely on any theoretical model or specific hardware; however, it requires
a preliminary phase for capturing features which are in?uenced by changes in the area of interest [25].

in this section, we explain the criteria applied to select the features used in our solution before
moving forward to the description of the design of the four-layers architecture adopted for the
gathering and processing of the data. The section concludes with the discussion of the ML classifiers
that were selected to improve the performance of occupancy detection in indoor environments. Table 1
summarises the terms used in the remaining of the manuscript.

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Activity recognition
The goal of activity recognition is to recognize common human activities in real life

settings. Accurate activity recognition is challenging because human activity is complex
and highly diverse. Several probability-based algorithms have been used to build activity

 

models. The Hidden Markov Model and the Conditional Random Field are among the
most popular modeling techniques. We describe these two techniques in the context of an
eating activity example.

The Hidden Markov Model (HIVIM)

Simple activities can be modeled accurately as Markov Chains. However. complex or
unfamiliar activities are o?en dif?cult to rnrderstand and model. For example. a
researcher studying activities of daily living for a person vtu'th dementia will have a
dif?cult time ?tting a model unless she is an expert in dementia and turderstands its
related behavioral science. Fortunately. observing signals stemming from complex or
unfamiliar activities can be utilized to indirectly build a model of the activity. Such a
model is called a Hidden Markov Model or HMM. By observing the effects of an activity.
HMM is able to gradually construct the activity model. which can be further tuned.
extended and reused in similar studies.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Online learning is the process of answering a sequence of questions given knowledge of the correct
answers to previous questions and possibly additional available information Answering questions
in an intelligent fashion and being able to make rational decisions as a result is a basic feature of
everyday life. Will it rain today (so should I take an umbrella)? Should I ?ght the wild animal that
is after me, or should 1 mn away?? Should I open an attachment in an email message or is it a virus?
The study of online learning algorithms is thus an important domain in machine learning, and one
that has interesting theoretical properties and practical applications.

This dissertation describes a novel framework for the design and analysis of online learning
algorithms. We show that various online learning algorithms can all be derived as special cases of
our algorithmic framework. This uni?ed view explains the properties of existing algorithms and
also enables us to derive several new interesting algorithms.

Online learning is perforated in a sequence of consecutive rounds. where at each round the
learner is given a question and is required to provide an answer to this question. After predicting an
answer. the correct answer is revealed and the learner suffers a loss ifthere is a discrepancy between
his answer and the correct one.

The algorithmic framework for online learning we propose in this dissertation stems from a
connection that we make between the notions of regret in online learning and wr'ak duality in convex
optimization. Regret bounds are the common thread in the analysis of online learning algorithms.
A regret bound measures the performance of an online algorithm relative to the performance of a
competing prediction mechanism, called a competing hypothesis. The competing hypothesis can
be chosen in hindsight from a class of hypotheses. after observing the entire sequence of question?
answer pairs. Over the years, competitive analysis techniques have been re?ned and extended to
numerous prediction problems by employing complex and varied notions of progress toward a good

competing hypothesis.

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

for an optimization problem. in which we search for the optimal competing hypothesis. While the

optimal competing hypothe.

 

can only be found in hindsight, after observing the entire sequence
of question~answer pairs. this viewpoint relates regret bounds to lower bounds of minimization
problems.

The notion of duality. commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem. By generalizing the
notion of Fenchel duality. we are able to derive a dual optimization problem. which can be opti~
mized incrementally, as the online learning progresses. The main idea behind our derivation is the
connection between regret bounds and Fenchel duality. This connection leads to a reduction from
the process of online learning to the task of incrementally ascending the dual objective function.

In order to derive explicit quantitative regret bounds we make use of the weak duality prop?
erty. which tells us that the dual objective lower bounds the primal objective. The analysis of our
algorithmic framework uses the increase in the dual for assessing the progress of the algorithm.
This contrasts most if not all previous works that have analyzed online algorithms by measuring the
progress of the algorithm based on the correlation or distance between the online hypotheses and a
competing hypothesis.

We illustrate the power of our framework by deriving various learning algorithms. Our frame?
work yields the tightest known bounds for several known online learning algorithms. Despite the
generality ofour framework, the resulting analysis is more distilled than earlier analyses. The frame?
work also serves as a vehicle for deriving various new algorithms. First. we obtain new algorithms
for classic prediction problems by utilizing different techniques for ascending the dual objective.
We further propose ef?cient optimization procedures for performing the resulting updates of the
online hypotheses. Second, we derive novel algorithms for complex prediction problems, such as

ranking and structured output prediction.

This introduction presents an overview of the online learning model and the contributions of this
dissertation. The main concepts introduced here are covered in depth and more rigorously in later

chapters.

1.1 Online Learning

Online learning takes place in a sequence of consecutive rounds. On each round. the learner is
given a question and is required to provide an answer to this question. For example. a learner might
receive an encoding of an email message and the question is whether the email is spam or not.
To answer the question, the learner uses a prediction mechanism, termed a hypothesis. which is a
mapping from the set of questions to the set of admissible answers. After predicting an answer.
the learner gets the correct answer to the question. The quality of the leamer?s answer is assessed
by a loss function that measures the discrepancy between the predicted answer and the correct one.
The learner's ultimate goal is to minimize the cumulative loss suffered along its run. To achieve
this goal. the learner may update the hypothesis after each round so as to be more accurate in later
rounds.

As mentioned earlier, the perfomtance of an online learning algorithm is measured by the cu~
mulative loss suffered by the learning along his run on a sequence of question-answer pairs. We
also use the term example to denote a question?answer pair. The learner tries to deduce information
from previous examples so as to improve its predictions on present and future questions. Clearly,
learning is hopeless if there is no correlation between past and present examples. Classic statistical
theory of sequential prediction therefore enforces strong assumptions on the statistical properties of

the input sequence (for example, it must form a stationary stochastic process).

1.2.2 Problem Type

The Perceptron algorithm was originally designed for answering yes/no questions. In real?world
applications we are often interested in more complex answers. For example, in multiclass categov
rization tasks, the learner needs to choose the correct answer out of k possible answers.

Simple adaptations of the Perceptron for multiclass categorization tasks date back to Kessler's
construction [44]. Crammer and Singer [31] proposed more sophisticated variants of the Perceptron
for multiclass categorization. The usage of online learning for more complex prediction problems
has been further addressed by several authors. Some notable examples are multidimensional regres?
sion [76], discriminative training of Hidden Markov Models [23]. and ranking problems [28, 29].

1.2.3 Aggressiveness Level

The update procedure used by the Perceptron is extremely simple and is rather conservative. First,
no update is made if the predicted answer is correct. Second. all instances are added (subtracted)
from the weight vector with a unit weight. Finally. only the most recent example is used for updating
the weight vector. Older examples are ignored.

Krauth [78] proposed aggressive variants of the Perceptron in which updates are also perfonned
if the Perceptron's answer is correct but the input instance lies too close to the decision boundary.
The idea of twing to push instances away from the decision boundary is central to the Support
Vector Machines literature [1 17. 33, 100]. In addition. various authors [68. 57. 80, 74. 103. 31, 28]
suggested using more sophisticated learning rates. i.e., adding instances to the weight vector with
different weights.

Finally. early works in game theory derive strategies for playing repeated games in which all
past examples are used for updating the hypothesis. The most notable is followfthevleader ap-
proaches [63].

Our framework emerges from a new View on regret bounds, which are the common thread in
the analysis of online learning algorithms. As mentioned in Section 1.1, a regret bound measures
the performance of an online algorithm relative to the performance of a competing hypothesis. The
competing hypothesis can be chosen in retrospect from a class of hypotheses. after observing the
entire sequence of examples.

We propose an alternative View of regret bounds that is based on the notion of duality in con?
vex optimization. Regret bounds are universal in the sense that they hold for any possible ?xed
hypothesis in a given hypothesis class We therefore cast the universal bound as a lower bound for
an optimization problem. Speci?cally, the cumulative loss of the online learner should be bounded
above by the minimum value of an optimization problem in which we jointly minimize the cu?
mulative loss and a ?complexity" measure of a competing hypothesis. Note that the optimization
problem can only be solved in hindsight after observing the entire sequence of examples. Neverthe?
less. this viewpoint implies that the cumulative loss of the online learner fomts a lower bound for a
minimization problem.

The notion of duality, commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem (see for example [89]).
By generalizing the notion of Fenchel duality, we are able to derive a dual optimization problem,
which can be optimized incrementally as the online learning progresses. In order to derive explicit
quantitative regret bounds we make immediate use of the weak duality property, which tells us that
the dual objective lower bounds the primal objective. We therefore reduce the process of online
learning to the task of incrementally increasing the dual objective function. The amount by which
the dual increases serves as a new and natural notion of progress. By doing so we are able to
associate the cumulative loss of the competing hypothesis (as re?ected by the primal objective

value) and the cumulative loss of the online algorithm, using the increase in the dual.

In most of this dissertation we make no statistical assumptions regarding the origin of the se?

quence of examples. We allow the sequence to be determini

 

stochastic. or even adversarially
adaptive to our own behavior (as in the case of spam email ?ltering). Naturally. an adversary can
make the cumulative loss of our online learning algorithm arbitrarily large. For example, the adver?
sary can ask the same question on each online round. wait for the learner?s answer, and provide the
opposite answer as the correct answer. To overcome this de?ciency. we restate the learner's goal
based on the notion of regret. To help understand this notion, note that the learners prediction on
each round is based on a hypothesis. The hypothesis is chosen from a prede?ned class of hypothe?
ses. In this class, we de?ne the optimal ?xed hypothesis to be the hypothesis that minimizes the
cumulative loss over the entire sequence of examples. The learner's regret is the difference between
his cumulative loss and the cumulative loss of the optimal ?xed hypothesis. This is termed "regret?
since it measures how 'sorry' the learner is. in retrospect, not to have followed the predictions of the
optimal hypothesis. In the example above, where the adversary makes the learners cumulative loss
arbitrarily large, any competing ?xed hypothesis would also suffer a large cumulative loss. Thus,
the learner?s regret in this case would not be large.

This dissertation presents an algorithmic framework for online learning that guarantees low
regret. Speci?cally. we derive several bounds on the regret of the proposed online algorithms. The
regret bounds we derive depend on certain properties of the loss functions, the hypothesis class, and

the number of rounds we run the online algorithm.

1.2 Taxonomy of Online Learning Algorithms

Before delving into the description of our algorithmic framework for online learning. we would like

to highlight connections to and put our work in context of some of the more recent work on online

Section 5

Object Detection from video

The motion of object can be detected after the object is detected from Video. Trackmg the 301?le
or object from sequence Video frames this is the main goal of Video tracking. Blob tracking.
keinel-based tracking. Contour tracking are some common target representation and localization
algorithms. Ruolm Zhang [11] has proposed adaptive background subtraction about the Video
detecting and tracking moving object. He use median ?lter to achieve the backgroiuid subtraction,
This algorithm is used for both detecting and tracking moving objects in sequence of video. This
algorithm never suppoit for multi feature based object detection. Hong Lu and Hong Slieng Li
[12] were introduced a new approach to detect and track the moving object. The defme motion
model and the non-parameter distribution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kahnan filter estnnati g
its a?'me motion in next frame. The author shows Experimental results and proof the new method
can successfully track the object iuider such case as merging. splitting. scale Variation and scene
noise, The author Bayan [13] talks about adaptive mean shirt for automated multi tracking. The
bene?t of Gaussian mixture model is that it extracted Foreground image from video Er ine
sequence it also eliminate the shadow and noise from video sequence It is helpful in initiali 'ng
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from Video and hence we can track the object easily. The object can trap from
Video by changes in size and shape

In this paper section] gives introduction about HAR and gives motto of paper: over view of
traditional HMM classifier in section 2. Section 3 gives overview about HMM-based approach
that uses threshold aird voting and section 4 gives over view about HZMM-NN and NN-HMM.

Section 5 contaim the review of how object can be detected from other ways. The result of all
methods as conclusion in section 6 Sections ' contain references

Section 2

TRADITIONAL HMM (TLAssmIR

A hidden Markov model (HIVIM) is a statistical Markov model in which the system being
modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be
considered as the simplest dynamic Bayesian network. The logic behind the HMM was
developed by L. E. Baum and coworkers. It is nearly dependent on an earlier work on optimal
nonlinear ?ltering problem (stochastic processes) proposed by Ruslan L Stratonovich. who was
the ?rst to describe the forward-backward procedure [14]

In a regular Markov model. the state is directly visible to the observer. and therefore the state
transition probabilities are the only parameters. In a hidden Markov model. the state is not
directly visible. but output. dependent on the state. is visible. Each state has a probability
distribution over the possible output tokens. Therefore the sequence of tokens generated by an
HMM gives some infoiination about the sequence of states. Note that the word ?hidden' is refers
for the state sequence through which the model is passes. not for the parameters of the model.

Hidden Markov models are especially known for then" application in temporal pattern recognition
such as speech. handwriting. gesture recognition. pai?t-of-speech tagging. musical score following
partial discharges and bioinformatics

A hidden Markov model can be considered a generalization of a mixture model where the hidden
variables (or latent variables). which control the mixttu?e component to be selected for each
obseivation. are related through a Markov process rather than independent of each other.

PAC learning framework. For completeness, in Chapter B given in the appendix. we discuss the
applicability of our algorithmic framework to the PAC learning model. We start this chapter with
a short introduction to the PAC learning model. Next, we discuss the relative dif?culty of online
learning and PAC learning. Finally. we propose general conversion schemes from online learning
to the PAC setting.

1.4.2 Part II: Algorithms

The second part is (levoted to more speci?c algorithms and implementation details. We start in
Chapter 5 by deriving speci?c algorithms from our general algorithmic framework. In particular?,
we demonstrate that by varying the three components of the general framework we can design
algorithms with different update types, different aggressiveness levels. and for different problem
types.

Next. in Chapter 6 we show the applicability of our analysis for deriving boosting algorithms.
While boosting algorithms do not fall under the online learning model. our general analysis ?ts
naturally to general primal?dual incremental methods. As we discuss, the process of boosting can
be viewed as a primal~dual game between a weak learner and a booster.

Finally. in Chapter 7 we discuss the computational aspects of the different update schemes.
Depending on the loss function and update scheme at hand, we derive procedures for performing

the update with increasing computational complexity.

1.4.3 Part III: Applications

In the last part of the dissertation we demonstrate the applicability of our algorithms to real world

problems. We start with the problem of online email categorization. which is a natural online

Appendix B

Using Online Convex Programming for
PAC learning

In this chapter we outline the applicability of our algorithmic framework from the previous chapter
to Probably Approximately Correu (PAC) learning [116]. The PAC setting is also referred to as
hart/1 learning. We start this chapter with a short introduction to PAC learning. Next, in Section B2
we relate two important notions of learnahiliry: the notion of mistake bound in online learning
and the notion of VC dimension used in PAC learning. Based on this comparison. we demonstrate
that online learning is more dif?cult than PAC learning. Despite this disparity. as we have shown
in previous chapters. many classes of hypotheses can be ef?ciently learned in the online mode].
In Section B} we derive several simple conversions from the online setting to the batch setting.
The end result is that the existence of a good online learner implies the existence of good hatch
learning algorithms. These online?to?batch conversions are general and do not necessarily rely on
our speci?c algorithms for online learning.

B.1 Brief Overview of PAC Learning

hold as long as we have a suf?cient increment in the dual objective. By monitoring the increase in
the dual we are able to control the aggressiveness level of the resulting online learning algorithm.
To make this dissertation coherent and due to the lack of space, some of my research work was
omitted from this thesis. For example, I have also worked on boosting and online algorithms for
regression problems with smooth loss functions [SE]. 40]. online learning of pseudovmetrics [109],
online learning ofprediction suf?x trees [39], online learning with various notions of margin [103],
online learning with simultaneous projections [4], online learning with kernels on a budget [41],

and stochastic optimization using online learning techniques [1 10].

1.4 Outline

The dissertation is divided into three main parts. titled Theory. Algorithms, and Applications. In
each part, there are several chapters. The last section of each of the chapters includes a detailed

review of previous work relevant to the speci?c contents described in the chapter.

1.4.1 Part 1: Theory

In the theory part. we derive and analyze our algorithmic framework in its most general form We
start in Chapter 2 with a formal description of online learning and regret analysis. We then describe
a more abstract framework called online convex programming and cast online learning as a special
case of online convex programming. As its name indicates, online convex programming relies on
convexity assumptions. We describe a common construction used when the natural loss function for

an online learning task is not convex.

Abstract

Smartphones have become a global cormnunication tool and more recently a teclnrology for studying hrrrnan
behavior. Given their numerous built-in sensors. smartphones are able to capture detailed and continuous
observations on activities of daily living. However. translation of measurements from these consumer-grade
devices into research-grade physical activity patterns remains challenging. Over the years. researchers have
proposed various human activity recognition (HAR) systems which vary in algorithmic details and statistical
principles. In this paper, we summarize existing approaches to srnartphone-based HAR. We systematically
screened the literatru'e on Scopus. PubMed. and Web of Science in the areas of data acquisition. data
preprocessing, feature extraction. and activity classi?cation. We ultimately identi?ed 72 articles on
srnartphone-based HAR. To provide an understanding of the literature. we discuss each of these areas
separately, identify the most cormnon practices and their alternatives. and propose possible future research

directions for this interesting and important ?eld.

Keywords

Wearable computing; accelerometer; gyroscope: data acquisition: data processing: feature extraction:

activity classi?cation: digital plrenotyping machine learning: pattern recognition.

1.6 Bibliographic Notes

How to predict rationally is a key issue in various research areas such as game theory. machine
learning. and information theory. In this section we give a high level overview of related work in
different research ?elds. The last section of each of the chapters below includes a detailed review
of previous work relevant to the speci?c contents of each chapter.

ln game theory. the problem of sequential prediction has been addressed in the context of playing
repeated games with mixed strategies. A player who can achieve low regret (i.e. whose regret grows
sublinearly with the number of rounds) is called a Harman consistent player [63]. Hannan consistent
strategies have been obtained by Harman [63]. Blackwell [9] (in his proof of the approachability
theorem). Foster and Vohra [49, 50], Freund and Schapire [55]. and Hart and Mas?collel [64]. Von
Neumann?s classical minimax theorem has been recovered as a simple application of regret bounds
[55]. The importance of low regret strategies was further ampli?ed by showing that if all players
follow certain low regret strategies then the game converges to a correlated equilibrium (see for
example [65, 10]). Playing repeated games with mixed strategies is closely related to the expert
setting widely studied in the machine learning literature [41 82, 85, 119].

Prediction problems have also intrigued information theorists since the early days of the in?
formation theory ?eld. For example. Shannon estimated the entropy of the English language by
letting humans predict the next symbol in English texts [I l 1]. Motivated by applications of data
compression, Ziv and Lempel [124] proposed an online universal coding system for arbitrary in-
dividual sequences. In the compression setting. the learner is not committed to a single prediction
but rather assigns a probability over the set of possible outcomes. The success of the coding system
is measured by the total likelihood of the entire sequence of symbols. Feder, Merhav. and Gutman
[47] applied universal coding systems to prediction problems. where the goal is to minimize the

number of prediction errors. Their basic idea is to use an estimation of the conditional probabilities

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

approaches. The available studies both use existing methods and propose new methods for collection.
processing. and classi?cation of activities of daily limig. Authors commonly discuss data ?ltering and
feature selection techniques and compare the accuracy of various machine learning classi?ers either on
previously existing datasets or on datasets they have collected de ?um for the purposes of the specific study.

The results are typically siunmarized using classi?cation accuracy within different groups of activities
like ambulation. locomotion. and exercise. This paper aims to summarize recent efforts in smartphone-based
HAR research With the goal of providing an understanduig of the contextual complexity and

innltrdunensionality of the problem. the collected data. and the methods used to translate the digital

 

measurements into human activ

2. Methods

Oiu' systematic review was conducted by searching for articles published by June 30. 2019. on PubMed,
Scopus. and Web of Science databases. The databases were screened for titles. abstracts. and keywords
containing phrases ?acti\ity" AND ("recognitiorf OR ?estimation" OR "classi?cation") AND
(?srnai?tplione" OR ?cell phone" OR ?mobile phone?). The search was limited to full-length journal articles
written in English. A?er removing duplicates. we read titles and abstracts of the remaining publications.
References that did not uivestigate HAR approaches were excluded ?'oni further screening. In the following

step. we filtered out studies that employed auxiliary equipment. like wearable or ambient devices. and

algorithms. Therefore. only 292 references were assigned for full reading. of which 121 references that
employed additional hardware were excluded together with an additional 99 references that utilized built-in
microphones or' video cameras. Additionally, we excluded studies with sruai?rphones af?xed to the human
body. The remaining "2 references were read in detail.

Most HAR approaches consist of four stages. namely data acquisition. dam prepracasiing. feature
attraction, and activity classification. hi the follomirg. We provide an overview of these steps and brie?y
ponit to signi?cant methodological differences among the studies. Table 1 summarizes speci?c aspects of
each study: we have decomposed data acquisition process to sensor type. experimental envn'onruent.
mvestigated activities. and selected sinartphone location: We have also indicated which studies prepr'ocess
collected measurements using signal correction methods. sensor orientatioir-invar?iant transformations. and
noise filtering techniques. we have marked investigations due to types of sigma] features they extract. as
well as due to the feature selection approaches: fmally. we have indicated the adopted activity classi?cation
principles. utilized classi?ers. and practices for accuracy reporting. Figure 3 displays the most ?'equent

terms used in the included studies.

3.1. Data acquisition
We use the term data acquisition to refer to a process of collecting and storing raw sub-second level
srnartplioue measiu'ements for the purpose of HAR. The data are typically collected by a program or

application that runs on the device and samples data from built-m srnanphone sensors within predefined

1. Introduction

According to the GSM Association. there were roughly 2 billon srrrartplrones in use in 2019. and this number
is expected to dorrble in the next couple of years [1]. Such explosion iir worldwide snraitphone adoption
presents unprecedented opportunities for the study of hrunarr behavior. Smartphones now coirtairr rrrultiple
sensors to captru'e detailed. contnruous. and objective measurements of lrrurran behavior. including on
mobility and physical activity. Along with sufficient storage. powerful processors. and Wireless
transrrrission. such data cart be obtained Without additional hardware or instrumentation. which makes it
feasible to study large cohorts of subjects over extended tnne periods. Irrrportantly. smartphones are not a
niche product. as appears to be the case with most wearable activity trackers [2]. but instead they have
becorue a globally available technology. increasingly adopted by users of all ages both in advanced aird
ernergmg economies [3.4]. While these technological developments make the task of data collection easier.
analysis of the collected data is increasingly identified as the rrrarn bottleneck in research settings [5?"]. and
therefore it appears that the mam challenge iir human activity recognition (HAR) is now shi?ing from data
collection to statistical methodology and pattern recognition.

This proliferation of smartphones has riot goire unnoticed by the research comnrunity. At the time of
writing. there were nearly 300 articles published on HAR methods using srnartphones. a substantial increase
from just a handful of articles published a few years earlier (Figure 1). This growmg interest has takeir place

across various fields such as security aird surveillance [8]. personal navigation [9]. and health monitoring

individuals Less effort has been devoted to investigate populations with different demographic and disease
characteristics. such as elders [12] and subjects with Parkinson's disease [10]. As an example of a larger
study. Kelishomi et al. [13] analyzed data from 480 healthy individuals.

In the reviewed papers. data collection typically takes place in a research facility and or nearby outdoor
surroundings. In such environments. study participants are asked to perform a series of activities along
prede?ned routes and to interact Wllll prede?ned objects. The duration and order of performed activities are
usually determined by the study protocol and the subject is supervised by a research team member. A less
popular approach involves observation conducted in free-living emirouiuents. where participants perform
activities Without speci?c constraints. Such studies are likely to provide ruore insight into diverse activity
patterns due to individual habits and unpredictable real-life conditions. Compared to a single laboratory
visit. it also allows urvestigatois to monitor behavioral patterns over many weeks [14] or months [15].

Activity selection is one of the key aspects of HAR The studies considered here tend to focus on a
small set of activities. including sitting. standing. walking. running. and stair clunbing. The less common
activities involve various types of mobility. locomotion. fitness. and household routines. For instance. Wu
et al. [16] differentiate between slow. noirnal. and brisk walking: Guvensan et al. [1'] investigate multiple
transportation modes. like car. bus. train. train. metro. and ferry: Pei et al. [18] recognize sharp body-turns:
and Della Mea et al. [19] look into household activities. like sweeping a ?oor or walking with a shopping
bag. In Table 1. ?post\u?e" refers to lying. sitting. standing. or arty pair of these activities: ?mobility" refers

to Walking. stair climbnrg. body trims. riding elevator. or escalator. running. cycling. or airy pair of these

 

results. In the reviewed literatiu'e. subjects were o?en instructed to caiiy the device in pants pocket (either
front or back). although a number of studies also considered other placements. such as Jacket pocket [30].
bag or backpack [31]. and holding the sinaitphone in hand [32].

To establish the ground mith for physical activity in HAR studies. the data are usually annotated
manually by trained research personnel or by subjects themselves [33.34]. However. We also encoiuitered
several approaches that automate this process both in controlled and free-living conditions. For instance. in
[14] the data were labelled using a designated sniartphone application. A different approach was proposed
in [35]. where authors used a built-in step coiuiter to produce "Weak" labels. Also. the annotation can be
done usuig built-in microphone [36] and video camera [12].

Finally. the data acquisition process is carried out on purposely designed applications which captiu'e
and transmit data to the external server using cellular. Wi-Fi. Bluetooth. or wired connection. In online
activity classi?cation. the collected data do not leave the device but instead the entire HAR pipeline is

implemented on the smartphoue.

3.2. Data preprocessing
We use the term data prepi'ocessing to refer to a collection of procediu'es auned at repairmg. cleaning. and
transfoiining measiu'enients recorded for HAR. The need for such step is threefold: (1) nieasiu?ement

systems embedded in smartpliones are often less stable than research-grade data acquisition units. and the

which is possibly due to the temporally dense. high-resolution measurements they provide for distinguishing
among activity classes. The inertial sensors are o?en used synchronously providuig more insight into the
dynamic state of the device. Some stridies show that the use of a smgle sensor can yield similar activity
recognition results [20]. To alleviate the impact of sensor position. researchers collect data using built-m
barometer and GPS sensors to investigate changes in altitude and geographic location [21.22]. Certain
approaches bene?t from the broader set of capabilities of sinaitphones. and the researchers may additionally
exploit proximity and light sensors which allow the recognition of a nieasiu?ement?s context. e. g.. the

distance between siiiartphone and subject's body. and changes between in-pocket and out-of-pocket

 

locations based on chang' g illumination [23.24]. The selection of sensors is also affected by secondaiy
research goals. like simplicity of classi?cation and minimization of battery drain. In such approaches. data
collection is carried out on a single sensor (e.g. accelerometer [l4]). a small group of sensors (e.g..
accelerometer and GPS [25]). or with purposely modi?ed sampling frequency to reduce the amount of data
collected and processed [26].

The sampling frequency describes how many observations are collected by a particular sensor in one
second. The selection of sampling frequency is usually performed as a trade-off between measurement
accuracy and battery drain. In a typical data acquisition setting. the sampling frequency ranges between 20

to 30Hz for inertial sensors and l to lOHz for barometer and GPS. The most significant variations from this

description are required for inertial sensors if limited energy consumption is a pi'ioirty (e.g.. accelerometer

3.3. Feature extraction
We use the term feature extraction to refer to a process of selecting and computing meaningful summaries
of smartphone data for the goal of activity classification. A typical extraction scheme includes data
Visualization. data segmentation. featiu?e selection. and feature calculation. A careful featiu'e extraction step
allows urvestigators not only to understand the physical nature of activities and its manifestation in digital
measurements. but more importantly also helps uncover hidden structures and patterns in the data. The
identified differences are later quanti?ed through various statistical measures to distinguish among
activities. In an alternative approach, the entire process of feature extraction is automated rising deep
learning. which handles both segmentation and feature selection. On the other hand. and as with most
applications of deep leaniing. this often results in loss of irrteipr?etability and limited control over the process.
The conventional approach to feature extraction begins with data exploration. For this purpose.
researchers employ various graphical techniques. like scatter plots. lag plots. autocoirelations plots.
histograms. and power spectra [44]. The choice of tools is often dictated by the study objectives and
methods. For example. research on inertial sensors typically presents raw three-dimensional data from
accelerometers. gyroscopes. and magnetometers plotted for the corresponding activities of standing,
walking. stair climbing. etc. A similar approach is used for barometric pressiu'e data Acceleration data are
o?en inspected in the frequency-domain. particularly to observe periodic motions of walking. running. and

cycling [29]. and the impact of external environment. like natural vibration frequencies of a bus or a subway

 

frequency. Derawi and Bours [8] propose the use of linear mterpolation. while Gu et al. [3 7] utilize spline
interpolation. Such procedures are imposed on a range of affected sensors. typically including
accelerometer. gyroscope magnetometer. and barometer, Frnther time-domain preprocessing considers data
trimming. carried out to remove unwanted data components. For this purpose. the beginning and end of each
activity borrt are clipped as nonrepresentative for the given activity [30]. where a bout refers to a short period
of activity of a specified kind. During this stage the researchers also deal with dataset imbalance. The
imbalance occurs when observations of one activity signi?cantly dominate over others. Such situation
makes the classi?er susceptible to ovei?ttnig in favor of the larger class: however. the issue might be solved
by up- or downsamplmg of data [13.38]. Additionally. the rneasru'ements are processed for high-frequency
noise cancellation (m Table 1. see ?denoising?). The literature identifies several methods suitable for serving
this task. includnig the use of low-pass ?nite impulse response ?lters (with cut-off frequency typically equal
to 10Hz for inertial sensors and 0.1Hz for barometers) [39.40]. weighted monng average [8]. moving
median [29]. and singular value decomposition [41].

Another element of data preprocessmg considers device orientation (in Table 1. see ?tr"ansformation").
Sniartphone measurements are sensitive to device orientation. which may be due to personal choices.
clothing. body shape. and movement during dynamic activities [38]. One of the popular solutions is to
transform the three-dimensional signal into univarrate vector magnitude which is invariant to rotations and

more robust to translations. This procedure is often apphed to accelerometer. gyroscope. and magnetometer

ABSTRACT."

The rapid nnprovement m teahnologv Causes more attention towards to Remgnrzmg ofhuman aenvmes
?'om video. These new teehnolagreaz growth has made vision?based research much more mterestrng anal
e/?ctent than ever before. ihts paper present navel HMM (Hmaen Markov Model) based approach for
Human amwty r-eaagnraon from video. zhere are different approaches ofHMM to rerogn'ue ac?mi of
human from video. ere threshold and voting to automanmlly ana e?eetrvezy segment and reeogntze
sampler aanwtres, segment and recognwe eampten aatwmes and ?u srmple aammes we use Elmnn
Network (EN) and two hyanas ofNeuraINetwork (NN) andHll/lM, re. HMM?NN and NNI?VIM.

KEY Worms:

Human Activity reaagnmon, Hidden Markov Model, Hybrid model afHMM, Image mpmrmg from Video,
complex activity,

 

TRODUCTI

 

Automatically recognizing human activities from video is important for applications such as
automated suiyeillance systems and smart home applications. Several human activity recogn' 'on
methods [l][2][3][4][5][6] were proposed in the past few years to classify single human actitities
such as walking skipping. sitting down. etc. Human activity recognition (HAR) research has
been on the use because of the rapid technological development of the image-capturing software
and hardware. in addition to the omnipresence of reasonably low-cost high-performance personal
computers. The main goal of tlns recognition is used to develop the different application which
make human machine interaction is easy and interesting.

     

In the jouiney of developing algorithms for human activity recognition. some new developed
algorithms adds some new features in pre?ously developed algorithm. In this paper. we present a
novel HMM-based approach that uses threshold and wring to automatically and effectively
segment and recognize complex activities. And also survey on two hybrids of Neiu?al Network
(NN) and HMM. e. HMNI?NN and 3 ??]-]]\IM. This paper also compares their perfoimance
with that of the traditional HMM.

   

select the optimal window size. which emphasizes the inipoitance of this parameter to the performance of
HAR systenis [46?48]. This calibration aims to closely match the window size with the duration of a single
mstance of the activity. Smiilar motivation leads researchers to seek more adaptable segmentation methods.
One idea is to segment data based on speci?c tune-domain events. like zero-cross points. peak points. or

valley points. which represent the start and end points of a particular? activity bout [8.38]. This allows for

 

segments to have different lengths corresponding to a single fundamental period of the acti ity in question
Such approach is typically used to recognize quasipeiiodic activities like walking. running. stair climbing.
and sitting and standing [41].

The literature offers a large variety of signal features used for HAR. Such features can be divided into
several categories based on the initial signal processing procedure. This enables one to distinguish between
activity templates (i.e.. raw signal). time-domain features. and frequency-doinaui features. The most popular
features in the reviewed papers are calculated from time-domain signals as descriptive statistics. such as
local mean. variance. minimum and maximum. interquartile range. energy. and higher order statistics. Other
time-domain features include mean absolute deviation. mean (or zero) crossing rate. regression coef?cients.
and autocorrelation. Some studies describe novel and customized time-domain features. like histogram of
gradients [49]. magnitude of standard deviations [50]. number of local maxima and minima. their airiphtiide
and temporal distance between them [26] The described time-domain features are typically calculated over

each axis of the three-dimensional measurement or oiieiitation-invariant vector magnitude. Studies that use

 

iiniltidirneusionahty of smartplione measurement and data analysis. and offer guidelines and direction to

anyone interested in this challenging but inipoitant topic.

Acknowledgements

We are grateful to C ipr'ian Craiuiceanu. J aroslavv Harezlak. Emily Huang. and Greyson Liu for their careful

reading of our manuscript and then" insightful feedback.

Funding sources

The authors were supported by NIH?NHLBI award U01HLl453 86.

Author biographies

Martin Straczkiewicz is a Postdoctoral Research Fellow in the Department of Biostatistics at Harvard
University. His research is focused on developing novel statistical methods for quanti?cation of human
movements using wearable devices. primarily accelerometers and smanphones.

J'P Onnela is Associate Professor of Biostatistics at Harvard University. He obtained his doctorate in
Finland and subsequently completed fellowships at the University of Oxford. Harvard Kennedy School. and
Harvard Medical School. His main interest is in developing quantitative methods in statistical network

science and digital phenotyping.

Activity templates function essentially as blueprints for different types of physical activity. In HAR
systems. these templates are compared to patterns of observed raw measurements using various distance
metrics [25], Given the heterogeneous nature of human activity. activity templates are often enhanced rising
techniques similar to dynamic time warping [38]. As an alternative to raw measurements. sortie studies use
signal symbolic approximations created by discretization functions that transform data segments into
symbols [5. .54],

In the reviewed articles. the number of extracted features typically varies from a few to a dozen.
However. some studies purposely calculate too many features (sometimes hundreds) and let the analytical

method identify those that are most relevant and informative to HAR, Support vector machines [52]. gam

 

ratio [55]. recursive feature elimination [ ]. correlation-based feature selection [33]. and principal
component analysis [56] are among popular feature selection dimension reduction methods. A comparison

of feature selection methods is prorided by Saeedi and El-Sheimy [57].

3.4. Activity classi?cation

We use the term activity classi?cation to refer to a process of associating extracted features with par1icular
activity classes based on the adopted classi?cation principle. The classification is typically performed by a
supervised learning algorithm that has been trained to recognize patterns between features and labeled

physical activities. The fitted model is then validated on separate observations. usually using data from the

conscientious feature selection [29.54]. Computation time is sometimes repoited for complex methods. such

 

as deep neural networks [66]. extreme learning machine [67]. or synnbolic representation [53 .68]. as Well
as in comparative analyses [30]. Nevertheless. a comprehensive coinpaiison of results is dif?cult or

impossible as discussed next.

4. Discussion

Over the past few years many studies have investigated HAR using smartphones. The reviewed literature
provides detailed descriptions of essential aspects of data collection. data processing. and activity
classification. The studies have been conducted with one or more objectives. e.g.. to limit technological
imperfections (e.g.. no GPS signal reception indoors). to minimize computational requirements (e.g.. online
systems). and to maximize classi?cation accuracy (all studies). Oiu' review summarizes most frequently
used methods and offers available alternatives. We do not however identify any one ultimate activity
recognition procedure. and we doubt that one even exists. This results in pan from the complexity of the
task. Different studies use different activities. signal processing techniques. and classifiers. and each likely
suffers from speci?c potential drawbacks.

Some of our concerns relate to the quality of the collected data. While datasets are usually collected in
laboratoiy settings. there is little evidence that algorithms trained usuig data ?'om these controlled settings

generalize to free-living conditions [69.'()]. In free-living settings. such aspects as duration. frequency. and

We observed that the majority of studies utilize srnaitphones positioned stationary at a single body position
(i.e.. a speci?c pants pocket). and sometimes even with ?xed orientation. Such scenarios are however rarely
observed in real-life settings. and these types of studies should therefore be considered more as proofs of
concept rather than HAR systems that generalize to free-living settings. Other data quality considerations
relate to the description of the experiment and study protocol. including demographic details of the enrolled
cohort. enviromnental context. and details of the performed activities. Such information should be reported
as fully and as accurately as possible.

Only a few papers consider classi?cation in a context that involves actinties outside the de?ned
research scope. i.e.. activities that the HAR system was not named on. The designed classi?ers were instead
tasked to associate every movement with one of the prespeci?ed set of activities. Real-life activities are
however not limited to any particular set of behaviors. i.e.. we do not only sit still. stand still. walk. and
climb stairs. These classi?ers. when applied to free-living conditions. will naturally miss the activities they
were not trained on but will also likely overestimate others. An improved recognition scheme could assume
that the observed activities are a sample from a broader spectrum of possible behaviors or assess the
uncertainty associated with the classi?cation of each event.

Despite meeting the technical and practical requirements for human activity monitoring. a lack of
standardized procedures makes an apples-to-apples comparison of the studies dif?cult. The research also

suffers from de?cits in publicly available datasets. soiu'ce code. and named classi?cation models. Although

Abstract As performance of dedicated facilities continually improved, massive pulsar
candidates are being received. which makes selecting valuable pulsar signals from candi?
dates challenging. In this paper, we designed a deep convolutional neural network (CNN)
with I 1 layers for classifying pulsar candidates. Compared to arti?cial designed features.
CNN chose sub~integrations plot artd sub~bands plot in each candidate as inputs without
carrying bias '. To address the imbalanced problem. data augmentation method based
on synthetic minority samples is proposed according to characteristics of pulsars. The
maximum pulses of pulsar candidates were ?rst translated to the same position. then new
samples were generaned by adding up multiple subplots of pulsars. The data augmentation
method is simple and effective for obtaining varied and representative samples which keep
pulsar characteristics. In the experiments on HTRU I dataset. it shows that this model can
achieve recall as 0.962 while precision as 0.963.

 

Key words: pulsars: general ? methods: statistical ? methods: data analysis

1 INTRODUCTION

Pulsar searching is an important frontier in radio astronomy. Scientists pay more attention to pulsars
because of broad impact across physi s. astronomy. astronautics (Cordes et al. 2004: Lorimer et al.
1998: Lyne et al. 2004: Hobbs et al. 2000: Sheikh et al. 2006), etc. Many dedicated surveys have been
used to search more pulsar signals. such as Parkes multi?beam survey (PMPS. Manchester et al. 2001).
High time resolution universe survey (HTRU. Keith et al. 2010). and so on. With the advent of the
large?scale search surveys. such as Five Hundred Meter Spherical Telescope (FAST, Nan et al. 2011).
the Square Kilometre Array (SKA, Smits et al. 2009), weaker pulsar signals can be received, while
coming with more and more noise or radio frequency interferences (RFIS). which makes it dif?~
cult to select valuable suspected pulsar signal from massive pulsar candidates. Researchers have ap?
plied many successful methods to select pulsar candidates. including manual selection (Stokes et al.
1986; Johnston etal. 1902). selection with graphical tools (Faulkner et al. 2004: Keith et al. 2009),
ranking & scoring approaches (Lee et al. 2013) and machine learning methods (Eatough et al. 2010;
Bates et al. 2012: Morello et al. 2014; Zhu et al. 2014: Lyon et al. 2016; Devine et al. 2016: Tan et al.
2018; Guo et al. 2017).

While the mass of data is steadily increasing and data streams constantly
gain in speed, online learning algorithms used to process the data are naturally
limited by their maximal instance processing speed. As the evolution of these
massive data streams is much faster than the improvement of CPU power after
Moore's law [4], the gap increases between available and processable data As a
consequence, not all provided instances in a stream can be used by the learning
algorithm and some have to be skipped This could be especially harmful if the
skipped instances would reveal important insights to the user. To avoid skip?
ping instances and to still enable (potential) insights, currently not processable
instances could, in principle, be externally stored for later processing. However,
as the data stream speed is higher than the processing speed. the algorithm is
constantly challenged by the amount of data, and the amount of stored instances
is constantly increasing Besides memory usage, the time span from the arrival
of the instances to their processing (response time) is steadily increasing as well.
This processing delay can result in outdated information, and important events
might be missed To address these issues this paper introduces PAFAS (Pre?
diction Assured Framework for Arbitrarily Fast Data Streams), a framework to
handle high-speed data streams that potentially go to or beyond the limits of the
processing speed of the online learning algorithm. The contributions of PAFAS
are:

1. All unlabeled instances in the data stream receive a prediction.

2. The prediction is given promptly after the instances" arrival time.

3 The prediction model is constantly improved (independent of the DS speed).
4. No external instance storage is needed

it All unlabeled instances in the data stream receive a prediction.

2A The prediction is given promptly after the instances" arrival time.

3 The prediction model is constantly improved (independent of the DS speed).
4 No external instance storage is needed

The framework can be applied whenever events have to be detected as soon
as possible, and no information is allowed to be missed to detect these events.
We believe that concepts for the embedding of machine learning into real?world
systems are required, taking into account the time it takes to make a prediction
as well as the time it takes to train or re?ne a model. In this paper, we discuss
one such framework and present evidence from experiments with varying loads.

This paper is organized as follows First, related work is presented. Then. the
problem setting is presented along with the proposed framework. Subsequently.
the evaluation of the framework is presented in Section 44 The paper closes with
a discussion.

2 Related Work

Data stream mining has developed considerably in the past decade and attracted
many researchers to adopt existing algorithms for the challenging task to process
and reason about instances received at a very high speed [5] One part addresses
the adaptation of batch algorithms to cope with the data stream setting [6] by.
eg incremental batch approaches [7] To provide a speci?c environment for ef-
?cient data stream processing data stream management systems (DSMS) have
been developed. Such systems are adaptations of database management systems

2 DATA SET

To train and test our model. we need labelled convictive datasets. At present. the public labelled datasets
are relatively few. The most common one is the HTRU 1 dataset '. produced by Morello et al. (20l4).

This dataset is a part of outputs of a new processing of HTRU intermediate galactic latitude data
(Morello et al. 2014). It contains 1196 pulsars from 521 distinct sources with varied spin periods. duty
cycles. and signal to noise ratios. Besides. it has 89.995 non-pulsar candidates. it has been used in sortie
recent works (Morello et al. 2014: Lyon et al. 2016: Guo et a]. 2017: Ford 2017). In this paper. we used
it to train and measure our model.

Figure I is an example of pulsar candidates (pulsar20023) in HTRU 1 dataset with its four most
important subplots. The ?rst one subplot is fold pro?le plot. It is obtained by summing signal in all
frequency and period. Pulse pro?le of typical pulsar would make up with one or several narrow peaks
above the noise ?oor. The lower left one is subvintegrations plot. it is obtained by summing data ofdifv
ferent frequency channels. It re?ects the intensity ofthe signal duri g the observation time. For the ideal
pulsar signal. signal would be observed throughout the observation period. so one or several vertical
stripes will form corresponding to the peak positions in pro?le curve. Sub~bands plot is at the upper
right. By summing data over all period. it re?ects the intensity of signal at different frequencies. Since
radio pulsars are broadband. there should be one or more vertical stripes in most frequencies. In DM~
SNR curve at lower right. signal to noise ratio (S/N) ratio as a function of DM is recorded. As the pulse

The power of actionable data

Data collected from one IoT device has limited value on its own Plus,
according to Forrester Research, 60% to 73% of data in an enterprise
that could be used for analysis goes untapped. Real value is derived by
combining data sets from multiple devices to uncover patterns that can
be used to predict future performance

For example, a manufacturing plant may have 10 IoT devices or sensors
monitoring processes on various production machines. Rather than
looking at the data from only one device or each device independently,
analyzing data from across the site can provide a holistic view of what is
happening at that location.

Al is the enabling technology that processes large amounts of data and
recognizes patterns in the data Using powerful algorithms, AI adjusts to
new inputs and makes decisions based on what it has learned over time
to provide automated, accurate feedback to guide decision-makings It?s
the tool that adds value to all the data collected by loT devices. Al takes
advantage of the aggregation of big data to do more than just discover
what happened in the past Rather, AI produces analyses about ways
processes can be more ef?cient and predicts what could happen based
on multiple scenarios.

 

   

For example, A38 is a leader in industrial motion, power grids, electri?cation products and
industrial transport infrastructure. One of its biggest challenges is predicting plant emissions in
advance and taking proactive measures before violations occur.

ABB?s Predictive Emission Monitoring System (FEMS) powered by Al uses an empirical
model to predict emission concentrations based on processed data. The team successfully
implemented PEMS as part of a comprehensive Environmental Management System in one
of the largest gas processing plants ln the world. However, PEMS (inferential analyzer),
cannot measure emissions directly, So, the system uses an empirical model to predict
emission concentrations based on data like fuel ?ow, load, operating pressure and
ambient air temperature. These kinds of intelligence analytics applications are driving

Al adoption?and forcing companies to consider whether cloud?based or edge?based
analytics are right for their particular business case.

 

In related works. supervised machine learning methods have become more signi?cant the major
methods in classifying pulsar candidates.

The ?rst published work which attempted to use a machine learning approach to select candidates
is Eatough et al. (2010). They implemented arti?cial neural networks (ANN) with 12 designed experv
imental features as input vectors. Then Bates et al. (2012) and Morello et al. (2014) have made some
work to improve the performance by optimizing designed features with ANN. In these methods, de?
signed features relied on humans experience. may carry unexpected biases against panicular types of
pulsar candidates (Morello et al. 2014: Lyon et al. 2016). To address these problems. Lyon et al. (2016)
and Tan et al. (2018) selected fundamental and statistical features which aimed to minimize biases and
selection effects (Moiello et al. 2014) to provide better generalization performance.

In addition to these approaches with arti?cial designed features. data-driven methods also play
an important role in this ?eld. Zhu et al. (2014) developed Pulsar Image?based Classi?cation System
(PICS) system by using a group of supervised machine learning approaches. It makes the classi?cation
based on image patterns. The inputs are four important diagnostic plots of candidates rather than ex-
tracted features. It avoids possible defects of arti?cial design features and relying on excessive informa~
tion. It has been validated superior ability of recognition in PALFA survey pipeline and has discovered
six new pulsars. To address class imbalance problem in pulsar candidates. Guo et al. (2017) used Deep
Convolution Generative Adversarial Network (DCGAN, Radford et al. 2015) to generate more candiv
dates and automatically extract deep features at the same time. Then they used deep features to classify
data. which help to makes the classi?er more accurate.

In this paper. we take a step towards improving performance by the data?driven method. We de?
signed a deep CNN with eight convolutional layers, one ?atten layer and two fully connected layers.
The inputs are sub?integrations plot and sub~bands plot in each candidate rather than arti?cial designed
features. To make class balanced. we designed a simple and useful approach to synthesize more diverse
pulsar candidates. New samples were synthesized by adding up multiple subplots of pulsars after max-
imum pulses of pulsars were shifted to the same position. We tested our model on HTRU 1 dataset.
The results show that our model can provide satisfactory results on both recall and precision. The fol~
lowing pans are organized as follows: Section 2. the dataset used for training is introduced. Section 3
describes the data augmentation method for pulsar candidates. Section 4 introduces the network archi-
lecture and training details. Section 5 presents the experimental results ot?our model and analyses of its
performance. Finally. Section 6 is conclusions of our work.

HTK is a toolkit for building Hidden Markov Models (HMMs). HMMs can be used to model
any time series and the core of HTK is similarly general-purpose. However, HTK is primarily
designed for building HMM~based speech processing tools, in particular recognisers. Thus, much of
the infrastructure support in HTK is dedicated to this task. As shown in the picture above, there
are two major processing stages involved. Firstly, the HTK training tools are used to estimate
the parameters of a set of HMMs using training utterances and their associated transcriptions.
Secondly, unknown utterances are transcribed using the HTK recognition tools.

The main body of this hook is mostly concerned with the mechanics of these two processes.
However, before launching into detail it is necessary to understand some of the basic principles of
HMMs. It is also helpful to have an overview of the toolkit and to have some appreciation of how
training and recognition in HTK is organised.

This ?rst part of the book attempts to provide this information. In this chapter, the basic ideas
of HMle and their use in speech recognition are introduced. The following chapter then presents a
brief overview of HTK and, for users of older versions, it highlights the main differences in version
2.0 and later. Finally in this tutorial part of the book, chapter 3 describes how a HMM?based
speech recogniser can be built using HTK. It does this by describing the construction of a simple
small vocabulary continuous speech recogniser.

The second part of the book then revisits the topics skimmed over here and discusses each in
detail. This can be read in conjunction with the third and ?nal part of the book which provides
a reference manual for HTK. This includes a description of each tool, summaries of the various
parameters used to con?gure HTK and a list of the error messages that it generates when things
go wrong.

Finally, note that this book is concerned only with HTK as a tool-kit. It does not provide
information for using the HTK libraries as a programming environment.

Fig. 1.1 hiessage
Encoding/Decoding

Speech recognition systems generally assume that the speech signal is a realisation of some mes
sage encoded as a sequence of one or more symbols (see Fig. 1.1). To e?ect the reverse operation of
recognising the underlying symbol sequence given a spoken utterance, the continuous speech wave?
form is ?rst converted to a sequence of equally spaced discrete parameter vectors. This sequence of
parameter vectors is assumed to form an exact representation of the speech waveform on the basis
that for the duration covered by a single Vector (typically lOms or so), the speech waveform can
be regarded as being stationary. Although this is not strictly true, it is a reasonable approxima~
tion. Typical parametric representations in common use are smoothed spectra or linear prediction
coei?cients plus various other representations derived from these.

The role of the recogniser is to effect a mapping between sequences of speech vectors and the
wanted underlying symbol sequences. Two problems make this very dif?cult. Firstly, the mapping
from symbols to speech is not oneto?one since different underlying symbols can give rise to similar
speech sounds. Furthermore, there are large variations in the realised speech Waveform due to
speaker variability, mood, environment, etc. Secondly, the boundaries between symbols cannot
be identi?ed explicitly from the speech waveform. Hence, it is not possible to treat the speech
waveform] as a sequence of concatenated static patterns.

The second problem of not knowing the word boundary locations can be avoided by restricting
the task to isolated Word recognition. As shown in Fig. 1.2, this implies that the speech waveform
corresponds to a single underlying symbol (erg. word) chosen from a ?xed vocabulary. Despite the
fact that this simpler problem is somewhat arti?cial, it nevertheless has a Wide range of practical
applications. Furthermore, it serves as a good basis for introducing the basic ideas of HMM?based
recognition before dealing with the more complex continuous speech case. Hence, isolated word
recognition using HMMs will be dealt with ?rst.

(DBMS) to query continuous, unbounded data streams possibly in combina?
tion with pre?stored, ?xed datasetsi Two well-known DSMS. AURORA [8] and
STREAM [9], use their own language to query data streams. Both systems also
address the problem of too fast data streams. i.ei, when the system is not capa?
ble of processing all of the instances provided by the data stream. They use load
shedding (also implemented in a system environment [IOU to select instances of
the data stream that should be processed Based on Quality?Of?Service (QOS)
speci?cations. the system decides which instances are useful for the system to
fetch and which instances can be discarded. The main idea is to select instances
that will most probably lead to a good prediction. Another possibility to cope
With too fast data streams is sampling. Sampling is a technique to represent a
larger dataset by a smaller selected subseti It was frequently applied to reduce
the overall processing time of data mining algorithms and to efficiently scan large
datasets [ll]i In the simplest case it selects a random subset from the Whole data
set as an input for the learner Frequently, the purpose of this is to estimate the
quality of the result [12] Another possibility to cope with very fast data streams
is to adapt the mining technique corresponding to the currently available re?
sources Such methods are summarized under the heading of granularity-based
techniques. While load shedding and sampling change the input granularity of
the data mining method, the output of the data mining method can also be re?
duced, e.gi the number of rules or clusters [13]4 Then. the model that is used for
classi?cation is smaller and thus also more time?e?icient i.e.< more instances can
be processed in less time This method termed Algorithm Output Granularity
(AOG) can also be applied on various data mining schemes like clustering, clas?
si?cation or frequent set mining Lasti, anytime algorithms are also often used
for altering data stream speeds, as they can be interrupted anytime to return
an intermediate result [14] The more time available, the better the result has

 

 

The basic principles of HMMehased recognition were outlined in the previous chapter and a
number of the key HTK tools have already been mentioned. This chapter describes the software
architecture of a HTK tool. It then gives a brief outline of all the HTK tools and the way that
they are used together to construct and test HMM?based recognisers. For the bene?t of existing
HTK users, the major changes in recent versions of HTK are listed. The following chapter will then
illustrate the use of the HTK toolkit by working through a practical example of building a simple
continuous speech recognition system.

2.1 HTK Software Architecture

Much of the functionality of HTK is built into the library modules. These modules ensure that
every tool interfaces to the outside world in exactly the same way. They also provide a central
resource of commonly used functions. Fig. 2.1 illustrates the software structure of a typical HTK
tool and shows its input / output interfaces.

User input /output and interaction with the operating system is controlled by the library module
HSHELL and all memory management is controlled by HMEM. Math support is provided by HMATH
and the signal processing operations needed for speech analysis are in HSIGP. Each of the ?le types
required by HTK has a dedicated interface module. HLABEL provides the interface for label ?les,
HLM for language model ?les, HNET for networks and lattices, HDICT for dictionaries, HVQ for
VQ codebooks and HMODEL for HMM de?nitions.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

This concept of a path is extremely important and it is generalised below to deal with the
continuous speech case.

This completes the discussion of isolated word recognition using HMMs. There is no HTK tool
which implements the above Viterbi algorithm directlyr Instead, a tool called HVITE is provided
which along with its supporting libraries, HNET and HREC, is designed to handle continuous
speech. Since this recogniser is syntax directedT it can also perform isolated word recognition as a
special case. This is discussed in more detail below.

1.6 Continuous Speech Recognition

Returning now to the conceptual model of speech production and recognition exempli?ed by Fig. 1.1,
it should be clear that the extension to continuous speech simply involves connecting HMMs together
in sequence. Each model in the sequence corresponds directly to the assumed underlying symbol.
These could be either whole words for so?called connected speech recognition or sub-words such as
phonemes for continuous speech recognition. The reason for including the non-emitting entry and
exit states should now be evident, these states provide the glue needed to join models together.

There are, however, some practical dif?culties to overcome. The training data for continuous
speech must consist of continuous utterances and, in general, the boundaries dividing the segments
of speech corresponding to each underlying subword model in the sequence will not be known. In
practice, it is usually feasible to mark the boundaries of a small amount of data by hand. All of
the segments corresponding to a given model can then be extracted and the isolated word style
of training described above can be used. However, the amount of data obtainable in this way is
usually very limited and the resultant models will be poor estimates. Furthermore, even if there
was a large amount of dataT the boundaries imposed by handAmarking may not be optimal as far
as the HMMs are concerned. HenceT in HTK the use of HlNIT and HREST for initialising sub?word
models is regarded as a bootstrap operation??. The main training phase involves the use of a tool
called HEREST which does embedded training.

Embedded training uses the same Baum-Welch procedure as for the isolated case but rather
than training each model individually all models are trained in parallel. It works in the following
steps:

 

Figure 1.1: Diagram of an HMM showing the hidden Markov chain Xp, and the
conditional independence of the observation variables Yk given the states Xk. The arrows
indicate conditional dependence (e.g., Yo is dependent on X0 but Y1 is conditionally
independent of yo given X0 and X1).

3. Given the observations YD,Y1. . . . , yr, estimate the (unknown) parameters of the

HMM A that generated them.

Each of these three basic problems has a well known solution based on the HMM forward
backward procedure (see [14] and references therein for details). For example. the third
problem (i.e. the HMM parameter estimation problem) can be solved using the popular
Baum?VVelch algorithm (which uses the forward?backward procedure on the hatch of
observations YD,Y17. . . ya) [14].

over recent decades. a modi?ed HMM parameter estimation problem has been posed
by introducing the additional requirement that the observations Yn, y1, . . . in should be
processed sequentially (i.e. online) rather than stored and processed as a batch. This

online (or recursive) formulation of HMM parameter estimation has become a topic of

A robot is a machine?especially one programmable by a computer? capable of carrying out a complex series of actions automaticallylzl Robots can be
guided by an external control device or the control may be embedded within. Robots may be constructed on the lines of human form, but most robots are
machines designed to perform a task with no regard to their aesthetics.

Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY?s
TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed

swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating
movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the coming decade? with

home robotics and the autonomous car as some of the main drivers?)

The branch of technology that deals with the design, construction, operation, and application of robots,[5] as well as computer systems for their control, sensory
feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous
environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature
contributing to the ?eld of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.

From the time of ancient civilization there have been many accounts of user-con?gurable automated devices and even automata resembling animals and
humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications
such as automated machines, remotecontrol and wireless remote-control.

The term comes from a Czech word, robola, meaning "forced labor";[61 the word ?robot? was ?rst used to denote a ?ctional humanoid in a 1920 play R.U.R.
(Rossumovi Univerza?lrli Roboti - Rossum's Universal Robots) by the Czech writer, Karel Capek but it was Karel's brother Josef Capek who was the words true
inventorlms?g] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey
Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen. The
?rst commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where
it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New
Jerseylml

Robots have replaced humans?? in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations,
or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their
role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions.?21 The use of robots in
military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in ?ction and may be a realistic
concern in the future.

It the accuracy difference between two experiments is bigger than the con?dence
interval, this di?erence can be considered signi?cant with a 9596 of probability. in this
paper. for all the experiments, the con?dence interval is lower than 0.596, so any di?err
ence higher than this value, this difference can be considered as signi?cant with a 9596 a
probability.

Data analysis

lu this section, we ?rst conduct some experiments for l-lAR system con?guration tuning
in order to analyze how the evaluation, the sensor type, or the feature type in?uences the
system performance.

Evaluation method

This section includes the experiments considering the two different evaluation meth?
ods. In these experiments, are setup is: ideal?placement, Null?activity removed (as in the
original paper), and time?based features. The experimental results are shown in Table 1.
From the results, we can clearly see that the result given by the random?partitioning
method is signi?cantly better man the subjectrwise method: the accuracy (Acc%) differ?
ence is 3.496 (99.1?95596) higher than ?re con?dence interval, 0.596.

This result supports the hypothesis suited in me previous secn'on: in the random?part
evaluation, training and testing subsets could contain information from a same subiect
and this characteristic produces better classi?cation results, ln the rest of the paper, we
will only consider the subjectwise evaluation method (more challenging situation).

Type elsensor
This section includes the experiments on different sensor types. ln these experiments,
the setup is: ideals placement, Nullractivity removed (as in die original paper), and time
based features. The experimental results are shown in Table 2. From the results, we can
clearly see that the 3D magnetometer works best among are four types of sensor and the
quaternion sensor performs the worst. The accuracy (Acc?xi) differences are statistically
relevant because they are bigger than the con?dence interval (0.596).

(CNN) ? Ricky Gervais? "After Life" was a bittersweet little gem, but the ?rst season basically
told a reasonably complete story. As a consequence, the second six?episode run feels as if it's
essentially retracing old tenitory ?? moving in places. out With less urgency, and more prone to
silly detours to ?esh out the run.

GeNais' formula of two seasons and out (plus a follow?up special) worked well enough for the
original "The Of?ce" and "Extras." But his ?lmography has been more uneven of late, With "After
Life" very much in keeping with the Writer?producer?star?s outspoken atheism and darker, if not
irredeemabie view of life.

As a brief recap. the ?rst season found Gervais? Tony sleepwalk?ing through his days after his
beloved Wife died of cancer, consoling himself by watching old videos and home movies, With a
faithful dog (Eho?s a good girl? Vou are) as a companion.

By the end, Tony's outlook had brightened, showtng traces of generosity toward coworkers at
the local newspaper Where he grudgingly churned out human?interest stories, and ?nding a
potential new romance in the nurse, Emma (Ashley Jensen, splendid as always), looking after his
dementia?stricken dad (David Bradley).

The new season, however. finds Tony backsliding, again wallowing in grief to the point of
endangering his relationship With Emma, who understandably struggles with his behavior. This
continues, notably, despite the advice that Tony seems intent on ignoring from his cemetery pal
Anne (Penelope Wilton), who chides him not to mex things up.

in the most unoomfortable real?life echo, the aforementioned newspaper is struggling ?nancially
?? at a moment when that industry is painfully unraveling ?? posing an additional challenge to
Tony's boss and brother?inelaw. Matt (Torn Basden), whose marriage is falling apart even as he
presides over the paper's Woes.

 

TECHNEWSWORLD DEVELOPERS

(omD

 

Re ews Se

 

rig iternet rr MD

   

ty Technalngv TechEIDQ

sharpens the Developer 5 Edge

With more developers having an accessible, free place to create software, there
will be more opportunities to collaborate on projects without being limited by
prices, noted Netdata's Tsaousis. With more collaboration, open source startups
and other projects likely will grow and scale at a faster rate.

"Netdata's ?rsthand experience has proved how powerful these tools can be. As
an open source project, founded by a sole developer, Netdata's roots are ?in
using GitHub's open source repositories," he said.

With the help of the community's contributors, along with dedicated company
engineers, the Netdata Agent has grown into one of the most watched and
starred projects on Gitllub and is downloaded more than half a million times a
day, Tsaousis pointed out. The new free access expansion will make it even
easier for an organization to standardize on Gitllub, which is at the heart of the
pricing decision.

"Becoming a de facto standard is a huge advantage with regards to competitive
solutions," he noted. ?However, once an organization is set on GitHub, it can be
dif?cult to revisit that decision if Pro and Team features ever become cost?
prohibitive,"

Bril ant Growth Move

Gillab is a growmg threat to GitHub, and a key reason for GitHub?s change in
strategy, suggested Thomas Hatch, CTO of SaItStack.

"This move is cntical for Gitllub to stay relevant as GitLab continues to steal
users and customers from them. In a nutshell, this allows Gitllub to give open
source users and developers the same thing that G'ltLab has been delivering for
many years,? he told LinuxInsider.

As more big names open?source their software and participate in the
community, it's important to remember why open source existed in the first
place, and the value of the foundation from which you can build, noted Aiven

of ?ow meter based disaggregation. it also can determine the volume of water used during each
classified usage event. waterSense requires a sample every 2 s for flow and one every seven seconds
for the motion sensor. The latter is only processed when motion is detected.

4. Classi?cation of Water Even?s

As previously discussed, the approaches for water use disaggregation make use of a variety
of different sensing modalities. The sensed signals are then subject to pattern analysis by applying
classification techniques in order to identify the corresponding water usage events. Depending on
the nature of the sensing modalities, different classifiers are utilised. These classifiers can be further
divided into discriminative and generative ones. in the following we structure our discussion on the
classification techniques according to the utilised sensing modality.

4.1. Water Flow Based Methods

A key assumption of non?intrusive monitoring approaches based on flow meters is that the
use of a particular water fixture or fixture type causes a distinct flow pattern in the residential pipe
infrastructure, which can be observed by a single sensor point. By applying pattern recognition
algorithms to the recorded time series data, water usage events for the specific water fixture or fixture
type can be identified.

Water flow in households is typically modelled with Poisson rectangular pulse as described in [42].
Figure 2 shows a 24-11 dataset generated from the aforementioned model. The number of events in a
unit of time follows a Poisson distribution, while duration and intensity have their own mean and
variance. All the parameters should be adjusted by time of the day with diurnal multipliers. The
observed flows are directly related to the volumes. Reported figures about water volumes and flows
are shown in Table 1 [34,43].

Non~intrusive monitoring techniques based on flow meters face the following difficulties;

. lrregularities of flow patterns for some fixture types. Some mechanically driven water valves,
such as in a washing machine or dishwasher, usrurlly exhibit more regular water usage patterns,
unlike faucets or showers where human users have the ability to vary the amount of flow and
duration significantly. Even the more regular flow patterns of washers can show variation
based on a diversity of different water saving programs and washing cycles that are available in
modem~day devices.

. Similarity of flow patterns among instances of the same fixture type. Many homes have multiple
toilets and/or faucets, which may be located in different rooms. This makes it challenging to
identify an individual ?xture if multiple instances of the same type exist [34,33].

I Wasn't the type of kid who got attention. Teachers always wrote ?needs to participate more" on my report cards (with a smiley face to make my parents feel better]. I never got into trouble
and barely ever stood out on purpose. A fewyears earlier, I accidentally peed my pants because my zipper had gotten stuck in the bathroom at the last moment. I tried to convince everyone
that I had fallen into a puddle at recess. The custodian, Mr. Salazar, charged outside with a mop and brought me to find the puddle. My guess is that we wasted an hour looking around at the
gravel. My mom dropped off some new clothes and nobody really noticed my wardrobe change (...or that it hadn't rained inweeks].

That's how it was. Whether I did something spectacular or sneezed myself out of a chair, nobody cared, and almost nobody said my name. As far as school has concerned, all those things had
happened to ?some kid". So, why would a frogwith glasses jump up on a windowsill to stare at ?some kid??

Teachers, on the other hand, were different. Once her story ended, it hadn't taken Miss Weaver long to realize that Imsn?t paying attention. She called me up to the blackboard to make an
example out of me.

?Since you don't feel the need to listen, why don't you solve a problem on the board instead?? she said, sitting down at her desk.

My stomach did a ?ip. The problem would take a minute or two to solve, and being in front of the class almys made me nervous. How could I be expected to do anything when there was a
spectacled frog staring me down!

I stood to the right of the equation on the board so that I could check on the frog with quick glances. Despite the distraction, I did my best to focus. Halfway through, I saw that the frog had
moved towards the front of the classroom. It stopped at the window by Miss Weaver's desk. It took me a moment to figure out what it ms doing. I couldn't believe what [was seeing. It was
trying to lift the window.

Focusing on the problem became almost impossible. I made a mistake and then quickly erased it. The next time I looked over, the window was open. Why should that surprise me? Of course
a frog with glasses would also be super strong. The window was only open an inch, but that was enough for it to slip through. I dropped the chalk, and some of my classmates laughed.
Bending down to pick it up, I tried convincing myself that when I stood back up again the frog would be gone. ?It?s not there, I just think it's there.?

When I straightened up, the frog was sitting on Miss Weaver's left shoulder. This was a brave frog. Her head blocked the class from seeing it, and I realized that [was still the only one who
could. Either the frogwas real or my imagination had outdone itself. It wastit all that surprising that Miss Weaver didn't feel it there, because the shoulder pads inside her jacket were large
and ?uffy. I had heard that she rested her head on them like pillows during her breaks. So, now there ms a frog sitting on Miss Weaver's shoulder and nobody else knew it. And [was
supposed to be doing math.

Now that it was closer, I could see the frog better. It didn't look like some new species of frog to me. It looked like every other frog I had seen (except for the glasses]. I wondered if they made
contacts small enough for a frog. But, it wasn't the right time to worry about frogvision. That's a job for a frog eye doctor, anyway.

I had daydreams all the time when I was drawing, and sometimes I got lost in them. I tried one last time to explain the frog away, by saying it had to be part of an elaborate daydream. I
concentrated hard, finished the problem, and put the chalk down. The frog couldn't be real. I shook my head confidently.

On a high level, disaggregation allows domestic water use to be broken down into fixture
categories, which identify the amount of water consumption of individual fixture types in a household.
Typical fixture categories for indoor use are shower, bathtub, toilet, and faucet, as well appliances,
such as washing machines and dishwashers. Typical outdoor fixture categories are exterior hose bib,
swirrrming pools and irrigation systems. Most current end use studies provide insights on residential
water use at the level of the fixture category. Sometimes, however, it is necessary to distinguish
between hot and cold water use, and the location (room, indoor or outdoor) where the water is being
used. Such water usage break down requires knowledge of water use at individual fixtures or even
valves (in the case of hot and cold water).

A further level of contextualisation is the attribution of water use to individual residents in a
multi-party home or the mapping of water use to individual activities (e.g., washing hands, cleaning
teeth, watering the garden, eta). The latter contextualisations are particularly hard to obtain

One of the most common methods for deriving a breakdown of domestic water end use
information is through manual data collection acquired by consumer surveys, diaries/self?reports
and in situ observations in domestic living environments. Such studies are able to capture a diversity
of information, even detailed information that is sometimes very difficult to capture, such as water
usage activities. However they tend to be very labour intensive and do not scale well for longitudinal
analysis (over longer periods of time for larger populations). 0nline survey tools, such as the Water
Energy Calculator [28], have made it easier to reach wider audiences [6], however, such studies rely on
the truthfulness of the persons participating in the studies. Despite their best attempts at being honest,
users often reflect perceptual bias or may accidentally misreport relevant information 129]. Furthermore,
self-reports and surveys are not able to capture the exact amount ot water use and represent only
estimates that have to be complemented by more detailed metered water use. Researchers in the
field have therefore looked into instnrmenting households and applying data analytics solutions to
measured data traces in order to gain a better insight into water usage patterns.

A very accurate but inefficient way to obtain such insights is through extensive instrumentation
of a household. Each individual fixture or even valve can be instrumented with a flow meter. Such
deployments are mainly limited to testbed settings [30732], in order to establish a ground truth tor
other experiments with less intrusive techniques. It is not difficult to see that such an approach is, not
only cumbersome and costly in terms of deployment, but highly intrusive. Such a case can be seen as
analogous to intrusive load monitoring in the energy domain [33].

ln order to overcome the above limitations, a variety of non~imrusive monitoring approaches have
been proposed, which are able to perform water disaggregation based on data obtained from a single
sensing point or from a limited set of sensing points deployed at strategic locations of a residential
water pipe infrastructure and /or rooms ot a residences.

 

evaluation methods used in this work and the experimental results Obtained with the
proposed system. Sixth section summarizes the main conclusions.

Background
Human Activity Recognition systems can be categorized by machine learning algorithm
and the type of sensor they used. Human activity recognition can be seen as a machine
learning problem. To deal with this problem, the HAR system must extract features from
sensor signals, generate a model for each activity, and classify next activities based on
these models. In the literature, different machine learning solutions have been applied
to the recognition of activities including Naive Bayes [3], Decision Trees [4], Support
Vector Machines (SVMs) [5], Deep Neural Networks [6] and Hidden Markov Models
(HMMs) [7]. In many works, several approaches have been compared using the WEKA
learning toolkit [8] because it incorporates many machine learning algorithms, For
example, Yang [9] compares the performance of several machine learning approaches:
C45 Decision Trees, Naive Bayes, krNearest Neighbor, and Support Vector Machines,
Kwapisz [10] compares three learning algorithms: logistic regression, 148, and multilayer
perceptron. Not only supervised but also, unsupervised algorithms have been studied
[11], In many works [12], complex algorithms, like the Random Forest, have demon?
strated a very good performance compared to simple classi?cation algorithms. Because
of this, the Random Forest has been the algorithm selected in this work.

For HAR, there are two main types of sensors: ambient and on?body sensors. In
terms of ambient sensors, the most widely used sensors are video cameras [13]. Video
recording is one of the main strategies for supervising human behavior and activities

[14]. But, this behavior can also be studied by analyzing acoustic events, The human

USA
TODAV

NFL draft moves along after wild night

First round of the 2020 NFL draft is in the books and will move ahead to the second
and third rounds on Friday night (7 p.mi ET on ABC, ESPN and NFL Network), Joe
Burrow went ?rst overall to the Bengals as expected, and in the next several
selections, teams followed the chalk: Chase Young to Washington, Jeff Okudah to
Detroit. Offensive tackle Andrew Thomas went to the Giants, followed by
quarterbacks Tua Tagovailoa and Justin Herbert to the Dolphins and Chargers,
respectively The "virtual" NFL draft also made for interesting TV on Thursday
night, from Cardinals coach Kliff Kingsbury showing off his lavish pad to whatever
it was that was going on at Titans coach Mike Vrabel's housei USA TODAY

Sports will have live updates and analysis of all of the news from the event,
including an up?tofthe?minute tracker breaking down every pick in real time.

. NT]. drz?'s most intriguing ?rst?round pidm Packers selection of QB Jordan Low raises eyebrows

- SEC breaks its own record for ?rst-round NFL draft picks with 15, led by LSU
and Alabama

Virtual vigil to be held in Canada for mass shooting victims

Victims of one of Canada's deadliest mass killings will be remembered Friday night
in a virtual vigil. Twenty?two people were killed last weekend when a gunman
dressed as a Royal Canadian Mounted Police of?cer went on a 12?hour killing spree
that began in the rural town of Portapique, Nova Scotia, and included at least 16
crime scenes across the provinceThe suspect, 517year?old Gabriel Wortman, was
shot dead by police The vigil will be livestreamed on the Facebook group
Colchester ? Supporting our Communities at 7 pm, local time (6 p.m. ET) and will
also air on the CBC News Network.

Type offeature
We also make a comparison on the performance of different kinds of features, more spe?
ci?cally, temporal features and frequency features as described in ?Feature extraction?f
Same as the experiments on sensor types, here, we also consider the idealvplacement,
removing the Null?activitv. For the sake of con?dence, we repeat the experiments with
different types of sensor.

From the results shown in fig. 2, it is obvious that the temporal features always beat
the frequency features and their combination in the cases of all mree sensor types.
Therefore, we consider only the tjme?based features in the rest experiments.

As a conclusion, it is clear that using me signals from magnetometer and the time
based features is currently the best system con?guration. By including all sensors, we
obtain even higher system accuracy: 97.096.

 

When tr ning and testing with different subjects, it is important to deal with the
intenuser variability. In order to reduce this variability, we propose several normali?
zation strategies. In this work, we evaluate six normalization methods, considering
two different places where this normalization is applied: before and after the feature
extraction.

1. Mean removal: Subtract the lnean value from each value in a feature or signal vector.

2. ZsScore: Mean removal ?rst, and divide each value by its standard deviation.

3. Histogram equalization. Consider all the values in a graysscale, and equalize its his?
togram,

4. 0?1 mapping: Distribute all data to the (L1 range.

Vector normalization: Divide each value in a vector With the vector's magnitude

6. Vector normalization with mean normalization. Vector normalization followed by

9"

mean removal

3.1. Assessing Individual Water Consumption

An effective way of measuring household level water consumption is through metering the
water supply at the premises of a customer. Only about half of UK households currently have a
water meter installed [21]. The vast majority of these meters are not Internetfonnected, and require
a manual readout by the water company or the customer. Meter readouts often take place on an
annual or monthly basis, in order to estimate the domestic water hill. Non~metemd customers are
charged an amount that is proportionate to the rateable value of the property [22]. This results in
an annual ?at rate that does not take into consideration the size of the household [23]. Automated
meter reading (AMR) or smart meter reading provides the ability to automatically capture water
usage information at more regular intervals. In their most basic form, such meters do not require a
connectivity infrastructure. They act as standalone meters that can be read through some wireless
channel in a walk~by (e. g., handheld devices) or driveby fashion (e.g., utility service vehicle). A more
effective way is connecting AMR/smart meter devices via a dedicated metering infrastructure to the
utility company, or via existing communication networks available at the household (e.g., phone line,
lntemet router). While this comes at increased costs and complexity it removes the burden of relying
on physical proximity for retrieving the meter readout, theoretically allowing near maHime reporting
of metering information in practice, meters are typically monitored on a daily, hourly basis or 15 min
basis [24], as dictated by the costs for data communication and data storage, respectively

In contrast to AMR devices, which provide only simple reporting functionality, smart meters can
provide bidirectional communications. Depending on their extended capabilities, smart meters can
provide some configuration options to the utility company, such as the configuration of the reading
interval or other system settings Some smart meters can be even interfaced to in-home displays or
smart home platforms, providing residents with information on their current or historic water use [25].

Despite their advanced metering capabilities, current smart meters are only capable of answering
how much water is being used and when. Breaking down the residential water use to more finegrained
levels, e.g., fixture level use, requires higher resolution readings combined with external data analytics
and possible additional instnrrnentation. Recent work in the field of energy metering calls for an
evolution of smart meters to become ?cognitive meters" [26] that are able to disaggregate the water use
within the metering device. While basic features, such as household leak detection on smart meters, is
already feasible [27], this vision still requires some further advances in the field.

interestingly, the scientific community has been working since the 19905 on approaches for water
use disaggregation at the household level. In the following we will examine these works and identify
strengths and weaknesses of these and current gaps.

4.1.1. Discriminative Classifiers

Flow trace analysis is one of the first automated techniques to infer water usage from single
flow meter readings. It was initially proposed by Dziegielewski ct al. [45], and is currently the most
widely?used technique for identifying water usage events in the water industry due to its maturity
and the availability of commercial service offerings based on it. Flow trace analysis relies on the fact
that domestic water use exhibits Common patterns that are distinctive enough to discriminate water
usage events of different fixture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decisionstree based classi?ers, the current water source for these water usage events can be determined.

A first extensive study that utilised flow trace analysis was presented by De0reo el al. [34]. The
authors performed a collection of signature flow traces for each fixture inside of 16 homes at a rate
of one sample every 10 s using a flow meter The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each. Using the signatures, data~?ow traces were
determined based on visual analysis. When a type of flow was identified, it was isolated in a window
and the integral of the flow rate over this window provided the volume of water used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a slgnal~processing algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, flow rate
change over time and time of the day cues. The authors however did not provide any assessment of
the performance of their solution

The two market leading commercial tools, TraceWizard [46] and ldentiflow 147], are also based on
the principle of flow trace analysis. According to a previous review by Nguyen ct al. [43] for these two
systems, both use decision tree based classifiers and require a timeconsuming and labour-intensive
process to perform offline fixture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak flow rate, the most common flow rate, and how often this most common flow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
fixtures are used at the same time or 0% when three or more were used. Similarly ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.8% in terms of the correctlyaclassified volume. As it relies on
fixed physical features ofvarious water?using devices, such as volume and flow rate for disaggregation,

the ?nal classification accurag is greatly degendent on the existing Mes of water devices

 

TECHNEWSWORLD DEVELOPERS

Commit d I ernet rr Mn leTecli Rev ws Secur Teclniulngy reclining

 

Right Timing

"This ?ls a huge investment we are making, and it is good for Github's business
long term because more developers globally will be able to use the platform," a
spokesperson said in a statement provided to Llnuxlnsider by company rep
Nicole Numrich.

The growth factor is the key reason for the change of plans in providing
expanded free access, according to the spokesperson. It has 40 million
developers now, and global development is not slowmg down.

The GitHub Enterprise produti is reaching more companies than ever with 29
Global Fortune 50 companies building the software behind their businesses on
Gitliub Enterprise, the spokesperson noted. This shift from a "pay?for?privacy"
model to a "pay?for?features" model IS a fundamental change to the business
architedure of Gitllub.

Plummeting Price Plan

Gitl?lub reduced the entry?level paid tier price to $4 per user per month instead
of $9. The company still offers a more expensive tier (521) with SAML sign?on
and greatly expanded storage and actions.

Also still available is the specialized GitHub One service. Account managers
with high?value customers can negotiate special subscription fees.

Under the previous cost structure, Gitl-lub offered a free tier for private
development. However, it limited the number of collaborators with access to a
private repository to three.

Teams interested in using GitHub for private development had to subscribe to
one of its paid plans. GitHub previously offered unlimited repositories for free
only to public projects or those with a small number of users. That precluded
use of the free tier by several different types of teams, organizations and
companies.

In this chapter. we are focused on quickly determining if a manoeuvre has occurred.
rather than estimating the manoeuvre time T and postemanoeuvre velocity VG. we
therefore aim to design manoeuvre detection algorithms that minimise the time delay
hetween when the manoeuvre occurs and when the null hypothesis H0 is rejected (whilst
avoiding false alarms). we shall present our proposed manoeuvre detection algorithms
as (stopping) rules for selecting the time to declare that a manoeuvre has occurred (by

rejecting H0).

lmportantly. our aircraft manoeuvre detection problem can he viewed a non?Bayesian
quickest change detection problem. In this chapter, we will exploit our minimax ro
hustness results of Chapters 4 and 5 by proposing two classes of vision?based aircraft
manoeuvre detection algorithms: a heading?based class of approaches inspired by our
i.i.d. process results of Chapter 4; and a transition?based class of approaches inspired
by our dependent process results of Chapter 5. Here. in addition to proposing and
investigating robustness?inspired manoeuvre detection algorithms, we will also consider
adaptive algorithm that attempt to detect the unknown manoeuvre by estimating any

unknown postemanoeuvre imageplane velocity information.

We shall ?rst brie?y revise an HMM representation of the aircraft imageplane
dynamics (5.1) that underpins both our transition?based and heading?based aircraft mae
noenvre detection approaches. We will also introduce an intuitive method of estimating
the aircraft?s imageeplane heading app that we shall use to propose our heading?based

manoeuvre detectors.

a least favourable parameter approach that requires the uncertainty set of parameters
to satisfy an informationrtheoretic Pythagorean inequality condition. This information?
theoretic Pythagorean inequality condition is closely related to the partial stochastic
boundedness concept we introduced in Chapter 4 to identify least favourable distributions

for our i.i.d. process asymptotic rninimacr quickest change detection problems.

In the case of i.i.d. observations before and after the changetime, the Lorden and
Pollok results of this chapter are special cases of the results we established previously
in Chapter 4. speci?cally, the results of this chapter only apply in the i.i.d. case
when the prechange distribution is known. and when the uncertainty in the post?change
distribution is described by unknown parameters (i.e., when the uncertainty is parametric
rather than nonparametric). The results in this chapter are signi?cant because they
hold for a large variety of stochastic processes including Markov chains and linear state
space systems (whilst the results of Chapter 4 and [e] are limited to processes with i.i.d.

observations before and after the changetime).

This chapter is structured as follows: In section 5.1. we propose Lorden. Pollak, and
Bayesian minimar robust quickest change detection problems with polynomial delay
penalties and parametric uncertainty. In section 5.2. we introduce our information?
theoretic Pythagorean condition on the uncertainty set, and identify asymptotically
minimax robust Lorden. Pollak and Bayesian quickest change detection rules. In section
5.3, we present Markov chain and linear statespace system examples and simulation

results. Finally, we provide conclusions in section 5.4.

5.2.2. Data Fusion

In general, the classification of fused data can yield better results than the classification over
single data sources [59]. A promising direction of investigation is given by the nexus between energy
consumption, water consumption, and human presence in a house (also gas metering could be an
additional data source). In an extreme example, a 50% classification between laundry and gardening
could be better disambiguated by the analysis of instant energy consumption given that one of the
two activities uses energy and water at the same time. An example of the nexus between energy and
water is presented in Reference {60} In that paper, the authors leverage electricity non-intrusive load
monitoring (NILM) to acquire water disaggregation as a set of water/ energy correlated states.

5.2.3. Working at Scale

Applying standard rates derived from sample studies is misleading because of the high variability
in water use from one customer to another, even among customers with a similar infrastructure and
social-economic pro?le. The model extracted from a single house?s data is limited, and does not
leverage the information hidden in the broader population. What we consider parameters for a single
house (pipe size, extension of parcel, number of rooms, habits of tenants, ?le.) could be considered
as independent variables in a broader model, comprising a full set of properties in a city or region.
The collection of massive datasets for an entire city or region is, nowadays, technically feasible and
possible to maintain in the long ternL Hence, an interesting research question is to build and evaluate
large-scale models.

6. Conclusions

Non-intrusive water disaggregation is a valuable approach for estimating fixture-specific water
consumption, while keeping installation costs affordable, and, at the same time, the underlying
complexity of processing remains manageable. We have presented a review of water disaggregation
methods that make use of either mono?modal sensing or multi?modal sensing (e.g., combining different
variables, such as water flow, pressure, etc). The result of our review can be summarized in the
following conclusions:

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

5. Discussion on Issues and Future Challenges

Water usage disaggregation is the equivalent of non-intrusive electricity load monitoring, applied
in the water domain, but with an important difference: While electricity outlets can be monitored with
non-invasive, out-of-the-box meters, water fixtures are, in general, unpowered and more difficult to
wire to a data communication infrastructure. This entails battery-operated instrumentation and, in
turn, constrained communication capabilities. Moreover, when dealing with supervised classifiers, a
necessary step is fitting the model with labelled data In the case of water, this may require special
purpose sensors, plumbing, and battery-operated equipment to be installed. Unfortunately, in real
houses, it is not viable to install a flow switch in every fixture or a Closed Circuit Television (CCTV)
camera in every room just to fit the classification model because plumbing is expensive and invasive.
Any viable approach should then comply with the principle of minimal installation requirements, and,
further, any sensors or equipment installed should already be an off-the-shelf product with a high
degree of acceptance among the general public. In summary, we can identify three requirements for
instrumenting a house with sensors:

. High acceptance (design, shape, part of shopping trends, identification of a user need)
. Low cost to buy and install
. Minimal or zero maintenance

All the works described in this survey challenges the previous state-of?the-art against classification
accuracy and are thus built on some hi-tech lab-level setup that requires continuous manual
intervention to ensure a reliable collection and processing of water data and ground truth.
To summarize:

Flow traces analysis [34] requires a data logger to be installed and then data should be manually
collected every 14 days, the data collected are then manually analysed and added to a database. It
seems a feasible solution to analyse a given period of time, but is not practical to perform online
disaggregation. HydroSense [38] reaches an accuracy of 8 ?u, but needs at least two days of ground
truth collectiono Their current approach trains the language model using data from the home where it

unnecessary amount of radio pollution, which is, in turn, is blamed as a potential cancer cause. The
second one is with respect to violations of citizens? rights. Detractors accuse governments of being
driven by the interest of suppliers and that the smart metering roadmap has been laid out without
public consultation, in violation of the spirit of shared consensus and democracy [54]. The last concern,
and probably the one with a proven impact, is that of privacy. There are many examples of how
high-resolution metering could be used to identify personal habits and retrieve personal information.
Notable proof of this concept is shown in Reference [55], where TV programs actually watched by
home occupants is inferred by correlating features such as the luminosity of scenes to high-resolution
energy consumption data. An approach to increase acceptance of industrial-level smart and cognitive
meters a viable solution is twofold:

- Give control to end users (they must be able to switch on/ off the metering; to set up the resolution;
to control the amount of radio messaging inside the property, etc.)

. Locally process most of the data and locally reveal the insights needed by end users to monitor
and improve their water demand. Powerful insight, such as usage disaggregation, could occur
in-home rather than being inferred remotely. This allows to send, to the supplier, only the
strictly-necessary data for operation (for instance daily average consumption over a week).

However, the above-mentioned approach does not take into full account the detailed needs of
water suppliers, as the focus is mainly on user privacy. Thus, as explained in Reference [56], developing
a context-specific framework for assessing how the collection and processing of detailed water usage
impacts the users privacy, and identifying a set of best practices to mitigate the impact is of paramount
importance. We expect that this issue will be addressed as soon as smart metering and cognitive
metering become ubiquitously available for the adaptive management of urban water resources.

52, A Few Promising Research Directions towards Real World Adoption

In this section, some promising directions of further investigation are described. The general
rationale is not to encourage competition in classification techniques to achieve 100% accuracy, but
rather to bridge the gaps for water disaggregation to become a viable tool in real world environments.

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

OPPORTUNITY dataset

The OPPORTUNITY dataset contains data from four subjects, performing six di??erent
runs each of: ADLlfADLS and Drill. In the Drill run, subject must act in a predeterr
mined activity sequence and, as for ADLlfADLS, there is no restriction on the order
and number of activities, For each subject, there is information from three types of
sensors: bodyrworn sensors, object sensors and ambient sensors The onebody sensors
include 7 multiesensor inertial measurement units with another 12 3D acceleration sen?
sors: 145 signals in total, Since only body?worn sensors are concerned in the evaluation
section of the original paper [31], the data from object and ambient sensors are trun?
cated in the following experiments. In terms of activities or classes, this damsel has 3
ditterent sets: 4 types of locomotion (highrlevel activities); 17 types of gesture (midelevel
actions); and lowelevel actions to objects (which is ignored in this work),

Experiments on the OPPORTUNITY dataset

We retrain and evaluate our system using the same experimental setting as in the origi?
nal paper [31]: using ADLz and ADL3 from one subject as the testing set and use Drill,
ADLI, ADL4 and ADLs from the same subject as the training set. We conduct experi?
ments in this con?guration for all four subjects and in the two tasks: highelevel locos
motion (Table 7) and midelevel gestures (Table 8). The ?rst column shows the different
proposed systems, and the best systems are remarked with bold font.

For me highelevel locomotion task (Table 7), ?le system proposed in this paper obtains
?ue best results for all subjects when the Null class is not considered (the 4 last columns).
When including the Null class (the 4 ?rst columns), we obtain the best results for all
subjects except 83.

For the mid?level gesture task (Table 3), the system proposed in this paper obtains
the best results for all subiects except 54 when the Null class is included (the 4 ?rst

Table 7 Experimental results on the OPPORTUNITY dataset (hlgh-level locomntlnn classl-
?cauon)

itallrvaluer imitate the best remit; in this expevlment

columns), ln conclusion, the proposed sysmm is also a competitive solution for home
care monitoring applications,

Conclusions
This paper has proposed a HAR system for classifying 33 di?'erent physical activities
composed of two main modules: feature extraction and activity recognition modules.

The first contribution has been an analysis of several feature extraction strategies:
timerbased and frequencysbased. The timerbased features have provided betmr results
compared to the frequencyrbased ones, This paper has also evaluated several normalis
zation methods for reducing the degradation produced when training and testing with
different users. Thanks to the new feature extraction module and the normalization
strategy, the system has shown strong robustness when facing me Nullractivity and dif
ferent placement scenarios, two vital aspects for real applications.

Regarding the type ofsensor, the magnetometer signals have provided better discrimir
nation capability, The best results have been obtained when combining the information
from all the sensors. ln this case, the improvement is significant. The main experiments
have been done on a public available dataset, REALDISP Activity Recognition dataset.
Final results have exhibited that the proposed system largely improves me performance
compared to previous works on the same damset [24]. Under the best configuration, the
accuracy reaches 99.196 and Frmeasule 0.991.

The proposed system luas been also evaluated with another public dataset (OPPORr
TUNITY dataset) demonstrating competitive results (compared to previous work [31])
in two main tasks for home care monitoring: highrlevel locomotion and midrlevel gess
ture classi?catlon,

 

for training the system is very small (2 out of the 3 subjects recorded in this scenario).
In order to analyze the in?uence of the amount of data, we repeat the same experiments
but using the ideal?p|acement data for training the system. Although there is a mismatch
in the conditions, the amount of available data for training would increase a lot (from 2
to 16 subjects). The experiments are shown in Table 6. In the ?Train Set" and ?Test Set"
columns, we have also included the number of subjects considered for training and mstr
ing the system.

The results show that when being tmined with ideal datasets and tested with mutual
damsets, the system reaches a very good accuracy though the training and testing sets
come from diti'erent placement scenarios. For example, for mutual4, the accuracy goes
from 87.9 to 99.096 (the ?rst row). These results support the hypothesis that the amount
of data for training is an important factor in the system performance.

with the idea of cross?dataset experiment, we go further on me idealrplacement and
sel?placement scenarios (the last row in Table 6). As Table 6 shows, there is not a signi??
cant di?'erence on the accuracy when testing with sel?placement dataset and training
with ideal or self placement (99.1 vs. 98,936, di?'erence lower than the con?dence interval
0.596). In this case, the amount of data available in ideal?placement and sel?placement
scenarios is me same.

System analysis in a new domain: home care monitoring

In the introduction, we commented two main applications of HAR: physical exercise
monitoring and home care monitoring. The REALDISP dataset is focused on the ?rst
one: physical exercise monitoring, In order to verify the viability of the proposed system
in a home care monitoring application, we have evaluated the best system con?guration
with another dataset: the OPPORTUNITY dataset for HAR from wearable, object, and
ambient sensors [30]. The recordings include daily morning activities: getting up from
the bed, preparing and having breakfast (a coffee and a salami sandwich) and clean?
ing the kitchen latter. This dataset is a very popular l-[AR dataset on thls research field.
There is no constraining on the location or body posture in any ofthe scripted activities.

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

number of sensors and devices used in these studies were significantly high, our proposal is focused
on answering the question whether it is possible to obtain similar or better results regarding occupancy
detection using fewer resources. Even more, our proposal uses a non~intrusive ground truth strategy
to avoid jeopardising the privacy and security of the occupants in the area of interest.

This research uses as inspiration some of the ideas proposed in the works discussed, in particular,
the gathering of data from different sources to move forward to a complete analysis of the data collected
using a ML approach. Our goal is to detect occupancy in indoor environments by preprocessing the
datasets collected betore applying ML to a binary and mul?eclass problem. Additionally we design our
solution focused on two main requirements: the first one is to try to take advantage of cheap/affordable
devices commonly deployed in Smart Environment, and the second one is to guarantee that the privacy
of the occupants in the area under study would not be compromised, Thus, after the discussion of
the works in this research area, our proposal uses environmental features via the combination of data
gathered from different sensors. This solution is presented in the following section.

3, occupancy Detection in Indoor Environments

As discussed in the previous section, occupancy detection could be used to trigger some actuation
mechanisms in Smart Environments in order to improve resource usage and user experience, among
other factors, An important issue that must be considered is the preservation of privacy of the data
collected and analysed. Additionally, it would be desirable to take advantage of the infrastructure
available in the surroundings to avoid incurring in extra expenses, while allowing the scalability of the
solution. Considering these factors, this research is focused on a nonintrusive and inexpensive solution
for occupancy estimation that ensures occupants privacy wlule taking advantage of the technological
infrastructure already available in common Smart Environments including Smart Buildings and Smart
Homes. From the analysis performed on Section 2, and to comply with the previously established
requirements, our occupancy detection solution is focused on environmental data.

A scene analysis approach is used in this research to extract the features of interest for indoor
scenarios to proceed and then toest-imate the occupancy in the area using the gathered data [24]. The
scene analysis method does not rely on any theoretical model or specific hardware; however, it requires
a preliminary phase for capturing features which are in?uenced by changes in the area of interest [25].

in this section, we explain the criteria applied to select the features used in our solution before
moving forward to the description of the design of the four-layers architecture adopted for the
gathering and processing of the data. The section concludes with the discussion of the ML classifiers
that were selected to improve the performance of occupancy detection in indoor environments. Table 1
summarises the terms used in the remaining of the manuscript.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

Activity recognition
The goal of activity recognition is to recognize common human activities in real life

settings. Accurate activity recognition is challenging because human activity is complex
and highly diverse. Several probability-based algorithms have been used to build activity

 

models. The Hidden Markov Model and the Conditional Random Field are among the
most popular modeling techniques. We describe these two techniques in the context of an
eating activity example.

The Hidden Markov Model (HIVIM)

Simple activities can be modeled accurately as Markov Chains. However. complex or
unfamiliar activities are o?en dif?cult to rnrderstand and model. For example. a
researcher studying activities of daily living for a person vtu'th dementia will have a
dif?cult time ?tting a model unless she is an expert in dementia and turderstands its
related behavioral science. Fortunately. observing signals stemming from complex or
unfamiliar activities can be utilized to indirectly build a model of the activity. Such a
model is called a Hidden Markov Model or HMM. By observing the effects of an activity.
HMM is able to gradually construct the activity model. which can be further tuned.
extended and reused in similar studies.

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

Online learning is the process of answering a sequence of questions given knowledge of the correct
answers to previous questions and possibly additional available information Answering questions
in an intelligent fashion and being able to make rational decisions as a result is a basic feature of
everyday life. Will it rain today (so should I take an umbrella)? Should I ?ght the wild animal that
is after me, or should 1 mn away?? Should I open an attachment in an email message or is it a virus?
The study of online learning algorithms is thus an important domain in machine learning, and one
that has interesting theoretical properties and practical applications.

This dissertation describes a novel framework for the design and analysis of online learning
algorithms. We show that various online learning algorithms can all be derived as special cases of
our algorithmic framework. This uni?ed view explains the properties of existing algorithms and
also enables us to derive several new interesting algorithms.

Online learning is perforated in a sequence of consecutive rounds. where at each round the
learner is given a question and is required to provide an answer to this question. After predicting an
answer. the correct answer is revealed and the learner suffers a loss ifthere is a discrepancy between
his answer and the correct one.

The algorithmic framework for online learning we propose in this dissertation stems from a
connection that we make between the notions of regret in online learning and wr'ak duality in convex
optimization. Regret bounds are the common thread in the analysis of online learning algorithms.
A regret bound measures the performance of an online algorithm relative to the performance of a
competing prediction mechanism, called a competing hypothesis. The competing hypothesis can
be chosen in hindsight from a class of hypotheses. after observing the entire sequence of question?
answer pairs. Over the years, competitive analysis techniques have been re?ned and extended to
numerous prediction problems by employing complex and varied notions of progress toward a good

competing hypothesis.

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

for an optimization problem. in which we search for the optimal competing hypothesis. While the

optimal competing hypothe.

 

can only be found in hindsight, after observing the entire sequence
of question~answer pairs. this viewpoint relates regret bounds to lower bounds of minimization
problems.

The notion of duality. commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem. By generalizing the
notion of Fenchel duality. we are able to derive a dual optimization problem. which can be opti~
mized incrementally, as the online learning progresses. The main idea behind our derivation is the
connection between regret bounds and Fenchel duality. This connection leads to a reduction from
the process of online learning to the task of incrementally ascending the dual objective function.

In order to derive explicit quantitative regret bounds we make use of the weak duality prop?
erty. which tells us that the dual objective lower bounds the primal objective. The analysis of our
algorithmic framework uses the increase in the dual for assessing the progress of the algorithm.
This contrasts most if not all previous works that have analyzed online algorithms by measuring the
progress of the algorithm based on the correlation or distance between the online hypotheses and a
competing hypothesis.

We illustrate the power of our framework by deriving various learning algorithms. Our frame?
work yields the tightest known bounds for several known online learning algorithms. Despite the
generality ofour framework, the resulting analysis is more distilled than earlier analyses. The frame?
work also serves as a vehicle for deriving various new algorithms. First. we obtain new algorithms
for classic prediction problems by utilizing different techniques for ascending the dual objective.
We further propose ef?cient optimization procedures for performing the resulting updates of the
online hypotheses. Second, we derive novel algorithms for complex prediction problems, such as

ranking and structured output prediction.

This introduction presents an overview of the online learning model and the contributions of this
dissertation. The main concepts introduced here are covered in depth and more rigorously in later

chapters.

1.1 Online Learning

Online learning takes place in a sequence of consecutive rounds. On each round. the learner is
given a question and is required to provide an answer to this question. For example. a learner might
receive an encoding of an email message and the question is whether the email is spam or not.
To answer the question, the learner uses a prediction mechanism, termed a hypothesis. which is a
mapping from the set of questions to the set of admissible answers. After predicting an answer.
the learner gets the correct answer to the question. The quality of the leamer?s answer is assessed
by a loss function that measures the discrepancy between the predicted answer and the correct one.
The learner's ultimate goal is to minimize the cumulative loss suffered along its run. To achieve
this goal. the learner may update the hypothesis after each round so as to be more accurate in later
rounds.

As mentioned earlier, the perfomtance of an online learning algorithm is measured by the cu~
mulative loss suffered by the learning along his run on a sequence of question-answer pairs. We
also use the term example to denote a question?answer pair. The learner tries to deduce information
from previous examples so as to improve its predictions on present and future questions. Clearly,
learning is hopeless if there is no correlation between past and present examples. Classic statistical
theory of sequential prediction therefore enforces strong assumptions on the statistical properties of

the input sequence (for example, it must form a stationary stochastic process).

Our framework emerges from a new View on regret bounds, which are the common thread in
the analysis of online learning algorithms. As mentioned in Section 1.1, a regret bound measures
the performance of an online algorithm relative to the performance of a competing hypothesis. The
competing hypothesis can be chosen in retrospect from a class of hypotheses. after observing the
entire sequence of examples.

We propose an alternative View of regret bounds that is based on the notion of duality in con?
vex optimization. Regret bounds are universal in the sense that they hold for any possible ?xed
hypothesis in a given hypothesis class We therefore cast the universal bound as a lower bound for
an optimization problem. Speci?cally, the cumulative loss of the online learner should be bounded
above by the minimum value of an optimization problem in which we jointly minimize the cu?
mulative loss and a ?complexity" measure of a competing hypothesis. Note that the optimization
problem can only be solved in hindsight after observing the entire sequence of examples. Neverthe?
less. this viewpoint implies that the cumulative loss of the online learner fomts a lower bound for a
minimization problem.

The notion of duality, commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem (see for example [89]).
By generalizing the notion of Fenchel duality, we are able to derive a dual optimization problem,
which can be optimized incrementally as the online learning progresses. In order to derive explicit
quantitative regret bounds we make immediate use of the weak duality property, which tells us that
the dual objective lower bounds the primal objective. We therefore reduce the process of online
learning to the task of incrementally increasing the dual objective function. The amount by which
the dual increases serves as a new and natural notion of progress. By doing so we are able to
associate the cumulative loss of the competing hypothesis (as re?ected by the primal objective

value) and the cumulative loss of the online algorithm, using the increase in the dual.

1.2.2 Problem Type

The Perceptron algorithm was originally designed for answering yes/no questions. In real?world
applications we are often interested in more complex answers. For example, in multiclass categov
rization tasks, the learner needs to choose the correct answer out of k possible answers.

Simple adaptations of the Perceptron for multiclass categorization tasks date back to Kessler's
construction [44]. Crammer and Singer [31] proposed more sophisticated variants of the Perceptron
for multiclass categorization. The usage of online learning for more complex prediction problems
has been further addressed by several authors. Some notable examples are multidimensional regres?
sion [76], discriminative training of Hidden Markov Models [23]. and ranking problems [28, 29].

1.2.3 Aggressiveness Level

The update procedure used by the Perceptron is extremely simple and is rather conservative. First,
no update is made if the predicted answer is correct. Second. all instances are added (subtracted)
from the weight vector with a unit weight. Finally. only the most recent example is used for updating
the weight vector. Older examples are ignored.

Krauth [78] proposed aggressive variants of the Perceptron in which updates are also perfonned
if the Perceptron's answer is correct but the input instance lies too close to the decision boundary.
The idea of twing to push instances away from the decision boundary is central to the Support
Vector Machines literature [1 17. 33, 100]. In addition. various authors [68. 57. 80, 74. 103. 31, 28]
suggested using more sophisticated learning rates. i.e., adding instances to the weight vector with
different weights.

Finally. early works in game theory derive strategies for playing repeated games in which all
past examples are used for updating the hypothesis. The most notable is followfthevleader ap-
proaches [63].

In most of this dissertation we make no statistical assumptions regarding the origin of the se?

quence of examples. We allow the sequence to be determini

 

stochastic. or even adversarially
adaptive to our own behavior (as in the case of spam email ?ltering). Naturally. an adversary can
make the cumulative loss of our online learning algorithm arbitrarily large. For example, the adver?
sary can ask the same question on each online round. wait for the learner?s answer, and provide the
opposite answer as the correct answer. To overcome this de?ciency. we restate the learner's goal
based on the notion of regret. To help understand this notion, note that the learners prediction on
each round is based on a hypothesis. The hypothesis is chosen from a prede?ned class of hypothe?
ses. In this class, we de?ne the optimal ?xed hypothesis to be the hypothesis that minimizes the
cumulative loss over the entire sequence of examples. The learner's regret is the difference between
his cumulative loss and the cumulative loss of the optimal ?xed hypothesis. This is termed "regret?
since it measures how 'sorry' the learner is. in retrospect, not to have followed the predictions of the
optimal hypothesis. In the example above, where the adversary makes the learners cumulative loss
arbitrarily large, any competing ?xed hypothesis would also suffer a large cumulative loss. Thus,
the learner?s regret in this case would not be large.

This dissertation presents an algorithmic framework for online learning that guarantees low
regret. Speci?cally. we derive several bounds on the regret of the proposed online algorithms. The
regret bounds we derive depend on certain properties of the loss functions, the hypothesis class, and

the number of rounds we run the online algorithm.

1.2 Taxonomy of Online Learning Algorithms

Before delving into the description of our algorithmic framework for online learning. we would like

to highlight connections to and put our work in context of some of the more recent work on online

Section 5

Object Detection from video

The motion of object can be detected after the object is detected from Video. Trackmg the 301?le
or object from sequence Video frames this is the main goal of Video tracking. Blob tracking.
keinel-based tracking. Contour tracking are some common target representation and localization
algorithms. Ruolm Zhang [11] has proposed adaptive background subtraction about the Video
detecting and tracking moving object. He use median ?lter to achieve the backgroiuid subtraction,
This algorithm is used for both detecting and tracking moving objects in sequence of video. This
algorithm never suppoit for multi feature based object detection. Hong Lu and Hong Slieng Li
[12] were introduced a new approach to detect and track the moving object. The defme motion
model and the non-parameter distribution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kahnan filter estnnati g
its a?'me motion in next frame. The author shows Experimental results and proof the new method
can successfully track the object iuider such case as merging. splitting. scale Variation and scene
noise, The author Bayan [13] talks about adaptive mean shirt for automated multi tracking. The
bene?t of Gaussian mixture model is that it extracted Foreground image from video Er ine
sequence it also eliminate the shadow and noise from video sequence It is helpful in initiali 'ng
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from Video and hence we can track the object easily. The object can trap from
Video by changes in size and shape

In this paper section] gives introduction about HAR and gives motto of paper: over view of
traditional HMM classifier in section 2. Section 3 gives overview about HMM-based approach
that uses threshold aird voting and section 4 gives over view about HZMM-NN and NN-HMM.

Section 5 contaim the review of how object can be detected from other ways. The result of all
methods as conclusion in section 6 Sections ' contain references

Section 2

TRADITIONAL HMM (TLAssmIR

A hidden Markov model (HIVIM) is a statistical Markov model in which the system being
modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be
considered as the simplest dynamic Bayesian network. The logic behind the HMM was
developed by L. E. Baum and coworkers. It is nearly dependent on an earlier work on optimal
nonlinear ?ltering problem (stochastic processes) proposed by Ruslan L Stratonovich. who was
the ?rst to describe the forward-backward procedure [14]

In a regular Markov model. the state is directly visible to the observer. and therefore the state
transition probabilities are the only parameters. In a hidden Markov model. the state is not
directly visible. but output. dependent on the state. is visible. Each state has a probability
distribution over the possible output tokens. Therefore the sequence of tokens generated by an
HMM gives some infoiination about the sequence of states. Note that the word ?hidden' is refers
for the state sequence through which the model is passes. not for the parameters of the model.

Hidden Markov models are especially known for then" application in temporal pattern recognition
such as speech. handwriting. gesture recognition. pai?t-of-speech tagging. musical score following
partial discharges and bioinformatics

A hidden Markov model can be considered a generalization of a mixture model where the hidden
variables (or latent variables). which control the mixttu?e component to be selected for each
obseivation. are related through a Markov process rather than independent of each other.

PAC learning framework. For completeness, in Chapter B given in the appendix. we discuss the
applicability of our algorithmic framework to the PAC learning model. We start this chapter with
a short introduction to the PAC learning model. Next, we discuss the relative dif?culty of online
learning and PAC learning. Finally. we propose general conversion schemes from online learning
to the PAC setting.

1.4.2 Part II: Algorithms

The second part is (levoted to more speci?c algorithms and implementation details. We start in
Chapter 5 by deriving speci?c algorithms from our general algorithmic framework. In particular?,
we demonstrate that by varying the three components of the general framework we can design
algorithms with different update types, different aggressiveness levels. and for different problem
types.

Next. in Chapter 6 we show the applicability of our analysis for deriving boosting algorithms.
While boosting algorithms do not fall under the online learning model. our general analysis ?ts
naturally to general primal?dual incremental methods. As we discuss, the process of boosting can
be viewed as a primal~dual game between a weak learner and a booster.

Finally. in Chapter 7 we discuss the computational aspects of the different update schemes.
Depending on the loss function and update scheme at hand, we derive procedures for performing

the update with increasing computational complexity.

1.4.3 Part III: Applications

In the last part of the dissertation we demonstrate the applicability of our algorithms to real world

problems. We start with the problem of online email categorization. which is a natural online

Appendix B

Using Online Convex Programming for
PAC learning

In this chapter we outline the applicability of our algorithmic framework from the previous chapter
to Probably Approximately Correu (PAC) learning [116]. The PAC setting is also referred to as
hart/1 learning. We start this chapter with a short introduction to PAC learning. Next, in Section B2
we relate two important notions of learnahiliry: the notion of mistake bound in online learning
and the notion of VC dimension used in PAC learning. Based on this comparison. we demonstrate
that online learning is more dif?cult than PAC learning. Despite this disparity. as we have shown
in previous chapters. many classes of hypotheses can be ef?ciently learned in the online mode].
In Section B} we derive several simple conversions from the online setting to the batch setting.
The end result is that the existence of a good online learner implies the existence of good hatch
learning algorithms. These online?to?batch conversions are general and do not necessarily rely on
our speci?c algorithms for online learning.

B.1 Brief Overview of PAC Learning

hold as long as we have a suf?cient increment in the dual objective. By monitoring the increase in
the dual we are able to control the aggressiveness level of the resulting online learning algorithm.
To make this dissertation coherent and due to the lack of space, some of my research work was
omitted from this thesis. For example, I have also worked on boosting and online algorithms for
regression problems with smooth loss functions [SE]. 40]. online learning of pseudovmetrics [109],
online learning ofprediction suf?x trees [39], online learning with various notions of margin [103],
online learning with simultaneous projections [4], online learning with kernels on a budget [41],

and stochastic optimization using online learning techniques [1 10].

1.4 Outline

The dissertation is divided into three main parts. titled Theory. Algorithms, and Applications. In
each part, there are several chapters. The last section of each of the chapters includes a detailed

review of previous work relevant to the speci?c contents described in the chapter.

1.4.1 Part 1: Theory

In the theory part. we derive and analyze our algorithmic framework in its most general form We
start in Chapter 2 with a formal description of online learning and regret analysis. We then describe
a more abstract framework called online convex programming and cast online learning as a special
case of online convex programming. As its name indicates, online convex programming relies on
convexity assumptions. We describe a common construction used when the natural loss function for

an online learning task is not convex.

Abstract

Smartphones have become a global cormnunication tool and more recently a teclnrology for studying hrrrnan
behavior. Given their numerous built-in sensors. smartphones are able to capture detailed and continuous
observations on activities of daily living. However. translation of measurements from these consumer-grade
devices into research-grade physical activity patterns remains challenging. Over the years. researchers have
proposed various human activity recognition (HAR) systems which vary in algorithmic details and statistical
principles. In this paper, we summarize existing approaches to srnartphone-based HAR. We systematically
screened the literatru'e on Scopus. PubMed. and Web of Science in the areas of data acquisition. data
preprocessing, feature extraction. and activity classi?cation. We ultimately identi?ed 72 articles on
srnartphone-based HAR. To provide an understanding of the literature. we discuss each of these areas
separately, identify the most cormnon practices and their alternatives. and propose possible future research

directions for this interesting and important ?eld.

Keywords

Wearable computing; accelerometer; gyroscope: data acquisition: data processing: feature extraction:

activity classi?cation: digital plrenotyping machine learning: pattern recognition.

1.6 Bibliographic Notes

How to predict rationally is a key issue in various research areas such as game theory. machine
learning. and information theory. In this section we give a high level overview of related work in
different research ?elds. The last section of each of the chapters below includes a detailed review
of previous work relevant to the speci?c contents of each chapter.

ln game theory. the problem of sequential prediction has been addressed in the context of playing
repeated games with mixed strategies. A player who can achieve low regret (i.e. whose regret grows
sublinearly with the number of rounds) is called a Harman consistent player [63]. Hannan consistent
strategies have been obtained by Harman [63]. Blackwell [9] (in his proof of the approachability
theorem). Foster and Vohra [49, 50], Freund and Schapire [55]. and Hart and Mas?collel [64]. Von
Neumann?s classical minimax theorem has been recovered as a simple application of regret bounds
[55]. The importance of low regret strategies was further ampli?ed by showing that if all players
follow certain low regret strategies then the game converges to a correlated equilibrium (see for
example [65, 10]). Playing repeated games with mixed strategies is closely related to the expert
setting widely studied in the machine learning literature [41 82, 85, 119].

Prediction problems have also intrigued information theorists since the early days of the in?
formation theory ?eld. For example. Shannon estimated the entropy of the English language by
letting humans predict the next symbol in English texts [I l 1]. Motivated by applications of data
compression, Ziv and Lempel [124] proposed an online universal coding system for arbitrary in-
dividual sequences. In the compression setting. the learner is not committed to a single prediction
but rather assigns a probability over the set of possible outcomes. The success of the coding system
is measured by the total likelihood of the entire sequence of symbols. Feder, Merhav. and Gutman
[47] applied universal coding systems to prediction problems. where the goal is to minimize the

number of prediction errors. Their basic idea is to use an estimation of the conditional probabilities

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

approaches. The available studies both use existing methods and propose new methods for collection.
processing. and classi?cation of activities of daily limig. Authors commonly discuss data ?ltering and
feature selection techniques and compare the accuracy of various machine learning classi?ers either on
previously existing datasets or on datasets they have collected de ?um for the purposes of the specific study.

The results are typically siunmarized using classi?cation accuracy within different groups of activities
like ambulation. locomotion. and exercise. This paper aims to summarize recent efforts in smartphone-based
HAR research With the goal of providing an understanduig of the contextual complexity and

innltrdunensionality of the problem. the collected data. and the methods used to translate the digital

 

measurements into human activ

2. Methods

Oiu' systematic review was conducted by searching for articles published by June 30. 2019. on PubMed,
Scopus. and Web of Science databases. The databases were screened for titles. abstracts. and keywords
containing phrases ?acti\ity" AND ("recognitiorf OR ?estimation" OR "classi?cation") AND
(?srnai?tplione" OR ?cell phone" OR ?mobile phone?). The search was limited to full-length journal articles
written in English. A?er removing duplicates. we read titles and abstracts of the remaining publications.
References that did not uivestigate HAR approaches were excluded ?'oni further screening. In the following

step. we filtered out studies that employed auxiliary equipment. like wearable or ambient devices. and

algorithms. Therefore. only 292 references were assigned for full reading. of which 121 references that
employed additional hardware were excluded together with an additional 99 references that utilized built-in
microphones or' video cameras. Additionally, we excluded studies with sruai?rphones af?xed to the human
body. The remaining "2 references were read in detail.

Most HAR approaches consist of four stages. namely data acquisition. dam prepracasiing. feature
attraction, and activity classification. hi the follomirg. We provide an overview of these steps and brie?y
ponit to signi?cant methodological differences among the studies. Table 1 summarizes speci?c aspects of
each study: we have decomposed data acquisition process to sensor type. experimental envn'onruent.
mvestigated activities. and selected sinartphone location: We have also indicated which studies prepr'ocess
collected measurements using signal correction methods. sensor orientatioir-invar?iant transformations. and
noise filtering techniques. we have marked investigations due to types of sigma] features they extract. as
well as due to the feature selection approaches: fmally. we have indicated the adopted activity classi?cation
principles. utilized classi?ers. and practices for accuracy reporting. Figure 3 displays the most ?'equent

terms used in the included studies.

3.1. Data acquisition
We use the term data acquisition to refer to a process of collecting and storing raw sub-second level
srnartplioue measiu'ements for the purpose of HAR. The data are typically collected by a program or

application that runs on the device and samples data from built-m srnanphone sensors within predefined

1. Introduction

According to the GSM Association. there were roughly 2 billon srrrartplrones in use in 2019. and this number
is expected to dorrble in the next couple of years [1]. Such explosion iir worldwide snraitphone adoption
presents unprecedented opportunities for the study of hrunarr behavior. Smartphones now coirtairr rrrultiple
sensors to captru'e detailed. contnruous. and objective measurements of lrrurran behavior. including on
mobility and physical activity. Along with sufficient storage. powerful processors. and Wireless
transrrrission. such data cart be obtained Without additional hardware or instrumentation. which makes it
feasible to study large cohorts of subjects over extended tnne periods. Irrrportantly. smartphones are not a
niche product. as appears to be the case with most wearable activity trackers [2]. but instead they have
becorue a globally available technology. increasingly adopted by users of all ages both in advanced aird
ernergmg economies [3.4]. While these technological developments make the task of data collection easier.
analysis of the collected data is increasingly identified as the rrrarn bottleneck in research settings [5?"]. and
therefore it appears that the mam challenge iir human activity recognition (HAR) is now shi?ing from data
collection to statistical methodology and pattern recognition.

This proliferation of smartphones has riot goire unnoticed by the research comnrunity. At the time of
writing. there were nearly 300 articles published on HAR methods using srnartphones. a substantial increase
from just a handful of articles published a few years earlier (Figure 1). This growmg interest has takeir place

across various fields such as security aird surveillance [8]. personal navigation [9]. and health monitoring

individuals Less effort has been devoted to investigate populations with different demographic and disease
characteristics. such as elders [12] and subjects with Parkinson's disease [10]. As an example of a larger
study. Kelishomi et al. [13] analyzed data from 480 healthy individuals.

In the reviewed papers. data collection typically takes place in a research facility and or nearby outdoor
surroundings. In such environments. study participants are asked to perform a series of activities along
prede?ned routes and to interact Wllll prede?ned objects. The duration and order of performed activities are
usually determined by the study protocol and the subject is supervised by a research team member. A less
popular approach involves observation conducted in free-living emirouiuents. where participants perform
activities Without speci?c constraints. Such studies are likely to provide ruore insight into diverse activity
patterns due to individual habits and unpredictable real-life conditions. Compared to a single laboratory
visit. it also allows urvestigatois to monitor behavioral patterns over many weeks [14] or months [15].

Activity selection is one of the key aspects of HAR The studies considered here tend to focus on a
small set of activities. including sitting. standing. walking. running. and stair clunbing. The less common
activities involve various types of mobility. locomotion. fitness. and household routines. For instance. Wu
et al. [16] differentiate between slow. noirnal. and brisk walking: Guvensan et al. [1'] investigate multiple
transportation modes. like car. bus. train. train. metro. and ferry: Pei et al. [18] recognize sharp body-turns:
and Della Mea et al. [19] look into household activities. like sweeping a ?oor or walking with a shopping
bag. In Table 1. ?post\u?e" refers to lying. sitting. standing. or arty pair of these activities: ?mobility" refers

to Walking. stair climbnrg. body trims. riding elevator. or escalator. running. cycling. or airy pair of these

 

results. In the reviewed literatiu'e. subjects were o?en instructed to caiiy the device in pants pocket (either
front or back). although a number of studies also considered other placements. such as Jacket pocket [30].
bag or backpack [31]. and holding the sinaitphone in hand [32].

To establish the ground mith for physical activity in HAR studies. the data are usually annotated
manually by trained research personnel or by subjects themselves [33.34]. However. We also encoiuitered
several approaches that automate this process both in controlled and free-living conditions. For instance. in
[14] the data were labelled using a designated sniartphone application. A different approach was proposed
in [35]. where authors used a built-in step coiuiter to produce "Weak" labels. Also. the annotation can be
done usuig built-in microphone [36] and video camera [12].

Finally. the data acquisition process is carried out on purposely designed applications which captiu'e
and transmit data to the external server using cellular. Wi-Fi. Bluetooth. or wired connection. In online
activity classi?cation. the collected data do not leave the device but instead the entire HAR pipeline is

implemented on the smartphoue.

3.2. Data preprocessing
We use the term data prepi'ocessing to refer to a collection of procediu'es auned at repairmg. cleaning. and
transfoiining measiu'enients recorded for HAR. The need for such step is threefold: (1) nieasiu?ement

systems embedded in smartpliones are often less stable than research-grade data acquisition units. and the

which is possibly due to the temporally dense. high-resolution measurements they provide for distinguishing
among activity classes. The inertial sensors are o?en used synchronously providuig more insight into the
dynamic state of the device. Some stridies show that the use of a smgle sensor can yield similar activity
recognition results [20]. To alleviate the impact of sensor position. researchers collect data using built-m
barometer and GPS sensors to investigate changes in altitude and geographic location [21.22]. Certain
approaches bene?t from the broader set of capabilities of sinaitphones. and the researchers may additionally
exploit proximity and light sensors which allow the recognition of a nieasiu?ement?s context. e. g.. the

distance between siiiartphone and subject's body. and changes between in-pocket and out-of-pocket

 

locations based on chang' g illumination [23.24]. The selection of sensors is also affected by secondaiy
research goals. like simplicity of classi?cation and minimization of battery drain. In such approaches. data
collection is carried out on a single sensor (e.g. accelerometer [l4]). a small group of sensors (e.g..
accelerometer and GPS [25]). or with purposely modi?ed sampling frequency to reduce the amount of data
collected and processed [26].

The sampling frequency describes how many observations are collected by a particular sensor in one
second. The selection of sampling frequency is usually performed as a trade-off between measurement
accuracy and battery drain. In a typical data acquisition setting. the sampling frequency ranges between 20

to 30Hz for inertial sensors and l to lOHz for barometer and GPS. The most significant variations from this

description are required for inertial sensors if limited energy consumption is a pi'ioirty (e.g.. accelerometer

3.3. Feature extraction
We use the term feature extraction to refer to a process of selecting and computing meaningful summaries
of smartphone data for the goal of activity classification. A typical extraction scheme includes data
Visualization. data segmentation. featiu?e selection. and feature calculation. A careful featiu'e extraction step
allows urvestigators not only to understand the physical nature of activities and its manifestation in digital
measurements. but more importantly also helps uncover hidden structures and patterns in the data. The
identified differences are later quanti?ed through various statistical measures to distinguish among
activities. In an alternative approach, the entire process of feature extraction is automated rising deep
learning. which handles both segmentation and feature selection. On the other hand. and as with most
applications of deep leaniing. this often results in loss of irrteipr?etability and limited control over the process.
The conventional approach to feature extraction begins with data exploration. For this purpose.
researchers employ various graphical techniques. like scatter plots. lag plots. autocoirelations plots.
histograms. and power spectra [44]. The choice of tools is often dictated by the study objectives and
methods. For example. research on inertial sensors typically presents raw three-dimensional data from
accelerometers. gyroscopes. and magnetometers plotted for the corresponding activities of standing,
walking. stair climbing. etc. A similar approach is used for barometric pressiu'e data Acceleration data are
o?en inspected in the frequency-domain. particularly to observe periodic motions of walking. running. and

cycling [29]. and the impact of external environment. like natural vibration frequencies of a bus or a subway

 

frequency. Derawi and Bours [8] propose the use of linear mterpolation. while Gu et al. [3 7] utilize spline
interpolation. Such procedures are imposed on a range of affected sensors. typically including
accelerometer. gyroscope magnetometer. and barometer, Frnther time-domain preprocessing considers data
trimming. carried out to remove unwanted data components. For this purpose. the beginning and end of each
activity borrt are clipped as nonrepresentative for the given activity [30]. where a bout refers to a short period
of activity of a specified kind. During this stage the researchers also deal with dataset imbalance. The
imbalance occurs when observations of one activity signi?cantly dominate over others. Such situation
makes the classi?er susceptible to ovei?ttnig in favor of the larger class: however. the issue might be solved
by up- or downsamplmg of data [13.38]. Additionally. the rneasru'ements are processed for high-frequency
noise cancellation (m Table 1. see ?denoising?). The literature identifies several methods suitable for serving
this task. includnig the use of low-pass ?nite impulse response ?lters (with cut-off frequency typically equal
to 10Hz for inertial sensors and 0.1Hz for barometers) [39.40]. weighted monng average [8]. moving
median [29]. and singular value decomposition [41].

Another element of data preprocessmg considers device orientation (in Table 1. see ?tr"ansformation").
Sniartphone measurements are sensitive to device orientation. which may be due to personal choices.
clothing. body shape. and movement during dynamic activities [38]. One of the popular solutions is to
transform the three-dimensional signal into univarrate vector magnitude which is invariant to rotations and

more robust to translations. This procedure is often apphed to accelerometer. gyroscope. and magnetometer

ABSTRACT."

The rapid nnprovement m teahnologv Causes more attention towards to Remgnrzmg ofhuman aenvmes
?'om video. These new teehnolagreaz growth has made vision?based research much more mterestrng anal
e/?ctent than ever before. ihts paper present navel HMM (Hmaen Markov Model) based approach for
Human amwty r-eaagnraon from video. zhere are different approaches ofHMM to rerogn'ue ac?mi of
human from video. ere threshold and voting to automanmlly ana e?eetrvezy segment and reeogntze
sampler aanwtres, segment and recognwe eampten aatwmes and ?u srmple aammes we use Elmnn
Network (EN) and two hyanas ofNeuraINetwork (NN) andHll/lM, re. HMM?NN and NNI?VIM.

KEY Worms:

Human Activity reaagnmon, Hidden Markov Model, Hybrid model afHMM, Image mpmrmg from Video,
complex activity,

 

TRODUCTI

 

Automatically recognizing human activities from video is important for applications such as
automated suiyeillance systems and smart home applications. Several human activity recogn' 'on
methods [l][2][3][4][5][6] were proposed in the past few years to classify single human actitities
such as walking skipping. sitting down. etc. Human activity recognition (HAR) research has
been on the use because of the rapid technological development of the image-capturing software
and hardware. in addition to the omnipresence of reasonably low-cost high-performance personal
computers. The main goal of tlns recognition is used to develop the different application which
make human machine interaction is easy and interesting.

     

In the jouiney of developing algorithms for human activity recognition. some new developed
algorithms adds some new features in pre?ously developed algorithm. In this paper. we present a
novel HMM-based approach that uses threshold and wring to automatically and effectively
segment and recognize complex activities. And also survey on two hybrids of Neiu?al Network
(NN) and HMM. e. HMNI?NN and 3 ??]-]]\IM. This paper also compares their perfoimance
with that of the traditional HMM.

   

select the optimal window size. which emphasizes the inipoitance of this parameter to the performance of
HAR systenis [46?48]. This calibration aims to closely match the window size with the duration of a single
mstance of the activity. Smiilar motivation leads researchers to seek more adaptable segmentation methods.
One idea is to segment data based on speci?c tune-domain events. like zero-cross points. peak points. or

valley points. which represent the start and end points of a particular? activity bout [8.38]. This allows for

 

segments to have different lengths corresponding to a single fundamental period of the acti ity in question
Such approach is typically used to recognize quasipeiiodic activities like walking. running. stair climbing.
and sitting and standing [41].

The literature offers a large variety of signal features used for HAR. Such features can be divided into
several categories based on the initial signal processing procedure. This enables one to distinguish between
activity templates (i.e.. raw signal). time-domain features. and frequency-doinaui features. The most popular
features in the reviewed papers are calculated from time-domain signals as descriptive statistics. such as
local mean. variance. minimum and maximum. interquartile range. energy. and higher order statistics. Other
time-domain features include mean absolute deviation. mean (or zero) crossing rate. regression coef?cients.
and autocorrelation. Some studies describe novel and customized time-domain features. like histogram of
gradients [49]. magnitude of standard deviations [50]. number of local maxima and minima. their airiphtiide
and temporal distance between them [26] The described time-domain features are typically calculated over

each axis of the three-dimensional measurement or oiieiitation-invariant vector magnitude. Studies that use

 

iiniltidirneusionahty of smartplione measurement and data analysis. and offer guidelines and direction to

anyone interested in this challenging but inipoitant topic.

Acknowledgements

We are grateful to C ipr'ian Craiuiceanu. J aroslavv Harezlak. Emily Huang. and Greyson Liu for their careful

reading of our manuscript and then" insightful feedback.

Funding sources

The authors were supported by NIH?NHLBI award U01HLl453 86.

Author biographies

Martin Straczkiewicz is a Postdoctoral Research Fellow in the Department of Biostatistics at Harvard
University. His research is focused on developing novel statistical methods for quanti?cation of human
movements using wearable devices. primarily accelerometers and smanphones.

J'P Onnela is Associate Professor of Biostatistics at Harvard University. He obtained his doctorate in
Finland and subsequently completed fellowships at the University of Oxford. Harvard Kennedy School. and
Harvard Medical School. His main interest is in developing quantitative methods in statistical network

science and digital phenotyping.

Activity templates function essentially as blueprints for different types of physical activity. In HAR
systems. these templates are compared to patterns of observed raw measurements using various distance
metrics [25], Given the heterogeneous nature of human activity. activity templates are often enhanced rising
techniques similar to dynamic time warping [38]. As an alternative to raw measurements. sortie studies use
signal symbolic approximations created by discretization functions that transform data segments into
symbols [5. .54],

In the reviewed articles. the number of extracted features typically varies from a few to a dozen.
However. some studies purposely calculate too many features (sometimes hundreds) and let the analytical

method identify those that are most relevant and informative to HAR, Support vector machines [52]. gam

 

ratio [55]. recursive feature elimination [ ]. correlation-based feature selection [33]. and principal
component analysis [56] are among popular feature selection dimension reduction methods. A comparison

of feature selection methods is prorided by Saeedi and El-Sheimy [57].

3.4. Activity classi?cation

We use the term activity classi?cation to refer to a process of associating extracted features with par1icular
activity classes based on the adopted classi?cation principle. The classification is typically performed by a
supervised learning algorithm that has been trained to recognize patterns between features and labeled

physical activities. The fitted model is then validated on separate observations. usually using data from the

conscientious feature selection [29.54]. Computation time is sometimes repoited for complex methods. such

 

as deep neural networks [66]. extreme learning machine [67]. or synnbolic representation [53 .68]. as Well
as in comparative analyses [30]. Nevertheless. a comprehensive coinpaiison of results is dif?cult or

impossible as discussed next.

4. Discussion

Over the past few years many studies have investigated HAR using smartphones. The reviewed literature
provides detailed descriptions of essential aspects of data collection. data processing. and activity
classification. The studies have been conducted with one or more objectives. e.g.. to limit technological
imperfections (e.g.. no GPS signal reception indoors). to minimize computational requirements (e.g.. online
systems). and to maximize classi?cation accuracy (all studies). Oiu' review summarizes most frequently
used methods and offers available alternatives. We do not however identify any one ultimate activity
recognition procedure. and we doubt that one even exists. This results in pan from the complexity of the
task. Different studies use different activities. signal processing techniques. and classifiers. and each likely
suffers from speci?c potential drawbacks.

Some of our concerns relate to the quality of the collected data. While datasets are usually collected in
laboratoiy settings. there is little evidence that algorithms trained usuig data ?'om these controlled settings

generalize to free-living conditions [69.'()]. In free-living settings. such aspects as duration. frequency. and

We observed that the majority of studies utilize srnaitphones positioned stationary at a single body position
(i.e.. a speci?c pants pocket). and sometimes even with ?xed orientation. Such scenarios are however rarely
observed in real-life settings. and these types of studies should therefore be considered more as proofs of
concept rather than HAR systems that generalize to free-living settings. Other data quality considerations
relate to the description of the experiment and study protocol. including demographic details of the enrolled
cohort. enviromnental context. and details of the performed activities. Such information should be reported
as fully and as accurately as possible.

Only a few papers consider classi?cation in a context that involves actinties outside the de?ned
research scope. i.e.. activities that the HAR system was not named on. The designed classi?ers were instead
tasked to associate every movement with one of the prespeci?ed set of activities. Real-life activities are
however not limited to any particular set of behaviors. i.e.. we do not only sit still. stand still. walk. and
climb stairs. These classi?ers. when applied to free-living conditions. will naturally miss the activities they
were not trained on but will also likely overestimate others. An improved recognition scheme could assume
that the observed activities are a sample from a broader spectrum of possible behaviors or assess the
uncertainty associated with the classi?cation of each event.

Despite meeting the technical and practical requirements for human activity monitoring. a lack of
standardized procedures makes an apples-to-apples comparison of the studies dif?cult. The research also

suffers from de?cits in publicly available datasets. soiu'ce code. and named classi?cation models. Although

Abstract As performance of dedicated facilities continually improved, massive pulsar
candidates are being received. which makes selecting valuable pulsar signals from candi?
dates challenging. In this paper, we designed a deep convolutional neural network (CNN)
with I 1 layers for classifying pulsar candidates. Compared to arti?cial designed features.
CNN chose sub~integrations plot artd sub~bands plot in each candidate as inputs without
carrying bias '. To address the imbalanced problem. data augmentation method based
on synthetic minority samples is proposed according to characteristics of pulsars. The
maximum pulses of pulsar candidates were ?rst translated to the same position. then new
samples were generaned by adding up multiple subplots of pulsars. The data augmentation
method is simple and effective for obtaining varied and representative samples which keep
pulsar characteristics. In the experiments on HTRU I dataset. it shows that this model can
achieve recall as 0.962 while precision as 0.963.

 

Key words: pulsars: general ? methods: statistical ? methods: data analysis

1 INTRODUCTION

Pulsar searching is an important frontier in radio astronomy. Scientists pay more attention to pulsars
because of broad impact across physi s. astronomy. astronautics (Cordes et al. 2004: Lorimer et al.
1998: Lyne et al. 2004: Hobbs et al. 2000: Sheikh et al. 2006), etc. Many dedicated surveys have been
used to search more pulsar signals. such as Parkes multi?beam survey (PMPS. Manchester et al. 2001).
High time resolution universe survey (HTRU. Keith et al. 2010). and so on. With the advent of the
large?scale search surveys. such as Five Hundred Meter Spherical Telescope (FAST, Nan et al. 2011).
the Square Kilometre Array (SKA, Smits et al. 2009), weaker pulsar signals can be received, while
coming with more and more noise or radio frequency interferences (RFIS). which makes it dif?~
cult to select valuable suspected pulsar signal from massive pulsar candidates. Researchers have ap?
plied many successful methods to select pulsar candidates. including manual selection (Stokes et al.
1986; Johnston etal. 1902). selection with graphical tools (Faulkner et al. 2004: Keith et al. 2009),
ranking & scoring approaches (Lee et al. 2013) and machine learning methods (Eatough et al. 2010;
Bates et al. 2012: Morello et al. 2014; Zhu et al. 2014: Lyon et al. 2016; Devine et al. 2016: Tan et al.
2018; Guo et al. 2017).

The power of actionable data

Data collected from one IoT device has limited value on its own Plus,
according to Forrester Research, 60% to 73% of data in an enterprise
that could be used for analysis goes untapped. Real value is derived by
combining data sets from multiple devices to uncover patterns that can
be used to predict future performance

For example, a manufacturing plant may have 10 IoT devices or sensors
monitoring processes on various production machines. Rather than
looking at the data from only one device or each device independently,
analyzing data from across the site can provide a holistic view of what is
happening at that location.

Al is the enabling technology that processes large amounts of data and
recognizes patterns in the data Using powerful algorithms, AI adjusts to
new inputs and makes decisions based on what it has learned over time
to provide automated, accurate feedback to guide decision-makings It?s
the tool that adds value to all the data collected by loT devices. Al takes
advantage of the aggregation of big data to do more than just discover
what happened in the past Rather, AI produces analyses about ways
processes can be more ef?cient and predicts what could happen based
on multiple scenarios.

 

   

For example, A38 is a leader in industrial motion, power grids, electri?cation products and
industrial transport infrastructure. One of its biggest challenges is predicting plant emissions in
advance and taking proactive measures before violations occur.

ABB?s Predictive Emission Monitoring System (FEMS) powered by Al uses an empirical
model to predict emission concentrations based on processed data. The team successfully
implemented PEMS as part of a comprehensive Environmental Management System in one
of the largest gas processing plants ln the world. However, PEMS (inferential analyzer),
cannot measure emissions directly, So, the system uses an empirical model to predict
emission concentrations based on data like fuel ?ow, load, operating pressure and
ambient air temperature. These kinds of intelligence analytics applications are driving

Al adoption?and forcing companies to consider whether cloud?based or edge?based
analytics are right for their particular business case.

 

it All unlabeled instances in the data stream receive a prediction.

2A The prediction is given promptly after the instances" arrival time.

3 The prediction model is constantly improved (independent of the DS speed).
4 No external instance storage is needed

The framework can be applied whenever events have to be detected as soon
as possible, and no information is allowed to be missed to detect these events.
We believe that concepts for the embedding of machine learning into real?world
systems are required, taking into account the time it takes to make a prediction
as well as the time it takes to train or re?ne a model. In this paper, we discuss
one such framework and present evidence from experiments with varying loads.

This paper is organized as follows First, related work is presented. Then. the
problem setting is presented along with the proposed framework. Subsequently.
the evaluation of the framework is presented in Section 44 The paper closes with
a discussion.

2 Related Work

Data stream mining has developed considerably in the past decade and attracted
many researchers to adopt existing algorithms for the challenging task to process
and reason about instances received at a very high speed [5] One part addresses
the adaptation of batch algorithms to cope with the data stream setting [6] by.
eg incremental batch approaches [7] To provide a speci?c environment for ef-
?cient data stream processing data stream management systems (DSMS) have
been developed. Such systems are adaptations of database management systems

2 DATA SET

To train and test our model. we need labelled convictive datasets. At present. the public labelled datasets
are relatively few. The most common one is the HTRU 1 dataset '. produced by Morello et al. (20l4).

This dataset is a part of outputs of a new processing of HTRU intermediate galactic latitude data
(Morello et al. 2014). It contains 1196 pulsars from 521 distinct sources with varied spin periods. duty
cycles. and signal to noise ratios. Besides. it has 89.995 non-pulsar candidates. it has been used in sortie
recent works (Morello et al. 2014: Lyon et al. 2016: Guo et a]. 2017: Ford 2017). In this paper. we used
it to train and measure our model.

Figure I is an example of pulsar candidates (pulsar20023) in HTRU 1 dataset with its four most
important subplots. The ?rst one subplot is fold pro?le plot. It is obtained by summing signal in all
frequency and period. Pulse pro?le of typical pulsar would make up with one or several narrow peaks
above the noise ?oor. The lower left one is subvintegrations plot. it is obtained by summing data ofdifv
ferent frequency channels. It re?ects the intensity ofthe signal duri g the observation time. For the ideal
pulsar signal. signal would be observed throughout the observation period. so one or several vertical
stripes will form corresponding to the peak positions in pro?le curve. Sub~bands plot is at the upper
right. By summing data over all period. it re?ects the intensity of signal at different frequencies. Since
radio pulsars are broadband. there should be one or more vertical stripes in most frequencies. In DM~
SNR curve at lower right. signal to noise ratio (S/N) ratio as a function of DM is recorded. As the pulse

While the mass of data is steadily increasing and data streams constantly
gain in speed, online learning algorithms used to process the data are naturally
limited by their maximal instance processing speed. As the evolution of these
massive data streams is much faster than the improvement of CPU power after
Moore's law [4], the gap increases between available and processable data As a
consequence, not all provided instances in a stream can be used by the learning
algorithm and some have to be skipped This could be especially harmful if the
skipped instances would reveal important insights to the user. To avoid skip?
ping instances and to still enable (potential) insights, currently not processable
instances could, in principle, be externally stored for later processing. However,
as the data stream speed is higher than the processing speed. the algorithm is
constantly challenged by the amount of data, and the amount of stored instances
is constantly increasing Besides memory usage, the time span from the arrival
of the instances to their processing (response time) is steadily increasing as well.
This processing delay can result in outdated information, and important events
might be missed To address these issues this paper introduces PAFAS (Pre?
diction Assured Framework for Arbitrarily Fast Data Streams), a framework to
handle high-speed data streams that potentially go to or beyond the limits of the
processing speed of the online learning algorithm. The contributions of PAFAS
are:

1. All unlabeled instances in the data stream receive a prediction.

2. The prediction is given promptly after the instances" arrival time.

3 The prediction model is constantly improved (independent of the DS speed).
4. No external instance storage is needed

HTK is a toolkit for building Hidden Markov Models (HMMs). HMMs can be used to model
any time series and the core of HTK is similarly general-purpose. However, HTK is primarily
designed for building HMM~based speech processing tools, in particular recognisers. Thus, much of
the infrastructure support in HTK is dedicated to this task. As shown in the picture above, there
are two major processing stages involved. Firstly, the HTK training tools are used to estimate
the parameters of a set of HMMs using training utterances and their associated transcriptions.
Secondly, unknown utterances are transcribed using the HTK recognition tools.

The main body of this hook is mostly concerned with the mechanics of these two processes.
However, before launching into detail it is necessary to understand some of the basic principles of
HMMs. It is also helpful to have an overview of the toolkit and to have some appreciation of how
training and recognition in HTK is organised.

This ?rst part of the book attempts to provide this information. In this chapter, the basic ideas
of HMle and their use in speech recognition are introduced. The following chapter then presents a
brief overview of HTK and, for users of older versions, it highlights the main differences in version
2.0 and later. Finally in this tutorial part of the book, chapter 3 describes how a HMM?based
speech recogniser can be built using HTK. It does this by describing the construction of a simple
small vocabulary continuous speech recogniser.

The second part of the book then revisits the topics skimmed over here and discusses each in
detail. This can be read in conjunction with the third and ?nal part of the book which provides
a reference manual for HTK. This includes a description of each tool, summaries of the various
parameters used to con?gure HTK and a list of the error messages that it generates when things
go wrong.

Finally, note that this book is concerned only with HTK as a tool-kit. It does not provide
information for using the HTK libraries as a programming environment.

Fig. 1.1 hiessage
Encoding/Decoding

Speech recognition systems generally assume that the speech signal is a realisation of some mes
sage encoded as a sequence of one or more symbols (see Fig. 1.1). To e?ect the reverse operation of
recognising the underlying symbol sequence given a spoken utterance, the continuous speech wave?
form is ?rst converted to a sequence of equally spaced discrete parameter vectors. This sequence of
parameter vectors is assumed to form an exact representation of the speech waveform on the basis
that for the duration covered by a single Vector (typically lOms or so), the speech waveform can
be regarded as being stationary. Although this is not strictly true, it is a reasonable approxima~
tion. Typical parametric representations in common use are smoothed spectra or linear prediction
coei?cients plus various other representations derived from these.

The role of the recogniser is to effect a mapping between sequences of speech vectors and the
wanted underlying symbol sequences. Two problems make this very dif?cult. Firstly, the mapping
from symbols to speech is not oneto?one since different underlying symbols can give rise to similar
speech sounds. Furthermore, there are large variations in the realised speech Waveform due to
speaker variability, mood, environment, etc. Secondly, the boundaries between symbols cannot
be identi?ed explicitly from the speech waveform. Hence, it is not possible to treat the speech
waveform] as a sequence of concatenated static patterns.

The second problem of not knowing the word boundary locations can be avoided by restricting
the task to isolated Word recognition. As shown in Fig. 1.2, this implies that the speech waveform
corresponds to a single underlying symbol (erg. word) chosen from a ?xed vocabulary. Despite the
fact that this simpler problem is somewhat arti?cial, it nevertheless has a Wide range of practical
applications. Furthermore, it serves as a good basis for introducing the basic ideas of HMM?based
recognition before dealing with the more complex continuous speech case. Hence, isolated word
recognition using HMMs will be dealt with ?rst.

 

 

The basic principles of HMMehased recognition were outlined in the previous chapter and a
number of the key HTK tools have already been mentioned. This chapter describes the software
architecture of a HTK tool. It then gives a brief outline of all the HTK tools and the way that
they are used together to construct and test HMM?based recognisers. For the bene?t of existing
HTK users, the major changes in recent versions of HTK are listed. The following chapter will then
illustrate the use of the HTK toolkit by working through a practical example of building a simple
continuous speech recognition system.

2.1 HTK Software Architecture

Much of the functionality of HTK is built into the library modules. These modules ensure that
every tool interfaces to the outside world in exactly the same way. They also provide a central
resource of commonly used functions. Fig. 2.1 illustrates the software structure of a typical HTK
tool and shows its input / output interfaces.

User input /output and interaction with the operating system is controlled by the library module
HSHELL and all memory management is controlled by HMEM. Math support is provided by HMATH
and the signal processing operations needed for speech analysis are in HSIGP. Each of the ?le types
required by HTK has a dedicated interface module. HLABEL provides the interface for label ?les,
HLM for language model ?les, HNET for networks and lattices, HDICT for dictionaries, HVQ for
VQ codebooks and HMODEL for HMM de?nitions.

In related works. supervised machine learning methods have become more signi?cant the major
methods in classifying pulsar candidates.

The ?rst published work which attempted to use a machine learning approach to select candidates
is Eatough et al. (2010). They implemented arti?cial neural networks (ANN) with 12 designed experv
imental features as input vectors. Then Bates et al. (2012) and Morello et al. (2014) have made some
work to improve the performance by optimizing designed features with ANN. In these methods, de?
signed features relied on humans experience. may carry unexpected biases against panicular types of
pulsar candidates (Morello et al. 2014: Lyon et al. 2016). To address these problems. Lyon et al. (2016)
and Tan et al. (2018) selected fundamental and statistical features which aimed to minimize biases and
selection effects (Moiello et al. 2014) to provide better generalization performance.

In addition to these approaches with arti?cial designed features. data-driven methods also play
an important role in this ?eld. Zhu et al. (2014) developed Pulsar Image?based Classi?cation System
(PICS) system by using a group of supervised machine learning approaches. It makes the classi?cation
based on image patterns. The inputs are four important diagnostic plots of candidates rather than ex-
tracted features. It avoids possible defects of arti?cial design features and relying on excessive informa~
tion. It has been validated superior ability of recognition in PALFA survey pipeline and has discovered
six new pulsars. To address class imbalance problem in pulsar candidates. Guo et al. (2017) used Deep
Convolution Generative Adversarial Network (DCGAN, Radford et al. 2015) to generate more candiv
dates and automatically extract deep features at the same time. Then they used deep features to classify
data. which help to makes the classi?er more accurate.

In this paper. we take a step towards improving performance by the data?driven method. We de?
signed a deep CNN with eight convolutional layers, one ?atten layer and two fully connected layers.
The inputs are sub?integrations plot and sub~bands plot in each candidate rather than arti?cial designed
features. To make class balanced. we designed a simple and useful approach to synthesize more diverse
pulsar candidates. New samples were synthesized by adding up multiple subplots of pulsars after max-
imum pulses of pulsars were shifted to the same position. We tested our model on HTRU 1 dataset.
The results show that our model can provide satisfactory results on both recall and precision. The fol~
lowing pans are organized as follows: Section 2. the dataset used for training is introduced. Section 3
describes the data augmentation method for pulsar candidates. Section 4 introduces the network archi-
lecture and training details. Section 5 presents the experimental results ot?our model and analyses of its
performance. Finally. Section 6 is conclusions of our work.

(DBMS) to query continuous, unbounded data streams possibly in combina?
tion with pre?stored, ?xed datasetsi Two well-known DSMS. AURORA [8] and
STREAM [9], use their own language to query data streams. Both systems also
address the problem of too fast data streams. i.ei, when the system is not capa?
ble of processing all of the instances provided by the data stream. They use load
shedding (also implemented in a system environment [IOU to select instances of
the data stream that should be processed Based on Quality?Of?Service (QOS)
speci?cations. the system decides which instances are useful for the system to
fetch and which instances can be discarded. The main idea is to select instances
that will most probably lead to a good prediction. Another possibility to cope
With too fast data streams is sampling. Sampling is a technique to represent a
larger dataset by a smaller selected subseti It was frequently applied to reduce
the overall processing time of data mining algorithms and to efficiently scan large
datasets [ll]i In the simplest case it selects a random subset from the Whole data
set as an input for the learner Frequently, the purpose of this is to estimate the
quality of the result [12] Another possibility to cope with very fast data streams
is to adapt the mining technique corresponding to the currently available re?
sources Such methods are summarized under the heading of granularity-based
techniques. While load shedding and sampling change the input granularity of
the data mining method, the output of the data mining method can also be re?
duced, e.gi the number of rules or clusters [13]4 Then. the model that is used for
classi?cation is smaller and thus also more time?e?icient i.e.< more instances can
be processed in less time This method termed Algorithm Output Granularity
(AOG) can also be applied on various data mining schemes like clustering, clas?
si?cation or frequent set mining Lasti, anytime algorithms are also often used
for altering data stream speeds, as they can be interrupted anytime to return
an intermediate result [14] The more time available, the better the result has

This concept of a path is extremely important and it is generalised below to deal with the
continuous speech case.

This completes the discussion of isolated word recognition using HMMs. There is no HTK tool
which implements the above Viterbi algorithm directlyr Instead, a tool called HVITE is provided
which along with its supporting libraries, HNET and HREC, is designed to handle continuous
speech. Since this recogniser is syntax directedT it can also perform isolated word recognition as a
special case. This is discussed in more detail below.

1.6 Continuous Speech Recognition

Returning now to the conceptual model of speech production and recognition exempli?ed by Fig. 1.1,
it should be clear that the extension to continuous speech simply involves connecting HMMs together
in sequence. Each model in the sequence corresponds directly to the assumed underlying symbol.
These could be either whole words for so?called connected speech recognition or sub-words such as
phonemes for continuous speech recognition. The reason for including the non-emitting entry and
exit states should now be evident, these states provide the glue needed to join models together.

There are, however, some practical dif?culties to overcome. The training data for continuous
speech must consist of continuous utterances and, in general, the boundaries dividing the segments
of speech corresponding to each underlying subword model in the sequence will not be known. In
practice, it is usually feasible to mark the boundaries of a small amount of data by hand. All of
the segments corresponding to a given model can then be extracted and the isolated word style
of training described above can be used. However, the amount of data obtainable in this way is
usually very limited and the resultant models will be poor estimates. Furthermore, even if there
was a large amount of dataT the boundaries imposed by handAmarking may not be optimal as far
as the HMMs are concerned. HenceT in HTK the use of HlNIT and HREST for initialising sub?word
models is regarded as a bootstrap operation??. The main training phase involves the use of a tool
called HEREST which does embedded training.

Embedded training uses the same Baum-Welch procedure as for the isolated case but rather
than training each model individually all models are trained in parallel. It works in the following
steps:

 

Figure 1.1: Diagram of an HMM showing the hidden Markov chain Xp, and the
conditional independence of the observation variables Yk given the states Xk. The arrows
indicate conditional dependence (e.g., Yo is dependent on X0 but Y1 is conditionally
independent of yo given X0 and X1).

3. Given the observations YD,Y1. . . . , yr, estimate the (unknown) parameters of the

HMM A that generated them.

Each of these three basic problems has a well known solution based on the HMM forward
backward procedure (see [14] and references therein for details). For example. the third
problem (i.e. the HMM parameter estimation problem) can be solved using the popular
Baum?VVelch algorithm (which uses the forward?backward procedure on the hatch of
observations YD,Y17. . . ya) [14].

over recent decades. a modi?ed HMM parameter estimation problem has been posed
by introducing the additional requirement that the observations Yn, y1, . . . in should be
processed sequentially (i.e. online) rather than stored and processed as a batch. This

online (or recursive) formulation of HMM parameter estimation has become a topic of

A robot is a machine?especially one programmable by a computer? capable of carrying out a complex series of actions automaticallylzl Robots can be
guided by an external control device or the control may be embedded within. Robots may be constructed on the lines of human form, but most robots are
machines designed to perform a task with no regard to their aesthetics.

Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY?s
TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed

swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating
movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the coming decade? with

home robotics and the autonomous car as some of the main drivers?)

The branch of technology that deals with the design, construction, operation, and application of robots,[5] as well as computer systems for their control, sensory
feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous
environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature
contributing to the ?eld of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.

From the time of ancient civilization there have been many accounts of user-con?gurable automated devices and even automata resembling animals and
humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications
such as automated machines, remotecontrol and wireless remote-control.

The term comes from a Czech word, robola, meaning "forced labor";[61 the word ?robot? was ?rst used to denote a ?ctional humanoid in a 1920 play R.U.R.
(Rossumovi Univerza?lrli Roboti - Rossum's Universal Robots) by the Czech writer, Karel Capek but it was Karel's brother Josef Capek who was the words true
inventorlms?g] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey
Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen. The
?rst commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where
it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New
Jerseylml

Robots have replaced humans?? in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations,
or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their
role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions.?21 The use of robots in
military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in ?ction and may be a realistic
concern in the future.

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

It the accuracy difference between two experiments is bigger than the con?dence
interval, this di?erence can be considered signi?cant with a 9596 of probability. in this
paper. for all the experiments, the con?dence interval is lower than 0.596, so any di?err
ence higher than this value, this difference can be considered as signi?cant with a 9596 a
probability.

Data analysis

lu this section, we ?rst conduct some experiments for l-lAR system con?guration tuning
in order to analyze how the evaluation, the sensor type, or the feature type in?uences the
system performance.

Evaluation method

This section includes the experiments considering the two different evaluation meth?
ods. In these experiments, are setup is: ideal?placement, Null?activity removed (as in the
original paper), and time?based features. The experimental results are shown in Table 1.
From the results, we can clearly see that the result given by the random?partitioning
method is signi?cantly better man the subjectrwise method: the accuracy (Acc%) differ?
ence is 3.496 (99.1?95596) higher than ?re con?dence interval, 0.596.

This result supports the hypothesis suited in me previous secn'on: in the random?part
evaluation, training and testing subsets could contain information from a same subiect
and this characteristic produces better classi?cation results, ln the rest of the paper, we
will only consider the subjectwise evaluation method (more challenging situation).

Type elsensor
This section includes the experiments on different sensor types. ln these experiments,
the setup is: ideals placement, Nullractivity removed (as in die original paper), and time
based features. The experimental results are shown in Table 2. From the results, we can
clearly see that the 3D magnetometer works best among are four types of sensor and the
quaternion sensor performs the worst. The accuracy (Acc?xi) differences are statistically
relevant because they are bigger than the con?dence interval (0.596).

(CNN) ? Ricky Gervais? "After Life" was a bittersweet little gem, but the ?rst season basically
told a reasonably complete story. As a consequence, the second six?episode run feels as if it's
essentially retracing old tenitory ?? moving in places. out With less urgency, and more prone to
silly detours to ?esh out the run.

GeNais' formula of two seasons and out (plus a follow?up special) worked well enough for the
original "The Of?ce" and "Extras." But his ?lmography has been more uneven of late, With "After
Life" very much in keeping with the Writer?producer?star?s outspoken atheism and darker, if not
irredeemabie view of life.

As a brief recap. the ?rst season found Gervais? Tony sleepwalk?ing through his days after his
beloved Wife died of cancer, consoling himself by watching old videos and home movies, With a
faithful dog (Eho?s a good girl? Vou are) as a companion.

By the end, Tony's outlook had brightened, showtng traces of generosity toward coworkers at
the local newspaper Where he grudgingly churned out human?interest stories, and ?nding a
potential new romance in the nurse, Emma (Ashley Jensen, splendid as always), looking after his
dementia?stricken dad (David Bradley).

The new season, however. finds Tony backsliding, again wallowing in grief to the point of
endangering his relationship With Emma, who understandably struggles with his behavior. This
continues, notably, despite the advice that Tony seems intent on ignoring from his cemetery pal
Anne (Penelope Wilton), who chides him not to mex things up.

in the most unoomfortable real?life echo, the aforementioned newspaper is struggling ?nancially
?? at a moment when that industry is painfully unraveling ?? posing an additional challenge to
Tony's boss and brother?inelaw. Matt (Torn Basden), whose marriage is falling apart even as he
presides over the paper's Woes.

 

TECHNEWSWORLD DEVELOPERS

(omD

 

Re ews Se

 

rig iternet rr MD

   

ty Technalngv TechEIDQ

sharpens the Developer 5 Edge

With more developers having an accessible, free place to create software, there
will be more opportunities to collaborate on projects without being limited by
prices, noted Netdata's Tsaousis. With more collaboration, open source startups
and other projects likely will grow and scale at a faster rate.

"Netdata's ?rsthand experience has proved how powerful these tools can be. As
an open source project, founded by a sole developer, Netdata's roots are ?in
using GitHub's open source repositories," he said.

With the help of the community's contributors, along with dedicated company
engineers, the Netdata Agent has grown into one of the most watched and
starred projects on Gitllub and is downloaded more than half a million times a
day, Tsaousis pointed out. The new free access expansion will make it even
easier for an organization to standardize on Gitllub, which is at the heart of the
pricing decision.

"Becoming a de facto standard is a huge advantage with regards to competitive
solutions," he noted. ?However, once an organization is set on GitHub, it can be
dif?cult to revisit that decision if Pro and Team features ever become cost?
prohibitive,"

Bril ant Growth Move

Gillab is a growmg threat to GitHub, and a key reason for GitHub?s change in
strategy, suggested Thomas Hatch, CTO of SaItStack.

"This move is cntical for Gitllub to stay relevant as GitLab continues to steal
users and customers from them. In a nutshell, this allows Gitllub to give open
source users and developers the same thing that G'ltLab has been delivering for
many years,? he told LinuxInsider.

As more big names open?source their software and participate in the
community, it's important to remember why open source existed in the first
place, and the value of the foundation from which you can build, noted Aiven

evaluation methods used in this work and the experimental results Obtained with the
proposed system. Sixth section summarizes the main conclusions.

Background
Human Activity Recognition systems can be categorized by machine learning algorithm
and the type of sensor they used. Human activity recognition can be seen as a machine
learning problem. To deal with this problem, the HAR system must extract features from
sensor signals, generate a model for each activity, and classify next activities based on
these models. In the literature, different machine learning solutions have been applied
to the recognition of activities including Naive Bayes [3], Decision Trees [4], Support
Vector Machines (SVMs) [5], Deep Neural Networks [6] and Hidden Markov Models
(HMMs) [7]. In many works, several approaches have been compared using the WEKA
learning toolkit [8] because it incorporates many machine learning algorithms, For
example, Yang [9] compares the performance of several machine learning approaches:
C45 Decision Trees, Naive Bayes, krNearest Neighbor, and Support Vector Machines,
Kwapisz [10] compares three learning algorithms: logistic regression, 148, and multilayer
perceptron. Not only supervised but also, unsupervised algorithms have been studied
[11], In many works [12], complex algorithms, like the Random Forest, have demon?
strated a very good performance compared to simple classi?cation algorithms. Because
of this, the Random Forest has been the algorithm selected in this work.

For HAR, there are two main types of sensors: ambient and on?body sensors. In
terms of ambient sensors, the most widely used sensors are video cameras [13]. Video
recording is one of the main strategies for supervising human behavior and activities

[14]. But, this behavior can also be studied by analyzing acoustic events, The human

of ?ow meter based disaggregation. it also can determine the volume of water used during each
classified usage event. waterSense requires a sample every 2 s for flow and one every seven seconds
for the motion sensor. The latter is only processed when motion is detected.

4. Classi?cation of Water Even?s

As previously discussed, the approaches for water use disaggregation make use of a variety
of different sensing modalities. The sensed signals are then subject to pattern analysis by applying
classification techniques in order to identify the corresponding water usage events. Depending on
the nature of the sensing modalities, different classifiers are utilised. These classifiers can be further
divided into discriminative and generative ones. in the following we structure our discussion on the
classification techniques according to the utilised sensing modality.

4.1. Water Flow Based Methods

A key assumption of non?intrusive monitoring approaches based on flow meters is that the
use of a particular water fixture or fixture type causes a distinct flow pattern in the residential pipe
infrastructure, which can be observed by a single sensor point. By applying pattern recognition
algorithms to the recorded time series data, water usage events for the specific water fixture or fixture
type can be identified.

Water flow in households is typically modelled with Poisson rectangular pulse as described in [42].
Figure 2 shows a 24-11 dataset generated from the aforementioned model. The number of events in a
unit of time follows a Poisson distribution, while duration and intensity have their own mean and
variance. All the parameters should be adjusted by time of the day with diurnal multipliers. The
observed flows are directly related to the volumes. Reported figures about water volumes and flows
are shown in Table 1 [34,43].

Non~intrusive monitoring techniques based on flow meters face the following difficulties;

. lrregularities of flow patterns for some fixture types. Some mechanically driven water valves,
such as in a washing machine or dishwasher, usrurlly exhibit more regular water usage patterns,
unlike faucets or showers where human users have the ability to vary the amount of flow and
duration significantly. Even the more regular flow patterns of washers can show variation
based on a diversity of different water saving programs and washing cycles that are available in
modem~day devices.

. Similarity of flow patterns among instances of the same fixture type. Many homes have multiple
toilets and/or faucets, which may be located in different rooms. This makes it challenging to
identify an individual ?xture if multiple instances of the same type exist [34,33].

USA
TODAV

NFL draft moves along after wild night

First round of the 2020 NFL draft is in the books and will move ahead to the second
and third rounds on Friday night (7 p.mi ET on ABC, ESPN and NFL Network), Joe
Burrow went ?rst overall to the Bengals as expected, and in the next several
selections, teams followed the chalk: Chase Young to Washington, Jeff Okudah to
Detroit. Offensive tackle Andrew Thomas went to the Giants, followed by
quarterbacks Tua Tagovailoa and Justin Herbert to the Dolphins and Chargers,
respectively The "virtual" NFL draft also made for interesting TV on Thursday
night, from Cardinals coach Kliff Kingsbury showing off his lavish pad to whatever
it was that was going on at Titans coach Mike Vrabel's housei USA TODAY

Sports will have live updates and analysis of all of the news from the event,
including an up?tofthe?minute tracker breaking down every pick in real time.

. NT]. drz?'s most intriguing ?rst?round pidm Packers selection of QB Jordan Low raises eyebrows

- SEC breaks its own record for ?rst-round NFL draft picks with 15, led by LSU
and Alabama

Virtual vigil to be held in Canada for mass shooting victims

Victims of one of Canada's deadliest mass killings will be remembered Friday night
in a virtual vigil. Twenty?two people were killed last weekend when a gunman
dressed as a Royal Canadian Mounted Police of?cer went on a 12?hour killing spree
that began in the rural town of Portapique, Nova Scotia, and included at least 16
crime scenes across the provinceThe suspect, 517year?old Gabriel Wortman, was
shot dead by police The vigil will be livestreamed on the Facebook group
Colchester ? Supporting our Communities at 7 pm, local time (6 p.m. ET) and will
also air on the CBC News Network.

I Wasn't the type of kid who got attention. Teachers always wrote ?needs to participate more" on my report cards (with a smiley face to make my parents feel better]. I never got into trouble
and barely ever stood out on purpose. A fewyears earlier, I accidentally peed my pants because my zipper had gotten stuck in the bathroom at the last moment. I tried to convince everyone
that I had fallen into a puddle at recess. The custodian, Mr. Salazar, charged outside with a mop and brought me to find the puddle. My guess is that we wasted an hour looking around at the
gravel. My mom dropped off some new clothes and nobody really noticed my wardrobe change (...or that it hadn't rained inweeks].

That's how it was. Whether I did something spectacular or sneezed myself out of a chair, nobody cared, and almost nobody said my name. As far as school has concerned, all those things had
happened to ?some kid". So, why would a frogwith glasses jump up on a windowsill to stare at ?some kid??

Teachers, on the other hand, were different. Once her story ended, it hadn't taken Miss Weaver long to realize that Imsn?t paying attention. She called me up to the blackboard to make an
example out of me.

?Since you don't feel the need to listen, why don't you solve a problem on the board instead?? she said, sitting down at her desk.

My stomach did a ?ip. The problem would take a minute or two to solve, and being in front of the class almys made me nervous. How could I be expected to do anything when there was a
spectacled frog staring me down!

I stood to the right of the equation on the board so that I could check on the frog with quick glances. Despite the distraction, I did my best to focus. Halfway through, I saw that the frog had
moved towards the front of the classroom. It stopped at the window by Miss Weaver's desk. It took me a moment to figure out what it ms doing. I couldn't believe what [was seeing. It was
trying to lift the window.

Focusing on the problem became almost impossible. I made a mistake and then quickly erased it. The next time I looked over, the window was open. Why should that surprise me? Of course
a frog with glasses would also be super strong. The window was only open an inch, but that was enough for it to slip through. I dropped the chalk, and some of my classmates laughed.
Bending down to pick it up, I tried convincing myself that when I stood back up again the frog would be gone. ?It?s not there, I just think it's there.?

When I straightened up, the frog was sitting on Miss Weaver's left shoulder. This was a brave frog. Her head blocked the class from seeing it, and I realized that [was still the only one who
could. Either the frogwas real or my imagination had outdone itself. It wastit all that surprising that Miss Weaver didn't feel it there, because the shoulder pads inside her jacket were large
and ?uffy. I had heard that she rested her head on them like pillows during her breaks. So, now there ms a frog sitting on Miss Weaver's shoulder and nobody else knew it. And [was
supposed to be doing math.

Now that it was closer, I could see the frog better. It didn't look like some new species of frog to me. It looked like every other frog I had seen (except for the glasses]. I wondered if they made
contacts small enough for a frog. But, it wasn't the right time to worry about frogvision. That's a job for a frog eye doctor, anyway.

I had daydreams all the time when I was drawing, and sometimes I got lost in them. I tried one last time to explain the frog away, by saying it had to be part of an elaborate daydream. I
concentrated hard, finished the problem, and put the chalk down. The frog couldn't be real. I shook my head confidently.

On a high level, disaggregation allows domestic water use to be broken down into fixture
categories, which identify the amount of water consumption of individual fixture types in a household.
Typical fixture categories for indoor use are shower, bathtub, toilet, and faucet, as well appliances,
such as washing machines and dishwashers. Typical outdoor fixture categories are exterior hose bib,
swirrrming pools and irrigation systems. Most current end use studies provide insights on residential
water use at the level of the fixture category. Sometimes, however, it is necessary to distinguish
between hot and cold water use, and the location (room, indoor or outdoor) where the water is being
used. Such water usage break down requires knowledge of water use at individual fixtures or even
valves (in the case of hot and cold water).

A further level of contextualisation is the attribution of water use to individual residents in a
multi-party home or the mapping of water use to individual activities (e.g., washing hands, cleaning
teeth, watering the garden, eta). The latter contextualisations are particularly hard to obtain

One of the most common methods for deriving a breakdown of domestic water end use
information is through manual data collection acquired by consumer surveys, diaries/self?reports
and in situ observations in domestic living environments. Such studies are able to capture a diversity
of information, even detailed information that is sometimes very difficult to capture, such as water
usage activities. However they tend to be very labour intensive and do not scale well for longitudinal
analysis (over longer periods of time for larger populations). 0nline survey tools, such as the Water
Energy Calculator [28], have made it easier to reach wider audiences [6], however, such studies rely on
the truthfulness of the persons participating in the studies. Despite their best attempts at being honest,
users often reflect perceptual bias or may accidentally misreport relevant information 129]. Furthermore,
self-reports and surveys are not able to capture the exact amount ot water use and represent only
estimates that have to be complemented by more detailed metered water use. Researchers in the
field have therefore looked into instnrmenting households and applying data analytics solutions to
measured data traces in order to gain a better insight into water usage patterns.

A very accurate but inefficient way to obtain such insights is through extensive instrumentation
of a household. Each individual fixture or even valve can be instrumented with a flow meter. Such
deployments are mainly limited to testbed settings [30732], in order to establish a ground truth tor
other experiments with less intrusive techniques. It is not difficult to see that such an approach is, not
only cumbersome and costly in terms of deployment, but highly intrusive. Such a case can be seen as
analogous to intrusive load monitoring in the energy domain [33].

ln order to overcome the above limitations, a variety of non~imrusive monitoring approaches have
been proposed, which are able to perform water disaggregation based on data obtained from a single
sensing point or from a limited set of sensing points deployed at strategic locations of a residential
water pipe infrastructure and /or rooms ot a residences.

 

 

TECHNEWSWORLD DEVELOPERS

Commit d I ernet rr Mn leTecli Rev ws Secur Teclniulngy reclining

 

Right Timing

"This ?ls a huge investment we are making, and it is good for Github's business
long term because more developers globally will be able to use the platform," a
spokesperson said in a statement provided to Llnuxlnsider by company rep
Nicole Numrich.

The growth factor is the key reason for the change of plans in providing
expanded free access, according to the spokesperson. It has 40 million
developers now, and global development is not slowmg down.

The GitHub Enterprise produti is reaching more companies than ever with 29
Global Fortune 50 companies building the software behind their businesses on
Gitliub Enterprise, the spokesperson noted. This shift from a "pay?for?privacy"
model to a "pay?for?features" model IS a fundamental change to the business
architedure of Gitllub.

Plummeting Price Plan

Gitl?lub reduced the entry?level paid tier price to $4 per user per month instead
of $9. The company still offers a more expensive tier (521) with SAML sign?on
and greatly expanded storage and actions.

Also still available is the specialized GitHub One service. Account managers
with high?value customers can negotiate special subscription fees.

Under the previous cost structure, Gitl-lub offered a free tier for private
development. However, it limited the number of collaborators with access to a
private repository to three.

Teams interested in using GitHub for private development had to subscribe to
one of its paid plans. GitHub previously offered unlimited repositories for free
only to public projects or those with a small number of users. That precluded
use of the free tier by several different types of teams, organizations and
companies.

Type offeature
We also make a comparison on the performance of different kinds of features, more spe?
ci?cally, temporal features and frequency features as described in ?Feature extraction?f
Same as the experiments on sensor types, here, we also consider the idealvplacement,
removing the Null?activitv. For the sake of con?dence, we repeat the experiments with
different types of sensor.

From the results shown in fig. 2, it is obvious that the temporal features always beat
the frequency features and their combination in the cases of all mree sensor types.
Therefore, we consider only the tjme?based features in the rest experiments.

As a conclusion, it is clear that using me signals from magnetometer and the time
based features is currently the best system con?guration. By including all sensors, we
obtain even higher system accuracy: 97.096.

 

When tr ning and testing with different subjects, it is important to deal with the
intenuser variability. In order to reduce this variability, we propose several normali?
zation strategies. In this work, we evaluate six normalization methods, considering
two different places where this normalization is applied: before and after the feature
extraction.

1. Mean removal: Subtract the lnean value from each value in a feature or signal vector.

2. ZsScore: Mean removal ?rst, and divide each value by its standard deviation.

3. Histogram equalization. Consider all the values in a graysscale, and equalize its his?
togram,

4. 0?1 mapping: Distribute all data to the (L1 range.

Vector normalization: Divide each value in a vector With the vector's magnitude

6. Vector normalization with mean normalization. Vector normalization followed by

9"

mean removal

In this chapter. we are focused on quickly determining if a manoeuvre has occurred.
rather than estimating the manoeuvre time T and postemanoeuvre velocity VG. we
therefore aim to design manoeuvre detection algorithms that minimise the time delay
hetween when the manoeuvre occurs and when the null hypothesis H0 is rejected (whilst
avoiding false alarms). we shall present our proposed manoeuvre detection algorithms
as (stopping) rules for selecting the time to declare that a manoeuvre has occurred (by

rejecting H0).

lmportantly. our aircraft manoeuvre detection problem can he viewed a non?Bayesian
quickest change detection problem. In this chapter, we will exploit our minimax ro
hustness results of Chapters 4 and 5 by proposing two classes of vision?based aircraft
manoeuvre detection algorithms: a heading?based class of approaches inspired by our
i.i.d. process results of Chapter 4; and a transition?based class of approaches inspired
by our dependent process results of Chapter 5. Here. in addition to proposing and
investigating robustness?inspired manoeuvre detection algorithms, we will also consider
adaptive algorithm that attempt to detect the unknown manoeuvre by estimating any

unknown postemanoeuvre imageplane velocity information.

We shall ?rst brie?y revise an HMM representation of the aircraft imageplane
dynamics (5.1) that underpins both our transition?based and heading?based aircraft mae
noenvre detection approaches. We will also introduce an intuitive method of estimating
the aircraft?s imageeplane heading app that we shall use to propose our heading?based

manoeuvre detectors.

a least favourable parameter approach that requires the uncertainty set of parameters
to satisfy an informationrtheoretic Pythagorean inequality condition. This information?
theoretic Pythagorean inequality condition is closely related to the partial stochastic
boundedness concept we introduced in Chapter 4 to identify least favourable distributions

for our i.i.d. process asymptotic rninimacr quickest change detection problems.

In the case of i.i.d. observations before and after the changetime, the Lorden and
Pollok results of this chapter are special cases of the results we established previously
in Chapter 4. speci?cally, the results of this chapter only apply in the i.i.d. case
when the prechange distribution is known. and when the uncertainty in the post?change
distribution is described by unknown parameters (i.e., when the uncertainty is parametric
rather than nonparametric). The results in this chapter are signi?cant because they
hold for a large variety of stochastic processes including Markov chains and linear state
space systems (whilst the results of Chapter 4 and [e] are limited to processes with i.i.d.

observations before and after the changetime).

This chapter is structured as follows: In section 5.1. we propose Lorden. Pollak, and
Bayesian minimar robust quickest change detection problems with polynomial delay
penalties and parametric uncertainty. In section 5.2. we introduce our information?
theoretic Pythagorean condition on the uncertainty set, and identify asymptotically
minimax robust Lorden. Pollak and Bayesian quickest change detection rules. In section
5.3, we present Markov chain and linear statespace system examples and simulation

results. Finally, we provide conclusions in section 5.4.

Abstract: Water monitoring in households is important to ensure the sustainability of fresh
water reserves on our planet. It provides stakeholders with the statistics required to formulate
optimal strategies in residential water management. However, this should not be prohibitive and
appliance?level water monitoring cannot practically be achieved by deploying sensors on every
faucet or water-consuming device of interest due to the higher hardware costs and complexity, not to
mention the risk of accidental leakages that can derive from the extra plumbing needed. Machine
learning and data mining techniques are promising techniques to analyse monitored data to obtain
non?intrusive water usage disaggregation. This is because they can discern water usage from the
aggregated data acquired from a single point of observation. This paper provides an overview of
water usage disaggregation systems and related techniques adopted for water event classification.
The state?of?the art of algorithms and testbeds used for fixture recognition are reviewed and a
discussion on the prominent challenges and future research are also included.

Keywords: water usage disaggregation; water monitoring; disaggregation algorithms; machine
learning; water management

 

1. Introduction

The global use of water is increasing at a rate faster than can be satisfied with current
usable water supplies [1]. While irrigation and electricity generation dominates water usage in
developed countries [2], household water conservation still represents an important factor in ensuring
sustainability of fresh water reserves on our planet. In the US, for example, nearly 10% of fresh
water consumption can be attributed to domestic use [3]. In the UK, each person uses about 142 L
of water each day with the average household using 349 L of water per/day [4]. Even though

 

3.1. Assessing Individual Water Consumption

An effective way of measuring household level water consumption is through metering the
water supply at the premises of a customer. Only about half of UK households currently have a
water meter installed [21]. The vast majority of these meters are not Internetfonnected, and require
a manual readout by the water company or the customer. Meter readouts often take place on an
annual or monthly basis, in order to estimate the domestic water hill. Non~metemd customers are
charged an amount that is proportionate to the rateable value of the property [22]. This results in
an annual ?at rate that does not take into consideration the size of the household [23]. Automated
meter reading (AMR) or smart meter reading provides the ability to automatically capture water
usage information at more regular intervals. In their most basic form, such meters do not require a
connectivity infrastructure. They act as standalone meters that can be read through some wireless
channel in a walk~by (e. g., handheld devices) or driveby fashion (e.g., utility service vehicle). A more
effective way is connecting AMR/smart meter devices via a dedicated metering infrastructure to the
utility company, or via existing communication networks available at the household (e.g., phone line,
lntemet router). While this comes at increased costs and complexity it removes the burden of relying
on physical proximity for retrieving the meter readout, theoretically allowing near maHime reporting
of metering information in practice, meters are typically monitored on a daily, hourly basis or 15 min
basis [24], as dictated by the costs for data communication and data storage, respectively

In contrast to AMR devices, which provide only simple reporting functionality, smart meters can
provide bidirectional communications. Depending on their extended capabilities, smart meters can
provide some configuration options to the utility company, such as the configuration of the reading
interval or other system settings Some smart meters can be even interfaced to in-home displays or
smart home platforms, providing residents with information on their current or historic water use [25].

Despite their advanced metering capabilities, current smart meters are only capable of answering
how much water is being used and when. Breaking down the residential water use to more finegrained
levels, e.g., fixture level use, requires higher resolution readings combined with external data analytics
and possible additional instnrrnentation. Recent work in the field of energy metering calls for an
evolution of smart meters to become ?cognitive meters" [26] that are able to disaggregate the water use
within the metering device. While basic features, such as household leak detection on smart meters, is
already feasible [27], this vision still requires some further advances in the field.

interestingly, the scientific community has been working since the 19905 on approaches for water
use disaggregation at the household level. In the following we will examine these works and identify
strengths and weaknesses of these and current gaps.

2. Understanding Water Usage

For a domestic setting, there are a variety of questions that can be asked, which can lead to insights
about the water use of a household at different levels of granularity:

- How much water is being used?

- What water is being usedfis it hot or cold water, fresh water or grey water?

- When is water being usedfhow does the water use change over the day, week and seasons?

- Why is water being usedfwhat are the activities related to water use?

- Where is water being usedfat which fixture or fixture type is water being abstracted from? Are
these indoor or outdoor uses?

- Who is using the waterfin a multi?occupancy household or building, how can water use be
attributable to individuals?

When unravelling insights about domestic water use, it is obvious that not all questions may
be relevant for all stakeholders and that different exploitations may require a specific water usage
contextualisation to be obtained.

From a water utility perspective, there are different motivations as to why obtaining insights
into household level information is important. At the most basic level, water usage information on
individual households is important for billing purposes. Aggregate monthly or even yearly water
usage information may be sufficient for water utilities to obtain adequate compensation for their
domestic water supply. Moreover, in order to achieve water distribution network efficiencies and
to understand investment levels, the relative costs and benefits of consumption are required; these
can only be estimated by determining households' water use [10]. This, in turn, impacts on water
pricing mechanisms. This information is useful to manage resource constraints and future demands.
From end-use analysis, utilities can gain insights into how new generation appliances will positively
affect demand, and can thereby avoid oversupply before it is needed. Having information of water

4.1.1. Discriminative Classifiers

Flow trace analysis is one of the first automated techniques to infer water usage from single
flow meter readings. It was initially proposed by Dziegielewski ct al. [45], and is currently the most
widely?used technique for identifying water usage events in the water industry due to its maturity
and the availability of commercial service offerings based on it. Flow trace analysis relies on the fact
that domestic water use exhibits Common patterns that are distinctive enough to discriminate water
usage events of different fixture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decisionstree based classi?ers, the current water source for these water usage events can be determined.

A first extensive study that utilised flow trace analysis was presented by De0reo el al. [34]. The
authors performed a collection of signature flow traces for each fixture inside of 16 homes at a rate
of one sample every 10 s using a flow meter The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each. Using the signatures, data~?ow traces were
determined based on visual analysis. When a type of flow was identified, it was isolated in a window
and the integral of the flow rate over this window provided the volume of water used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a slgnal~processing algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, flow rate
change over time and time of the day cues. The authors however did not provide any assessment of
the performance of their solution

The two market leading commercial tools, TraceWizard [46] and ldentiflow 147], are also based on
the principle of flow trace analysis. According to a previous review by Nguyen ct al. [43] for these two
systems, both use decision tree based classifiers and require a timeconsuming and labour-intensive
process to perform offline fixture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak flow rate, the most common flow rate, and how often this most common flow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
fixtures are used at the same time or 0% when three or more were used. Similarly ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.8% in terms of the correctlyaclassified volume. As it relies on
fixed physical features ofvarious water?using devices, such as volume and flow rate for disaggregation,

the ?nal classification accurag is greatly degendent on the existing Mes of water devices

Remark 6.10 We con?rmed that the e?ect at k z 120 taas due to imperfect image
stabilisation by inspecting the time?synchronised nuuiyational data in Figure mm) 54
(c) to con?rm that the target aircraft had not begun to manoeuvre. We also repeated the
erperiment raith an improued imaae stabilisation technique (that is not currently suitable

for real?time implementation) and there was less increase in the test statistics at this

frame.

6.6.4 Results Summary, Limitations and Alternatives

overall, our simulation and real data results suggest that our TRC and HRC deteo
tors can offer similar performance to our adaptive HGLR.. TGC. and TGLR detectors
(although we acknowledge that our HGLR and HRC detectors both perform poorly
in our third and fourth ground?based sequences since the manoeuvres involve a large
change in imageplane speed up). on the basis of our simulation and real data results.
it therefore seems reasonable to select manoeuvre detectors on secondary considerations
such as computational and memory complexity (see section 6.5). In this respect. our
proposed HRC and TRC detectors are particularly attractive. conversely, our proposed
TGLR and TGC detectors are extremely computationally expensive (although our TGC
is preferable to our TGLR), but do perform well across a range of manoeuvre and local

detectability scenarios.

Finally. as in radar?based aircraft manoeuvre detection (e.g. [13.40]; we acknowledge

that it seems possible to propose alternative vision?based aircraft manoeuvre detection

wm M. rated it kiwi

sl?lrlvcs classics mad-again 2017

 

iThe second paragraph contains spoilers, I'd steer clear oi it if you haven't
read the novelTable 6.9 suggests that the performance differences amongst our transition?based ma?
noeuvre detectors are marginal (although our proposed TGC detector performs slightly
better than the TRC and TGLR detector in videos 1. 2, and 3). Signi?cantly. our HRC
detector outperforms our TGC detector in videos 1 and 2 (although its performance
is poor in videos 3 and 4). Similarly, although our proposed HGLR detector performs
reasonably in videos 1 and 2, its performance in videos 3 and 4 is very poor. we believe
that the poor performance of our HGLR and HRC detectors in videos 3 and 4 is due
to our constant speed modelling assumption (Assumption 6.1) being violated. Indeed,
in videos 3 and 4, the aircraft manoeuvres first manifest themselves as changes in the
aircraft?s imageplane speed vk (before the aircraft reverses its imageplane heading).
overall. despite our ground?based real data results highlighting this shortcoming of
our headingebased approach to manoeuvre detection, they again illustrate (as in our
simulation study) that TRC and HRC detectors can offer similar (or better) performance
than the adaptive HGLR. TGC, and TGLR detectors.

6.6.3 Airborne Scenario

we ?nally examine the performance of our proposed vision?based aircraft manoeuvre
detectors on an airborne video sequence involving a tail chase of a manoeuvring Cessna
182 captured by a forward mounted camera on a Cessna 172 (at 15 frames per second,
8 bit grayscale. 1024 x 768 pixels). we captured time?synchronised navigational data

from CPS/INS navigation sensors tan?board the manoeuvring aircraft to demonstrate

Note that in Table 3, SIGNJ means normalization on signal data using method 1 and
FEATJ means normalization on feature data (i.e. after feature extraction) using me?iod
l. The results show that the vector normalization method (number 5) applied directly
on the signal data outperforms all other methods, Thus, for the rest of the report, we use
this normalization method before the feature extraction.

Final results and discussion
By applying the best experimental configuration described above, we conducted experi
ments using data from all signals and all sensors in all the three placement scenarios.

ldealplacemem
Table 4 shows our ?nal experimental results for the ideal placement scenario. After
introducing the signal normalization method and considering all sensors, the system
accuracy goes to 99.4%, a 2.496 improvement compared to the original paper (9796). This
improvement is higher than the con?dence interval (0.536) so the difference is statisti?
cally signi?cant with a 9596 of probability. it is important to notice that, in the original
paper, the evaluation method was random?partitioning and, based on the results pre
sented in "Evaluation memod?, the baseline accuracy would be even lower when using
the subject?wise crosssvalidation method.

Another aspect to comment is regarding the Nu??aCtIVity. In the ?rst two rows of
Table 4, the experiments are conducted without considering the Null?activity, in other
words, it is a 33?class classi?cation task. We truncated the Nulleactivity samples in order
to make a fair comparison with the original paper. In this work, we have also done expets
iments including the Null?acn?vity, which, in our opinion, is closer to a real situation. So,
are problem now becomes more challenging: a 34rclass classi?cation task. Regarding the
results shown in the mird row of Table 4, our system still maintains a high performance
when the Null?actlvity is included: the system only loses 0.396 accuracy (from 99.4 to
99,196) showing a significant improvement (2.196) respect to the baseline system (9796).

Environmental data are an excellent source of information for occupancy detection since the
presence of living beings affects the surroundings through heat or Carbon Dioxide (C02) emission
without jeopardising the privacy of the occupants in that particular location. Nevertheless, only with
data, it is almost impossible to gauge something. Machine Learning (ML) techniques look at the
data and try to find patterns; with these patterns, it is possible to affirm the occupancy with a certain
percentage of certainty. Although some contributions have been performed in this direction, there is
still room for improvement, and this research proposal is focused on that.

The main purpose of this research is the design and development of an affordable and
non-intrusive solution to improve occupants experience in Smart Environments with ML support.
The proposed solution monitors temperature, light intensity, noise, and CO; to estimate the presence of
occupants through these environmental features that can be integrated with other existent approaches.
First, the data are collected and analysed, before applying ML techniques to infer the occupancy of the
area under monitoring. In the first stage, our solution detects the presence or absence of occupants.
In the second stage, the number of occupants inside the area of interest is estimated.

This paper is structured as follows Section 2 discusses the related work. Section 3 presents the
solution developed to detect occupants, its architecture, and the key features that were considered, the
gathering system and the ML concerns. Section 4 shows the experimental implementation. Section 5
analyses and discusses the results obtained Finally, conclusions are presented in Section 6 as well as
suggestions for future works.

2. Related Work

Occupancy detection systems could be classified according to the need to use a terminal or
not [6,7]. In the case of the methods that require a terminal, it is necessary to attach a device to the
occupants to keep track of them (eg., a smartphone). In the non-terminal methods, the detection is
based on a passive approach that is focused on monitoring areas or spaces instead of the identification
of devices (eg, cameras monitoring a room). Figure 1 depicts a simple classification of the occupancy
detection methods following the terminal and non-terminal approaches, and their more specific
characterisations, which are used to organise the discussion of this section.

I. BOY MEETS FROG

The first time that I saw the frog, [was sitting in class. Its face was pressed up to the window next to me from the outside. I had been drawing in my notebook, but once I noticed it, I couldtit
stop looking. It couldn't stop looking at me either (if it had been a staring contest, Iwould have lost). Maybe frogs blinked, but with its big eyes smushed against the glass, this one didn't.
Stagwood Forest was just beyond the school yard and it was riddled with frogs, but they always avoided people. Iknew right away, in a way that I can think better than I can say, that this frog
was different.

Miss Weaver hadn't noticed. She'd been my teacher for a few months, and ms known for having a stack of black hair that rose a foot above her head. Before the school year started, I had
heard rumors about her, and within a weekl realized that they were all true. For one thing, she wore the same out?t every day; the colors changed, but she almys had on striped pants and a
striped jacket. For another thing, she was mind?numbingly boring. The kind of boring that makes your eyes shut without your permission. Part of the problem ms that she liked to tell
pointless stories instead of teaching. She was obsessed with telling stories about former students who had become famous. The ?rst couple of times weren't had, even kind of interesting, but
by the second week of school she had already started repeating herself, just like with her outfits.

I knew all the stories by heart. The professional football player who was good at math, the politician who was a teacher's pet; I knew every word. Instead of listening, I spent most of class
drawing. I drew imaginary places, and designed creatures to fill them. Every drawing had a story. But not that day. I had barely gotten started when the frog appeared, and changed my life
forever.

I tried to listen back in to Miss Weaver, just in time to hear the end of her story about Martin Shandals, the now?famous comedian. Martin had transferred schools half my through the year,
so I always felt like that one shouldrit count. We were supposed to be learning long division, but something had reminded her of Martin. I knew exactly what bad joke she would end the story
with, and much less about long division.

?Whenever he acted up in class I'd say, we?ve got a real comedian on our hands don't we?" And 1 was right!" she said with a giggle.

I was sure Miss Weaver would see the frog eventually, but she didnt. Nobody did. When I looked again to see if it was still there, I noticed something shiny. It made me forget all about class,
and Miss Weaver and Martin Shandals. There was no denying It: the frog had put on a tiny pair of glasses.

I wanted to lecture it, to explain that frogs don't wear glasses. It bothered me that it didn't already know that. On top of that, it had been staring at me for at least five minutes. It seemed like
it ms bordering on rude. Could a frog even be rude? I wasn't sure. But, the bigger question was why it was so interested in me.

3.3. Sensing

The sensing process for non~intrusive water use disaggregation approaches is determined by a
variety of factors. A first key discriminator is whether a single or multiple sensing points are required
for a residential setting.

While singlepoint sensing solutions are typically based on a single modality, multi-point
approaches can utilise one or more different sensing modalities. Approaches that utilise a single

 

Sensors 2016, 16, 738 6 of 20

sensing modality are referred to as mono?modal sensing approaches, while approaches that utilise
multiple sensing modalities for water use disaggregation are referred to as multi-modal.

Depending on the nature of sensing modality the sampling frequency may greatly vary. Low
frequency approaches typically operate in sub-Hz regions, while high frequency approaches can
require up to several kHz sampling of the sensing signal. The sampling frequency determines the data
rate, which has an impact on processing storage, and conununication requirements.

The sensing process also typically determines whether associated water volumes of water usage
events can be determined.

Figure 1 shows an overview of the different sensing modalities utilised in current approaches that
can be found in literature.

I?I

THE STELLAR ONE

by Daniel Errico

When the universe was young the sky was ?lled with planets, and stars, and stardust, and many many rocks.

One of these rocks was a bit more special than the rest. She was unlike any that came before her.

She was a kind and happy rock, who always ?oated near a big blue planet.

Sometimes when the light hit her surface, she would glow a brilliant green. At times like those, she almost didn't look like a rock at all.

As the sky moved from day to day, and week to week, the rock would see planets far off in the distance.

She would wonder what it would be like to go to them. Week after week and month after month she would wonder. Until one day she decided to go ?nd out.

The rock had never gone anywhere before and wasn?t sure how to go about it.
She started to rock back and forth.

Then she started to spin.

Soon enough, she was ?ying through the sky.

As she left, clouds swirled on the big blue planet. For it was sad to see her go, and when planets cry there is a rain storm.

At first the rock was not good at moving. She would spin too far to the right or too far to the left. Slowly she learned how to travel whichever direction she liked, and she enjoyed exploring

2.3 The Toolkit

The HTK tools are best introduced by going through the processing steps involved in building a
sub?Word based continuous speech recogniser. As shown in Fig. 2.2, there are 4 main phases: data
preparation, training, testing and analysis.

2.3.1 Data Preparation Tools

In order to build a set of HMMs, a set of speech data ?les and their associated transcriptions are
required. Very often speech data Will be obtained from database archives, typically on CD-ROMS.
Before it can be used in training, it must be converted into the appropriate parametric form and
any associated transcriptions must be converted to have the correct format and use the required
phone or word labels. If the speech needs to be recorded, then the tool HSLAB can be used both
to record the speech and to manually annotate it with any required transcriptions.

Although all HTK tools can parameterise waveforms on-the-?y, in practice it is usually better to
parameterise the data just once. The tool HCOPY is used for this. As the name suggests. HCOPY
is used to copy one or more source ?les to an output ?le. Normally, HCOPY copies the whole ?le,
but a variety of mechanisms are provided for extracting segments of ?les and concatenating ?les.
By setting the appropriate con?guration variables, all input ?les can be converted to parametric
form as they are read~in. Thus, simply copying each ?le in this manner performs the required
encoding. The tool HLIST can he used to check the contents of any speech ?le and since it can also
convert input on-the?y, it can he used to check the results of any conversions before processing
large quantities of data. Transcriptions will also need preparing. Typically the labels used in the
original source transcriptions will not be exactly as required, for example, because of di?erences in
the phone sets used. Also, HMM training might require the labels to be contextadependent. The
tool HLED is a scriptadriven label editor which is designed to make the required transformations
to label ?les. HLED can also output ?les to a single Master Label File MLF which is usually
more convenient for subsequent processing. Finally on data preparation, HLSTATS can gather and
display statistics on label ?les and where required, HQUANT can be used to build a VQ codebook
in preparation for building discrete probability HMM system.

 

TECHNEWSWORLD DEVELOPERS

corn;

 

e Tech Rev

 

Internet rr Mu Technnlngy Teclialnn

   

 

Open Source gn cance

The inclusion of Wordpress and digital marketing features in GitHub's offering
allows plenty of room to experiment, observed Pavan Raheja, business head at
Flint Technology|Consulting.

"GitHub being the best?smooth version control system takes a burden off me
and allows my team to try new features comfortably," he told LinuxInsider. "It
just gives complete freedom to my developers to try new features without
having to worry about anything. Especially, as WordPress is evolving to
decoupled systems and we are constantly trying to evolve, Github gives us the
complete freedom to try and test at will and with complete ease.?

Github makes it easy to collaborate with remote developers, Raheja pointed
out. It makes the Work?ow so simple that he can keep a complete track of
where the new developments are happening Without being worried about his
master copy.

"As 1 have complete control over the master and commit requests, 1 do not
have to be worried," Raheja remarked. "Ijust have to have a quick look a the
pull request In any of the branches my developers make the changes on. It
further gives me complete freedom to focus on other activities, like digital
marketing, without worrying about my code management.?

No Perfect Plan

Github's new access strategy may have an unintended bad consequence,

according to Prabhu Subramanian, creator of AppThreat and lead architect at
shiftLeft.

"This is significant but not for the better, unfortunately. Having all of the open
source artifacts ~ code, packages, vulnerabil

   

es, discussions, road maps ??
being aggregated under a single commercial umbrella is kind of the opposite of
what open source stands for. Free in open source is about freedom,"
Subramanian told LinuxInSider.

 

 

 

mwmm.

 

 

(a) (b)

Figure 1.3; Example of an aircraft climb manoeuvre observed from behind through a
video camera mounted on another aircraft; (a) a grayscale image of the manoeuvring
aircraft (the white image feature in the black square) with a manually annotated
historical track (blue dots) of the observed aircraft manoeuvre in the video; and (b)
navigation data from sensors on.board the manoeuvring aircraft showing the observed
climb manoeuvre (the observing aircraft is approximately 2 km to the southwest). More
details of this example are provided in the conference paper [C2] and Chapter 6.

1.4 Research Problems and Objectives

In this section, we shall use the open problems discussed in sections 1.1, 1.2, and 1.3
to formulate the research problems and research objectives of this thesis. Throughout
this thesis, we will focus on discretetirne processes (although there are analogous open

problems in continuoustirne stochastic processes).

 

Knowledge would be fatal, rt 15 aneemnnty that charms one.

A mist makes thtngs beautiful.
?Oscar Wild, The Prctmr of Damn Gmy

In this chapter, we build upon the independent and identically distributed (i.i.d.)
process results of Chapter 4 to investigate the problem of quickly detecting an unknown
abrupt change in a general dependent (noniid) stochastic process. The key contribu?

tions of this chapter are:

> The proposal and solution of Lorden, Pollak, and Bayesian asymptotic minimair ro
bust quickest change detection problems with polynomial delay penalties in general
dependent processes with unknown postrchzmge conditional density parameters;

and

> The derivation of new asymptotic bounds on the Lorden, Pollak and Bayesian
polynomial delay costs of asymptotically minimax robust rules compared to asyrnp

totically optimal rules.

Importantly, we identify asymptotic solutions to our minimax robust problems using

The results presented in this chapter appear in the submitted journal paper [JQ].

 

Avemges are too colourless, mdeed too abstmct to may may,
to represent conmte ezpenemle.
?Frederic Manning, The Middle Parts of Fortune

As we discussed in Chapters 1 and 2, compared to mid?air collision avoidance sys
terns based on other technologies such as radar. vision?based mid?air collision avoidance
systems are likely to be smaller, lighter. cheaper, and more power e?icient. Whilst there
has been recent success in designing and testing vision?based aircraft detection systems
in [15,16,53757], vision?based aircraft manoeuvre detection during potential collision
scenarios is yet to be investigated. In this chapter, we investigate the use of parameter
estimation and quickest change detection techniques (similar to those we have developed

in Chapters 3, 4, and 5) in the application of vision?based aircraft manoeuvre detection.

The main contribution of this chapter is:

> The proposal and evaluation of adaptive and rohustnessinspired algorithms for

quickly detecting aircraft manoeuvres in video sequences.

Importantly, the adaptive aircraft manoeuvre detectors of this chapter are inspired by
generalised likelihood ratio (GLR) rules for quickest change detection (and the parameter

Preliminary versions of some results presented in this chapter are published in [C1] and [02].

Such low degradation is made possible due to the large number of features extracted and
me suitable normalization method proposed in this paper.

Sellondmutualplucement
We repeat the previous experiments on the other two scenarios described in ?REALDs
ISP dataset": sel?placement and mutual?placement. The results are presenmd in Table 5.
In me ?rst two columns, this table shows ?te type of data (or scenario where the data
were recorded) used for training and testing the proposed system.

Regarding the sel?placement scenario, this table shows 8.696 accuracy drop compared
to the ideal?placement in the original paper (from 97.0 to 88.496), but with our system,
this reduction is lower than 0.596 (from 99.1 to 98.996). When comparing these results to
the original paper, there is a big improvement (more man 1096, from 88.4 to 98.996) in the
sel?placement scenario, The new feature extraction module shows a very good robusts
ness against different sensor placements.

For me mutual?placement scenario, the results are considerably low in both works
(baseline and this paper) but the degradation obtained with the system proposed in this
paper is considerably smaller compared to the baseline system: our system shows a bet?
mt robustness. This degradation is ditferent depending on the number of mis?displaces
ments (4, 5, 6, and 7).

In these experiments, we have used the data recorded in the mutual scenario for train?
ing and testing me system, As we commented in ?REALDISP dataset?; only three out of
the 17 volunteers were recorded for mutual?displacemem scenario so the amount of dam

Table 5 Flnal experimental result for sell-placement and mutual-placement scenarios

 

Train set Test set Baseline {24h This paper:
Evaluation method: randoms Evaluation method:
partitioning subjxrwiw
Nullsamvily: truncated Nullsattivity- included

Accuracy % Frmeasure Accuracy % Fsmeasure

 

{WV/UV " call Julran "
?>
" dial 332654 "

W

 

 

 

 

This ?nal chapter of the tutorial part of the book will describe the construction of a recogniser
for simple voice dialling apphcatious. This recogniser will be designed to recognise continuously
spoken digit strings and a hrnited set of names. It is sub?Word based so that adding a new name to
the vocabulary involves only modification to the pronouncing dictionary and task grammar. The
HMMs will be continuous density mixture Gaussian tied?state triphones with clustering performed
using phonetic decision trees. Although the voice dialling task itself is quite simple, the system
design is general?pulpose and would be useful for a range of apphcations.

The system will be built from scratch even to the extent of recording training and test data
using the HTK tool HSLAB. To make this tractable, the system will be speaker dependenl?, but
the same design would be followed to build a speaker independent system. The only ditference being
that data would be required from a large number of speakers and there would be a consequential
increase in model complexity.

Building a speech recogniser hem scratch involves a number of interrelated subtasks and ped?
agogicaily it is not obvious what the best order is to present them. In the presentation here, the
ordering is chronological so that in oifeet the text provides a recipe that could be followed to con?
struct a similar system. The entire process is described in considerable detail in order give a clear
view of the range of functions that HTK addresses and thereby to motivate the rest of the book.

The HTK software distribution also contains an example of constructing a recognition system
for the 1000 word ARPA Naval Resource Management Task. This is contained in the directory
llMl-I'X'K of the HTK distribution. Further demonstration of HTK?s capahihties can be found in the
directory HTKDemo. Some example scripts that may be of assistance during the tutorial are available
in the iiTKTutorial directory.

At each step of the tutorial presented in this chapter, the user is advised to thoroughly read
the entire section before executing the commands, and also to consult the reference section for
each HTK tool being introduced (chapter 17). so that all command line options and arguments are
clearly understood.

5. Discussion on Issues and Future Challenges

Water usage disaggregation is the equivalent of non-intrusive electricity load monitoring, applied
in the water domain, but with an important difference: While electricity outlets can be monitored with
non-invasive, out-of-the-box meters, water fixtures are, in general, unpowered and more difficult to
wire to a data communication infrastructure. This entails battery-operated instrumentation and, in
turn, constrained communication capabilities. Moreover, when dealing with supervised classifiers, a
necessary step is fitting the model with labelled data In the case of water, this may require special
purpose sensors, plumbing, and battery-operated equipment to be installed. Unfortunately, in real
houses, it is not viable to install a flow switch in every fixture or a Closed Circuit Television (CCTV)
camera in every room just to fit the classification model because plumbing is expensive and invasive.
Any viable approach should then comply with the principle of minimal installation requirements, and,
further, any sensors or equipment installed should already be an off-the-shelf product with a high
degree of acceptance among the general public. In summary, we can identify three requirements for
instrumenting a house with sensors:

. High acceptance (design, shape, part of shopping trends, identification of a user need)
. Low cost to buy and install
. Minimal or zero maintenance

All the works described in this survey challenges the previous state-of?the-art against classification
accuracy and are thus built on some hi-tech lab-level setup that requires continuous manual
intervention to ensure a reliable collection and processing of water data and ground truth.
To summarize:

Flow traces analysis [34] requires a data logger to be installed and then data should be manually
collected every 14 days, the data collected are then manually analysed and added to a database. It
seems a feasible solution to analyse a given period of time, but is not practical to perform online
disaggregation. HydroSense [38] reaches an accuracy of 8 ?u, but needs at least two days of ground
truth collectiono Their current approach trains the language model using data from the home where it

5.2.2. Data Fusion

In general, the classification of fused data can yield better results than the classification over
single data sources [59]. A promising direction of investigation is given by the nexus between energy
consumption, water consumption, and human presence in a house (also gas metering could be an
additional data source). In an extreme example, a 50% classification between laundry and gardening
could be better disambiguated by the analysis of instant energy consumption given that one of the
two activities uses energy and water at the same time. An example of the nexus between energy and
water is presented in Reference {60} In that paper, the authors leverage electricity non-intrusive load
monitoring (NILM) to acquire water disaggregation as a set of water/ energy correlated states.

5.2.3. Working at Scale

Applying standard rates derived from sample studies is misleading because of the high variability
in water use from one customer to another, even among customers with a similar infrastructure and
social-economic pro?le. The model extracted from a single house?s data is limited, and does not
leverage the information hidden in the broader population. What we consider parameters for a single
house (pipe size, extension of parcel, number of rooms, habits of tenants, ?le.) could be considered
as independent variables in a broader model, comprising a full set of properties in a city or region.
The collection of massive datasets for an entire city or region is, nowadays, technically feasible and
possible to maintain in the long ternL Hence, an interesting research question is to build and evaluate
large-scale models.

6. Conclusions

Non-intrusive water disaggregation is a valuable approach for estimating fixture-specific water
consumption, while keeping installation costs affordable, and, at the same time, the underlying
complexity of processing remains manageable. We have presented a review of water disaggregation
methods that make use of either mono?modal sensing or multi?modal sensing (e.g., combining different
variables, such as water flow, pressure, etc). The result of our review can be summarized in the
following conclusions:

OPPORTUNITY dataset

The OPPORTUNITY dataset contains data from four subjects, performing six di??erent
runs each of: ADLlfADLS and Drill. In the Drill run, subject must act in a predeterr
mined activity sequence and, as for ADLlfADLS, there is no restriction on the order
and number of activities, For each subject, there is information from three types of
sensors: bodyrworn sensors, object sensors and ambient sensors The onebody sensors
include 7 multiesensor inertial measurement units with another 12 3D acceleration sen?
sors: 145 signals in total, Since only body?worn sensors are concerned in the evaluation
section of the original paper [31], the data from object and ambient sensors are trun?
cated in the following experiments. In terms of activities or classes, this damsel has 3
ditterent sets: 4 types of locomotion (highrlevel activities); 17 types of gesture (midelevel
actions); and lowelevel actions to objects (which is ignored in this work),

Experiments on the OPPORTUNITY dataset

We retrain and evaluate our system using the same experimental setting as in the origi?
nal paper [31]: using ADLz and ADL3 from one subject as the testing set and use Drill,
ADLI, ADL4 and ADLs from the same subject as the training set. We conduct experi?
ments in this con?guration for all four subjects and in the two tasks: highelevel locos
motion (Table 7) and midelevel gestures (Table 8). The ?rst column shows the different
proposed systems, and the best systems are remarked with bold font.

For me highelevel locomotion task (Table 7), ?le system proposed in this paper obtains
?ue best results for all subjects when the Null class is not considered (the 4 last columns).
When including the Null class (the 4 ?rst columns), we obtain the best results for all
subjects except 83.

For the mid?level gesture task (Table 3), the system proposed in this paper obtains
the best results for all subiects except 54 when the Null class is included (the 4 ?rst

Table 7 Experimental results on the OPPORTUNITY dataset (hlgh-level locomntlnn classl-
?cauon)

itallrvaluer imitate the best remit; in this expevlment

columns), ln conclusion, the proposed sysmm is also a competitive solution for home
care monitoring applications,

Conclusions
This paper has proposed a HAR system for classifying 33 di?'erent physical activities
composed of two main modules: feature extraction and activity recognition modules.

The first contribution has been an analysis of several feature extraction strategies:
timerbased and frequencysbased. The timerbased features have provided betmr results
compared to the frequencyrbased ones, This paper has also evaluated several normalis
zation methods for reducing the degradation produced when training and testing with
different users. Thanks to the new feature extraction module and the normalization
strategy, the system has shown strong robustness when facing me Nullractivity and dif
ferent placement scenarios, two vital aspects for real applications.

Regarding the type ofsensor, the magnetometer signals have provided better discrimir
nation capability, The best results have been obtained when combining the information
from all the sensors. ln this case, the improvement is significant. The main experiments
have been done on a public available dataset, REALDISP Activity Recognition dataset.
Final results have exhibited that the proposed system largely improves me performance
compared to previous works on the same damset [24]. Under the best configuration, the
accuracy reaches 99.196 and Frmeasule 0.991.

The proposed system luas been also evaluated with another public dataset (OPPORr
TUNITY dataset) demonstrating competitive results (compared to previous work [31])
in two main tasks for home care monitoring: highrlevel locomotion and midrlevel gess
ture classi?catlon,

 

5. Discussion on Issues and Future Challenges

Water usage disaggregation is the equivalent of non-intrusive electricity load monitoring, applied
in the water domain, but with an important difference: While electricity outlets can be monitored with
non-invasive, out-of-the-box meters, water fixtures are, in general, unpowered and more difficult to
wire to a data communication infrastructure. This entails battery-operated instrumentation and, in
turn, constrained communication capabilities. Moreover, when dealing with supervised classifiers, a
necessary step is fitting the model with labelled data In the case of water, this may require special
purpose sensors, plumbing, and battery-operated equipment to be installed. Unfortunately, in real
houses, it is not viable to install a flow switch in every fixture or a Closed Circuit Television (CCTV)
camera in every room just to fit the classification model because plumbing is expensive and invasive.
Any viable approach should then comply with the principle of minimal installation requirements, and,
further, any sensors or equipment installed should already be an off-the-shelf product with a high
degree of acceptance among the general public. In summary, we can identify three requirements for
instrumenting a house with sensors:

. High acceptance (design, shape, part of shopping trends, identification of a user need)
. Low cost to buy and install
. Minimal or zero maintenance

All the works described in this survey challenges the previous state-of?the-art against classification
accuracy and are thus built on some hi-tech lab-level setup that requires continuous manual
intervention to ensure a reliable collection and processing of water data and ground truth.
To summarize:

Flow traces analysis [34] requires a data logger to be installed and then data should be manually
collected every 14 days, the data collected are then manually analysed and added to a database. It
seems a feasible solution to analyse a given period of time, but is not practical to perform online
disaggregation. HydroSense [38] reaches an accuracy of 8 ?u, but needs at least two days of ground
truth collectiono Their current approach trains the language model using data from the home where it

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

unnecessary amount of radio pollution, which is, in turn, is blamed as a potential cancer cause. The
second one is with respect to violations of citizens? rights. Detractors accuse governments of being
driven by the interest of suppliers and that the smart metering roadmap has been laid out without
public consultation, in violation of the spirit of shared consensus and democracy [54]. The last concern,
and probably the one with a proven impact, is that of privacy. There are many examples of how
high-resolution metering could be used to identify personal habits and retrieve personal information.
Notable proof of this concept is shown in Reference [55], where TV programs actually watched by
home occupants is inferred by correlating features such as the luminosity of scenes to high-resolution
energy consumption data. An approach to increase acceptance of industrial-level smart and cognitive
meters a viable solution is twofold:

- Give control to end users (they must be able to switch on/ off the metering; to set up the resolution;
to control the amount of radio messaging inside the property, etc.)

. Locally process most of the data and locally reveal the insights needed by end users to monitor
and improve their water demand. Powerful insight, such as usage disaggregation, could occur
in-home rather than being inferred remotely. This allows to send, to the supplier, only the
strictly-necessary data for operation (for instance daily average consumption over a week).

However, the above-mentioned approach does not take into full account the detailed needs of
water suppliers, as the focus is mainly on user privacy. Thus, as explained in Reference [56], developing
a context-specific framework for assessing how the collection and processing of detailed water usage
impacts the users privacy, and identifying a set of best practices to mitigate the impact is of paramount
importance. We expect that this issue will be addressed as soon as smart metering and cognitive
metering become ubiquitously available for the adaptive management of urban water resources.

52, A Few Promising Research Directions towards Real World Adoption

In this section, some promising directions of further investigation are described. The general
rationale is not to encourage competition in classification techniques to achieve 100% accuracy, but
rather to bridge the gaps for water disaggregation to become a viable tool in real world environments.

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Activity recognition
The goal of activity recognition is to recognize common human activities in real life

settings. Accurate activity recognition is challenging because human activity is complex
and highly diverse. Several probability-based algorithms have been used to build activity

 

models. The Hidden Markov Model and the Conditional Random Field are among the
most popular modeling techniques. We describe these two techniques in the context of an
eating activity example.

The Hidden Markov Model (HIVIM)

Simple activities can be modeled accurately as Markov Chains. However. complex or
unfamiliar activities are o?en dif?cult to rnrderstand and model. For example. a
researcher studying activities of daily living for a person vtu'th dementia will have a
dif?cult time ?tting a model unless she is an expert in dementia and turderstands its
related behavioral science. Fortunately. observing signals stemming from complex or
unfamiliar activities can be utilized to indirectly build a model of the activity. Such a
model is called a Hidden Markov Model or HMM. By observing the effects of an activity.
HMM is able to gradually construct the activity model. which can be further tuned.
extended and reused in similar studies.

Online learning is the process of answering a sequence of questions given knowledge of the correct
answers to previous questions and possibly additional available information Answering questions
in an intelligent fashion and being able to make rational decisions as a result is a basic feature of
everyday life. Will it rain today (so should I take an umbrella)? Should I ?ght the wild animal that
is after me, or should 1 mn away?? Should I open an attachment in an email message or is it a virus?
The study of online learning algorithms is thus an important domain in machine learning, and one
that has interesting theoretical properties and practical applications.

This dissertation describes a novel framework for the design and analysis of online learning
algorithms. We show that various online learning algorithms can all be derived as special cases of
our algorithmic framework. This uni?ed view explains the properties of existing algorithms and
also enables us to derive several new interesting algorithms.

Online learning is perforated in a sequence of consecutive rounds. where at each round the
learner is given a question and is required to provide an answer to this question. After predicting an
answer. the correct answer is revealed and the learner suffers a loss ifthere is a discrepancy between
his answer and the correct one.

The algorithmic framework for online learning we propose in this dissertation stems from a
connection that we make between the notions of regret in online learning and wr'ak duality in convex
optimization. Regret bounds are the common thread in the analysis of online learning algorithms.
A regret bound measures the performance of an online algorithm relative to the performance of a
competing prediction mechanism, called a competing hypothesis. The competing hypothesis can
be chosen in hindsight from a class of hypotheses. after observing the entire sequence of question?
answer pairs. Over the years, competitive analysis techniques have been re?ned and extended to
numerous prediction problems by employing complex and varied notions of progress toward a good

competing hypothesis.

for an optimization problem. in which we search for the optimal competing hypothesis. While the

optimal competing hypothe.

 

can only be found in hindsight, after observing the entire sequence
of question~answer pairs. this viewpoint relates regret bounds to lower bounds of minimization
problems.

The notion of duality. commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem. By generalizing the
notion of Fenchel duality. we are able to derive a dual optimization problem. which can be opti~
mized incrementally, as the online learning progresses. The main idea behind our derivation is the
connection between regret bounds and Fenchel duality. This connection leads to a reduction from
the process of online learning to the task of incrementally ascending the dual objective function.

In order to derive explicit quantitative regret bounds we make use of the weak duality prop?
erty. which tells us that the dual objective lower bounds the primal objective. The analysis of our
algorithmic framework uses the increase in the dual for assessing the progress of the algorithm.
This contrasts most if not all previous works that have analyzed online algorithms by measuring the
progress of the algorithm based on the correlation or distance between the online hypotheses and a
competing hypothesis.

We illustrate the power of our framework by deriving various learning algorithms. Our frame?
work yields the tightest known bounds for several known online learning algorithms. Despite the
generality ofour framework, the resulting analysis is more distilled than earlier analyses. The frame?
work also serves as a vehicle for deriving various new algorithms. First. we obtain new algorithms
for classic prediction problems by utilizing different techniques for ascending the dual objective.
We further propose ef?cient optimization procedures for performing the resulting updates of the
online hypotheses. Second, we derive novel algorithms for complex prediction problems, such as

ranking and structured output prediction.

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

Section 5

Object Detection from video

The motion of object can be detected after the object is detected from Video. Trackmg the 301?le
or object from sequence Video frames this is the main goal of Video tracking. Blob tracking.
keinel-based tracking. Contour tracking are some common target representation and localization
algorithms. Ruolm Zhang [11] has proposed adaptive background subtraction about the Video
detecting and tracking moving object. He use median ?lter to achieve the backgroiuid subtraction,
This algorithm is used for both detecting and tracking moving objects in sequence of video. This
algorithm never suppoit for multi feature based object detection. Hong Lu and Hong Slieng Li
[12] were introduced a new approach to detect and track the moving object. The defme motion
model and the non-parameter distribution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kahnan filter estnnati g
its a?'me motion in next frame. The author shows Experimental results and proof the new method
can successfully track the object iuider such case as merging. splitting. scale Variation and scene
noise, The author Bayan [13] talks about adaptive mean shirt for automated multi tracking. The
bene?t of Gaussian mixture model is that it extracted Foreground image from video Er ine
sequence it also eliminate the shadow and noise from video sequence It is helpful in initiali 'ng
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from Video and hence we can track the object easily. The object can trap from
Video by changes in size and shape

In this paper section] gives introduction about HAR and gives motto of paper: over view of
traditional HMM classifier in section 2. Section 3 gives overview about HMM-based approach
that uses threshold aird voting and section 4 gives over view about HZMM-NN and NN-HMM.

Section 5 contaim the review of how object can be detected from other ways. The result of all
methods as conclusion in section 6 Sections ' contain references

Section 2

TRADITIONAL HMM (TLAssmIR

A hidden Markov model (HIVIM) is a statistical Markov model in which the system being
modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be
considered as the simplest dynamic Bayesian network. The logic behind the HMM was
developed by L. E. Baum and coworkers. It is nearly dependent on an earlier work on optimal
nonlinear ?ltering problem (stochastic processes) proposed by Ruslan L Stratonovich. who was
the ?rst to describe the forward-backward procedure [14]

In a regular Markov model. the state is directly visible to the observer. and therefore the state
transition probabilities are the only parameters. In a hidden Markov model. the state is not
directly visible. but output. dependent on the state. is visible. Each state has a probability
distribution over the possible output tokens. Therefore the sequence of tokens generated by an
HMM gives some infoiination about the sequence of states. Note that the word ?hidden' is refers
for the state sequence through which the model is passes. not for the parameters of the model.

Hidden Markov models are especially known for then" application in temporal pattern recognition
such as speech. handwriting. gesture recognition. pai?t-of-speech tagging. musical score following
partial discharges and bioinformatics

A hidden Markov model can be considered a generalization of a mixture model where the hidden
variables (or latent variables). which control the mixttu?e component to be selected for each
obseivation. are related through a Markov process rather than independent of each other.

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

This introduction presents an overview of the online learning model and the contributions of this
dissertation. The main concepts introduced here are covered in depth and more rigorously in later

chapters.

1.1 Online Learning

Online learning takes place in a sequence of consecutive rounds. On each round. the learner is
given a question and is required to provide an answer to this question. For example. a learner might
receive an encoding of an email message and the question is whether the email is spam or not.
To answer the question, the learner uses a prediction mechanism, termed a hypothesis. which is a
mapping from the set of questions to the set of admissible answers. After predicting an answer.
the learner gets the correct answer to the question. The quality of the leamer?s answer is assessed
by a loss function that measures the discrepancy between the predicted answer and the correct one.
The learner's ultimate goal is to minimize the cumulative loss suffered along its run. To achieve
this goal. the learner may update the hypothesis after each round so as to be more accurate in later
rounds.

As mentioned earlier, the perfomtance of an online learning algorithm is measured by the cu~
mulative loss suffered by the learning along his run on a sequence of question-answer pairs. We
also use the term example to denote a question?answer pair. The learner tries to deduce information
from previous examples so as to improve its predictions on present and future questions. Clearly,
learning is hopeless if there is no correlation between past and present examples. Classic statistical
theory of sequential prediction therefore enforces strong assumptions on the statistical properties of

the input sequence (for example, it must form a stationary stochastic process).

ABSTRACT."

The rapid nnprovement m teahnologv Causes more attention towards to Remgnrzmg ofhuman aenvmes
?'om video. These new teehnolagreaz growth has made vision?based research much more mterestrng anal
e/?ctent than ever before. ihts paper present navel HMM (Hmaen Markov Model) based approach for
Human amwty r-eaagnraon from video. zhere are different approaches ofHMM to rerogn'ue ac?mi of
human from video. ere threshold and voting to automanmlly ana e?eetrvezy segment and reeogntze
sampler aanwtres, segment and recognwe eampten aatwmes and ?u srmple aammes we use Elmnn
Network (EN) and two hyanas ofNeuraINetwork (NN) andHll/lM, re. HMM?NN and NNI?VIM.

KEY Worms:

Human Activity reaagnmon, Hidden Markov Model, Hybrid model afHMM, Image mpmrmg from Video,
complex activity,

 

TRODUCTI

 

Automatically recognizing human activities from video is important for applications such as
automated suiyeillance systems and smart home applications. Several human activity recogn' 'on
methods [l][2][3][4][5][6] were proposed in the past few years to classify single human actitities
such as walking skipping. sitting down. etc. Human activity recognition (HAR) research has
been on the use because of the rapid technological development of the image-capturing software
and hardware. in addition to the omnipresence of reasonably low-cost high-performance personal
computers. The main goal of tlns recognition is used to develop the different application which
make human machine interaction is easy and interesting.

     

In the jouiney of developing algorithms for human activity recognition. some new developed
algorithms adds some new features in pre?ously developed algorithm. In this paper. we present a
novel HMM-based approach that uses threshold and wring to automatically and effectively
segment and recognize complex activities. And also survey on two hybrids of Neiu?al Network
(NN) and HMM. e. HMNI?NN and 3 ??]-]]\IM. This paper also compares their perfoimance
with that of the traditional HMM.

   

1.2.2 Problem Type

The Perceptron algorithm was originally designed for answering yes/no questions. In real?world
applications we are often interested in more complex answers. For example, in multiclass categov
rization tasks, the learner needs to choose the correct answer out of k possible answers.

Simple adaptations of the Perceptron for multiclass categorization tasks date back to Kessler's
construction [44]. Crammer and Singer [31] proposed more sophisticated variants of the Perceptron
for multiclass categorization. The usage of online learning for more complex prediction problems
has been further addressed by several authors. Some notable examples are multidimensional regres?
sion [76], discriminative training of Hidden Markov Models [23]. and ranking problems [28, 29].

1.2.3 Aggressiveness Level

The update procedure used by the Perceptron is extremely simple and is rather conservative. First,
no update is made if the predicted answer is correct. Second. all instances are added (subtracted)
from the weight vector with a unit weight. Finally. only the most recent example is used for updating
the weight vector. Older examples are ignored.

Krauth [78] proposed aggressive variants of the Perceptron in which updates are also perfonned
if the Perceptron's answer is correct but the input instance lies too close to the decision boundary.
The idea of twing to push instances away from the decision boundary is central to the Support
Vector Machines literature [1 17. 33, 100]. In addition. various authors [68. 57. 80, 74. 103. 31, 28]
suggested using more sophisticated learning rates. i.e., adding instances to the weight vector with
different weights.

Finally. early works in game theory derive strategies for playing repeated games in which all
past examples are used for updating the hypothesis. The most notable is followfthevleader ap-
proaches [63].

Our framework emerges from a new View on regret bounds, which are the common thread in
the analysis of online learning algorithms. As mentioned in Section 1.1, a regret bound measures
the performance of an online algorithm relative to the performance of a competing hypothesis. The
competing hypothesis can be chosen in retrospect from a class of hypotheses. after observing the
entire sequence of examples.

We propose an alternative View of regret bounds that is based on the notion of duality in con?
vex optimization. Regret bounds are universal in the sense that they hold for any possible ?xed
hypothesis in a given hypothesis class We therefore cast the universal bound as a lower bound for
an optimization problem. Speci?cally, the cumulative loss of the online learner should be bounded
above by the minimum value of an optimization problem in which we jointly minimize the cu?
mulative loss and a ?complexity" measure of a competing hypothesis. Note that the optimization
problem can only be solved in hindsight after observing the entire sequence of examples. Neverthe?
less. this viewpoint implies that the cumulative loss of the online learner fomts a lower bound for a
minimization problem.

The notion of duality, commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem (see for example [89]).
By generalizing the notion of Fenchel duality, we are able to derive a dual optimization problem,
which can be optimized incrementally as the online learning progresses. In order to derive explicit
quantitative regret bounds we make immediate use of the weak duality property, which tells us that
the dual objective lower bounds the primal objective. We therefore reduce the process of online
learning to the task of incrementally increasing the dual objective function. The amount by which
the dual increases serves as a new and natural notion of progress. By doing so we are able to
associate the cumulative loss of the competing hypothesis (as re?ected by the primal objective

value) and the cumulative loss of the online algorithm, using the increase in the dual.

In most of this dissertation we make no statistical assumptions regarding the origin of the se?

quence of examples. We allow the sequence to be determini

 

stochastic. or even adversarially
adaptive to our own behavior (as in the case of spam email ?ltering). Naturally. an adversary can
make the cumulative loss of our online learning algorithm arbitrarily large. For example, the adver?
sary can ask the same question on each online round. wait for the learner?s answer, and provide the
opposite answer as the correct answer. To overcome this de?ciency. we restate the learner's goal
based on the notion of regret. To help understand this notion, note that the learners prediction on
each round is based on a hypothesis. The hypothesis is chosen from a prede?ned class of hypothe?
ses. In this class, we de?ne the optimal ?xed hypothesis to be the hypothesis that minimizes the
cumulative loss over the entire sequence of examples. The learner's regret is the difference between
his cumulative loss and the cumulative loss of the optimal ?xed hypothesis. This is termed "regret?
since it measures how 'sorry' the learner is. in retrospect, not to have followed the predictions of the
optimal hypothesis. In the example above, where the adversary makes the learners cumulative loss
arbitrarily large, any competing ?xed hypothesis would also suffer a large cumulative loss. Thus,
the learner?s regret in this case would not be large.

This dissertation presents an algorithmic framework for online learning that guarantees low
regret. Speci?cally. we derive several bounds on the regret of the proposed online algorithms. The
regret bounds we derive depend on certain properties of the loss functions, the hypothesis class, and

the number of rounds we run the online algorithm.

1.2 Taxonomy of Online Learning Algorithms

Before delving into the description of our algorithmic framework for online learning. we would like

to highlight connections to and put our work in context of some of the more recent work on online

hold as long as we have a suf?cient increment in the dual objective. By monitoring the increase in
the dual we are able to control the aggressiveness level of the resulting online learning algorithm.
To make this dissertation coherent and due to the lack of space, some of my research work was
omitted from this thesis. For example, I have also worked on boosting and online algorithms for
regression problems with smooth loss functions [SE]. 40]. online learning of pseudovmetrics [109],
online learning ofprediction suf?x trees [39], online learning with various notions of margin [103],
online learning with simultaneous projections [4], online learning with kernels on a budget [41],

and stochastic optimization using online learning techniques [1 10].

1.4 Outline

The dissertation is divided into three main parts. titled Theory. Algorithms, and Applications. In
each part, there are several chapters. The last section of each of the chapters includes a detailed

review of previous work relevant to the speci?c contents described in the chapter.

1.4.1 Part 1: Theory

In the theory part. we derive and analyze our algorithmic framework in its most general form We
start in Chapter 2 with a formal description of online learning and regret analysis. We then describe
a more abstract framework called online convex programming and cast online learning as a special
case of online convex programming. As its name indicates, online convex programming relies on
convexity assumptions. We describe a common construction used when the natural loss function for

an online learning task is not convex.

PAC learning framework. For completeness, in Chapter B given in the appendix. we discuss the
applicability of our algorithmic framework to the PAC learning model. We start this chapter with
a short introduction to the PAC learning model. Next, we discuss the relative dif?culty of online
learning and PAC learning. Finally. we propose general conversion schemes from online learning
to the PAC setting.

1.4.2 Part II: Algorithms

The second part is (levoted to more speci?c algorithms and implementation details. We start in
Chapter 5 by deriving speci?c algorithms from our general algorithmic framework. In particular?,
we demonstrate that by varying the three components of the general framework we can design
algorithms with different update types, different aggressiveness levels. and for different problem
types.

Next. in Chapter 6 we show the applicability of our analysis for deriving boosting algorithms.
While boosting algorithms do not fall under the online learning model. our general analysis ?ts
naturally to general primal?dual incremental methods. As we discuss, the process of boosting can
be viewed as a primal~dual game between a weak learner and a booster.

Finally. in Chapter 7 we discuss the computational aspects of the different update schemes.
Depending on the loss function and update scheme at hand, we derive procedures for performing

the update with increasing computational complexity.

1.4.3 Part III: Applications

In the last part of the dissertation we demonstrate the applicability of our algorithms to real world

problems. We start with the problem of online email categorization. which is a natural online

PAC learning framework. For completeness, in Chapter B given in the appendix. we discuss the
applicability of our algorithmic framework to the PAC learning model. We start this chapter with
a short introduction to the PAC learning model. Next, we discuss the relative dif?culty of online
learning and PAC learning. Finally. we propose general conversion schemes from online learning
to the PAC setting.

1.4.2 Part II: Algorithms

The second part is (levoted to more speci?c algorithms and implementation details. We start in
Chapter 5 by deriving speci?c algorithms from our general algorithmic framework. In particular?,
we demonstrate that by varying the three components of the general framework we can design
algorithms with different update types, different aggressiveness levels. and for different problem
types.

Next. in Chapter 6 we show the applicability of our analysis for deriving boosting algorithms.
While boosting algorithms do not fall under the online learning model. our general analysis ?ts
naturally to general primal?dual incremental methods. As we discuss, the process of boosting can
be viewed as a primal~dual game between a weak learner and a booster.

Finally. in Chapter 7 we discuss the computational aspects of the different update schemes.
Depending on the loss function and update scheme at hand, we derive procedures for performing

the update with increasing computational complexity.

1.4.3 Part III: Applications

In the last part of the dissertation we demonstrate the applicability of our algorithms to real world

problems. We start with the problem of online email categorization. which is a natural online

Appendix B

Using Online Convex Programming for
PAC learning

In this chapter we outline the applicability of our algorithmic framework from the previous chapter
to Probably Approximately Correu (PAC) learning [116]. The PAC setting is also referred to as
hart/1 learning. We start this chapter with a short introduction to PAC learning. Next, in Section B2
we relate two important notions of learnahiliry: the notion of mistake bound in online learning
and the notion of VC dimension used in PAC learning. Based on this comparison. we demonstrate
that online learning is more dif?cult than PAC learning. Despite this disparity. as we have shown
in previous chapters. many classes of hypotheses can be ef?ciently learned in the online mode].
In Section B} we derive several simple conversions from the online setting to the batch setting.
The end result is that the existence of a good online learner implies the existence of good hatch
learning algorithms. These online?to?batch conversions are general and do not necessarily rely on
our speci?c algorithms for online learning.

B.1 Brief Overview of PAC Learning

hold as long as we have a suf?cient increment in the dual objective. By monitoring the increase in
the dual we are able to control the aggressiveness level of the resulting online learning algorithm.
To make this dissertation coherent and due to the lack of space, some of my research work was
omitted from this thesis. For example, I have also worked on boosting and online algorithms for
regression problems with smooth loss functions [SE]. 40]. online learning of pseudovmetrics [109],
online learning ofprediction suf?x trees [39], online learning with various notions of margin [103],
online learning with simultaneous projections [4], online learning with kernels on a budget [41],

and stochastic optimization using online learning techniques [1 10].

1.4 Outline

The dissertation is divided into three main parts. titled Theory. Algorithms, and Applications. In
each part, there are several chapters. The last section of each of the chapters includes a detailed

review of previous work relevant to the speci?c contents described in the chapter.

1.4.1 Part 1: Theory

In the theory part. we derive and analyze our algorithmic framework in its most general form We
start in Chapter 2 with a formal description of online learning and regret analysis. We then describe
a more abstract framework called online convex programming and cast online learning as a special
case of online convex programming. As its name indicates, online convex programming relies on
convexity assumptions. We describe a common construction used when the natural loss function for

an online learning task is not convex.

1.6 Bibliographic Notes

How to predict rationally is a key issue in various research areas such as game theory. machine
learning. and information theory. In this section we give a high level overview of related work in
different research ?elds. The last section of each of the chapters below includes a detailed review
of previous work relevant to the speci?c contents of each chapter.

ln game theory. the problem of sequential prediction has been addressed in the context of playing
repeated games with mixed strategies. A player who can achieve low regret (i.e. whose regret grows
sublinearly with the number of rounds) is called a Harman consistent player [63]. Hannan consistent
strategies have been obtained by Harman [63]. Blackwell [9] (in his proof of the approachability
theorem). Foster and Vohra [49, 50], Freund and Schapire [55]. and Hart and Mas?collel [64]. Von
Neumann?s classical minimax theorem has been recovered as a simple application of regret bounds
[55]. The importance of low regret strategies was further ampli?ed by showing that if all players
follow certain low regret strategies then the game converges to a correlated equilibrium (see for
example [65, 10]). Playing repeated games with mixed strategies is closely related to the expert
setting widely studied in the machine learning literature [41 82, 85, 119].

Prediction problems have also intrigued information theorists since the early days of the in?
formation theory ?eld. For example. Shannon estimated the entropy of the English language by
letting humans predict the next symbol in English texts [I l 1]. Motivated by applications of data
compression, Ziv and Lempel [124] proposed an online universal coding system for arbitrary in-
dividual sequences. In the compression setting. the learner is not committed to a single prediction
but rather assigns a probability over the set of possible outcomes. The success of the coding system
is measured by the total likelihood of the entire sequence of symbols. Feder, Merhav. and Gutman
[47] applied universal coding systems to prediction problems. where the goal is to minimize the

number of prediction errors. Their basic idea is to use an estimation of the conditional probabilities

1.6 Bibliographic Notes

How to predict rationally is a key issue in various research areas such as game theory. machine
learning. and information theory. In this section we give a high level overview of related work in
different research ?elds. The last section of each of the chapters below includes a detailed review
of previous work relevant to the speci?c contents of each chapter.

ln game theory. the problem of sequential prediction has been addressed in the context of playing
repeated games with mixed strategies. A player who can achieve low regret (i.e. whose regret grows
sublinearly with the number of rounds) is called a Harman consistent player [63]. Hannan consistent
strategies have been obtained by Harman [63]. Blackwell [9] (in his proof of the approachability
theorem). Foster and Vohra [49, 50], Freund and Schapire [55]. and Hart and Mas?collel [64]. Von
Neumann?s classical minimax theorem has been recovered as a simple application of regret bounds
[55]. The importance of low regret strategies was further ampli?ed by showing that if all players
follow certain low regret strategies then the game converges to a correlated equilibrium (see for
example [65, 10]). Playing repeated games with mixed strategies is closely related to the expert
setting widely studied in the machine learning literature [41 82, 85, 119].

Prediction problems have also intrigued information theorists since the early days of the in?
formation theory ?eld. For example. Shannon estimated the entropy of the English language by
letting humans predict the next symbol in English texts [I l 1]. Motivated by applications of data
compression, Ziv and Lempel [124] proposed an online universal coding system for arbitrary in-
dividual sequences. In the compression setting. the learner is not committed to a single prediction
but rather assigns a probability over the set of possible outcomes. The success of the coding system
is measured by the total likelihood of the entire sequence of symbols. Feder, Merhav. and Gutman
[47] applied universal coding systems to prediction problems. where the goal is to minimize the

number of prediction errors. Their basic idea is to use an estimation of the conditional probabilities

5.2.2. Data Fusion

In general, the classification of fused data can yield better results than the classification over
single data sources [59]. A promising direction of investigation is given by the nexus between energy
consumption, water consumption, and human presence in a house (also gas metering could be an
additional data source). In an extreme example, a 50% classification between laundry and gardening
could be better disambiguated by the analysis of instant energy consumption given that one of the
two activities uses energy and water at the same time. An example of the nexus between energy and
water is presented in Reference {60} In that paper, the authors leverage electricity non-intrusive load
monitoring (NILM) to acquire water disaggregation as a set of water/ energy correlated states.

5.2.3. Working at Scale

Applying standard rates derived from sample studies is misleading because of the high variability
in water use from one customer to another, even among customers with a similar infrastructure and
social-economic pro?le. The model extracted from a single house?s data is limited, and does not
leverage the information hidden in the broader population. What we consider parameters for a single
house (pipe size, extension of parcel, number of rooms, habits of tenants, ?le.) could be considered
as independent variables in a broader model, comprising a full set of properties in a city or region.
The collection of massive datasets for an entire city or region is, nowadays, technically feasible and
possible to maintain in the long ternL Hence, an interesting research question is to build and evaluate
large-scale models.

6. Conclusions

Non-intrusive water disaggregation is a valuable approach for estimating fixture-specific water
consumption, while keeping installation costs affordable, and, at the same time, the underlying
complexity of processing remains manageable. We have presented a review of water disaggregation
methods that make use of either mono?modal sensing or multi?modal sensing (e.g., combining different
variables, such as water flow, pressure, etc). The result of our review can be summarized in the
following conclusions:

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

5. Discussion on Issues and Future Challenges

Water usage disaggregation is the equivalent of non-intrusive electricity load monitoring, applied
in the water domain, but with an important difference: While electricity outlets can be monitored with
non-invasive, out-of-the-box meters, water fixtures are, in general, unpowered and more difficult to
wire to a data communication infrastructure. This entails battery-operated instrumentation and, in
turn, constrained communication capabilities. Moreover, when dealing with supervised classifiers, a
necessary step is fitting the model with labelled data In the case of water, this may require special
purpose sensors, plumbing, and battery-operated equipment to be installed. Unfortunately, in real
houses, it is not viable to install a flow switch in every fixture or a Closed Circuit Television (CCTV)
camera in every room just to fit the classification model because plumbing is expensive and invasive.
Any viable approach should then comply with the principle of minimal installation requirements, and,
further, any sensors or equipment installed should already be an off-the-shelf product with a high
degree of acceptance among the general public. In summary, we can identify three requirements for
instrumenting a house with sensors:

. High acceptance (design, shape, part of shopping trends, identification of a user need)
. Low cost to buy and install
. Minimal or zero maintenance

All the works described in this survey challenges the previous state-of?the-art against classification
accuracy and are thus built on some hi-tech lab-level setup that requires continuous manual
intervention to ensure a reliable collection and processing of water data and ground truth.
To summarize:

Flow traces analysis [34] requires a data logger to be installed and then data should be manually
collected every 14 days, the data collected are then manually analysed and added to a database. It
seems a feasible solution to analyse a given period of time, but is not practical to perform online
disaggregation. HydroSense [38] reaches an accuracy of 8 ?u, but needs at least two days of ground
truth collectiono Their current approach trains the language model using data from the home where it

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

unnecessary amount of radio pollution, which is, in turn, is blamed as a potential cancer cause. The
second one is with respect to violations of citizens? rights. Detractors accuse governments of being
driven by the interest of suppliers and that the smart metering roadmap has been laid out without
public consultation, in violation of the spirit of shared consensus and democracy [54]. The last concern,
and probably the one with a proven impact, is that of privacy. There are many examples of how
high-resolution metering could be used to identify personal habits and retrieve personal information.
Notable proof of this concept is shown in Reference [55], where TV programs actually watched by
home occupants is inferred by correlating features such as the luminosity of scenes to high-resolution
energy consumption data. An approach to increase acceptance of industrial-level smart and cognitive
meters a viable solution is twofold:

- Give control to end users (they must be able to switch on/ off the metering; to set up the resolution;
to control the amount of radio messaging inside the property, etc.)

. Locally process most of the data and locally reveal the insights needed by end users to monitor
and improve their water demand. Powerful insight, such as usage disaggregation, could occur
in-home rather than being inferred remotely. This allows to send, to the supplier, only the
strictly-necessary data for operation (for instance daily average consumption over a week).

However, the above-mentioned approach does not take into full account the detailed needs of
water suppliers, as the focus is mainly on user privacy. Thus, as explained in Reference [56], developing
a context-specific framework for assessing how the collection and processing of detailed water usage
impacts the users privacy, and identifying a set of best practices to mitigate the impact is of paramount
importance. We expect that this issue will be addressed as soon as smart metering and cognitive
metering become ubiquitously available for the adaptive management of urban water resources.

52, A Few Promising Research Directions towards Real World Adoption

In this section, some promising directions of further investigation are described. The general
rationale is not to encourage competition in classification techniques to achieve 100% accuracy, but
rather to bridge the gaps for water disaggregation to become a viable tool in real world environments.

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

OPPORTUNITY dataset

The OPPORTUNITY dataset contains data from four subjects, performing six di??erent
runs each of: ADLlfADLS and Drill. In the Drill run, subject must act in a predeterr
mined activity sequence and, as for ADLlfADLS, there is no restriction on the order
and number of activities, For each subject, there is information from three types of
sensors: bodyrworn sensors, object sensors and ambient sensors The onebody sensors
include 7 multiesensor inertial measurement units with another 12 3D acceleration sen?
sors: 145 signals in total, Since only body?worn sensors are concerned in the evaluation
section of the original paper [31], the data from object and ambient sensors are trun?
cated in the following experiments. In terms of activities or classes, this damsel has 3
ditterent sets: 4 types of locomotion (highrlevel activities); 17 types of gesture (midelevel
actions); and lowelevel actions to objects (which is ignored in this work),

Experiments on the OPPORTUNITY dataset

We retrain and evaluate our system using the same experimental setting as in the origi?
nal paper [31]: using ADLz and ADL3 from one subject as the testing set and use Drill,
ADLI, ADL4 and ADLs from the same subject as the training set. We conduct experi?
ments in this con?guration for all four subjects and in the two tasks: highelevel locos
motion (Table 7) and midelevel gestures (Table 8). The ?rst column shows the different
proposed systems, and the best systems are remarked with bold font.

For me highelevel locomotion task (Table 7), ?le system proposed in this paper obtains
?ue best results for all subjects when the Null class is not considered (the 4 last columns).
When including the Null class (the 4 ?rst columns), we obtain the best results for all
subjects except 83.

For the mid?level gesture task (Table 3), the system proposed in this paper obtains
the best results for all subiects except 54 when the Null class is included (the 4 ?rst

Table 7 Experimental results on the OPPORTUNITY dataset (hlgh-level locomntlnn classl-
?cauon)

itallrvaluer imitate the best remit; in this expevlment

columns), ln conclusion, the proposed sysmm is also a competitive solution for home
care monitoring applications,

Conclusions
This paper has proposed a HAR system for classifying 33 di?'erent physical activities
composed of two main modules: feature extraction and activity recognition modules.

The first contribution has been an analysis of several feature extraction strategies:
timerbased and frequencysbased. The timerbased features have provided betmr results
compared to the frequencyrbased ones, This paper has also evaluated several normalis
zation methods for reducing the degradation produced when training and testing with
different users. Thanks to the new feature extraction module and the normalization
strategy, the system has shown strong robustness when facing me Nullractivity and dif
ferent placement scenarios, two vital aspects for real applications.

Regarding the type ofsensor, the magnetometer signals have provided better discrimir
nation capability, The best results have been obtained when combining the information
from all the sensors. ln this case, the improvement is significant. The main experiments
have been done on a public available dataset, REALDISP Activity Recognition dataset.
Final results have exhibited that the proposed system largely improves me performance
compared to previous works on the same damset [24]. Under the best configuration, the
accuracy reaches 99.196 and Frmeasule 0.991.

The proposed system luas been also evaluated with another public dataset (OPPORr
TUNITY dataset) demonstrating competitive results (compared to previous work [31])
in two main tasks for home care monitoring: highrlevel locomotion and midrlevel gess
ture classi?catlon,

 

for training the system is very small (2 out of the 3 subjects recorded in this scenario).
In order to analyze the in?uence of the amount of data, we repeat the same experiments
but using the ideal?p|acement data for training the system. Although there is a mismatch
in the conditions, the amount of available data for training would increase a lot (from 2
to 16 subjects). The experiments are shown in Table 6. In the ?Train Set" and ?Test Set"
columns, we have also included the number of subjects considered for training and mstr
ing the system.

The results show that when being tmined with ideal datasets and tested with mutual
damsets, the system reaches a very good accuracy though the training and testing sets
come from diti'erent placement scenarios. For example, for mutual4, the accuracy goes
from 87.9 to 99.096 (the ?rst row). These results support the hypothesis that the amount
of data for training is an important factor in the system performance.

with the idea of cross?dataset experiment, we go further on me idealrplacement and
sel?placement scenarios (the last row in Table 6). As Table 6 shows, there is not a signi??
cant di?'erence on the accuracy when testing with sel?placement dataset and training
with ideal or self placement (99.1 vs. 98,936, di?'erence lower than the con?dence interval
0.596). In this case, the amount of data available in ideal?placement and sel?placement
scenarios is me same.

System analysis in a new domain: home care monitoring

In the introduction, we commented two main applications of HAR: physical exercise
monitoring and home care monitoring. The REALDISP dataset is focused on the ?rst
one: physical exercise monitoring, In order to verify the viability of the proposed system
in a home care monitoring application, we have evaluated the best system con?guration
with another dataset: the OPPORTUNITY dataset for HAR from wearable, object, and
ambient sensors [30]. The recordings include daily morning activities: getting up from
the bed, preparing and having breakfast (a coffee and a salami sandwich) and clean?
ing the kitchen latter. This dataset is a very popular l-[AR dataset on thls research field.
There is no constraining on the location or body posture in any ofthe scripted activities.

number of sensors and devices used in these studies were significantly high, our proposal is focused
on answering the question whether it is possible to obtain similar or better results regarding occupancy
detection using fewer resources. Even more, our proposal uses a non~intrusive ground truth strategy
to avoid jeopardising the privacy and security of the occupants in the area of interest.

This research uses as inspiration some of the ideas proposed in the works discussed, in particular,
the gathering of data from different sources to move forward to a complete analysis of the data collected
using a ML approach. Our goal is to detect occupancy in indoor environments by preprocessing the
datasets collected betore applying ML to a binary and mul?eclass problem. Additionally we design our
solution focused on two main requirements: the first one is to try to take advantage of cheap/affordable
devices commonly deployed in Smart Environment, and the second one is to guarantee that the privacy
of the occupants in the area under study would not be compromised, Thus, after the discussion of
the works in this research area, our proposal uses environmental features via the combination of data
gathered from different sensors. This solution is presented in the following section.

3, occupancy Detection in Indoor Environments

As discussed in the previous section, occupancy detection could be used to trigger some actuation
mechanisms in Smart Environments in order to improve resource usage and user experience, among
other factors, An important issue that must be considered is the preservation of privacy of the data
collected and analysed. Additionally, it would be desirable to take advantage of the infrastructure
available in the surroundings to avoid incurring in extra expenses, while allowing the scalability of the
solution. Considering these factors, this research is focused on a nonintrusive and inexpensive solution
for occupancy estimation that ensures occupants privacy wlule taking advantage of the technological
infrastructure already available in common Smart Environments including Smart Buildings and Smart
Homes. From the analysis performed on Section 2, and to comply with the previously established
requirements, our occupancy detection solution is focused on environmental data.

A scene analysis approach is used in this research to extract the features of interest for indoor
scenarios to proceed and then toest-imate the occupancy in the area using the gathered data [24]. The
scene analysis method does not rely on any theoretical model or specific hardware; however, it requires
a preliminary phase for capturing features which are in?uenced by changes in the area of interest [25].

in this section, we explain the criteria applied to select the features used in our solution before
moving forward to the description of the design of the four-layers architecture adopted for the
gathering and processing of the data. The section concludes with the discussion of the ML classifiers
that were selected to improve the performance of occupancy detection in indoor environments. Table 1
summarises the terms used in the remaining of the manuscript.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

Activity recognition
The goal of activity recognition is to recognize common human activities in real life

settings. Accurate activity recognition is challenging because human activity is complex
and highly diverse. Several probability-based algorithms have been used to build activity

 

models. The Hidden Markov Model and the Conditional Random Field are among the
most popular modeling techniques. We describe these two techniques in the context of an
eating activity example.

The Hidden Markov Model (HIVIM)

Simple activities can be modeled accurately as Markov Chains. However. complex or
unfamiliar activities are o?en dif?cult to rnrderstand and model. For example. a
researcher studying activities of daily living for a person vtu'th dementia will have a
dif?cult time ?tting a model unless she is an expert in dementia and turderstands its
related behavioral science. Fortunately. observing signals stemming from complex or
unfamiliar activities can be utilized to indirectly build a model of the activity. Such a
model is called a Hidden Markov Model or HMM. By observing the effects of an activity.
HMM is able to gradually construct the activity model. which can be further tuned.
extended and reused in similar studies.

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

Human Activity Recognition and Pattern
Discovery

Eunju Kim, Sumi Helal and Diane Cook

Activity recognition is an important technology in pervasive computing because it can be
applied to many real-life. human-centric problems such as eldercare and healthcare.
Successful research has so far focused on recognizing simple human activities.
Recognizing complex activities remains a challenging and active area of research.
Speci?cally. the nature of human activities poses the following challenges:

? Recognizing concurrent activities
People can do several activities at the same time [3]. For example. people can watch
television While talking to their friends. These behaviors should be recognized using
a different approach from that for sequential activity.

? Recognizing interleaved activities
Certain real life activities may be interleaved [3]. For instance. while cooking. if
there is a call from a friend. people pause cooking for a while and after talking to
their friend. they come back to the kitchen and continue to cook.

0 Aiiibiguiti? afiiiiei'pi?eiafian
Depending on the situation. the interpretation of similar activities may be different.
For example. an activity ?open refrigerator" can belong to several activities. such as
?cooking" or ?cleaning".

? rMultiple residents
In many enviromnents more than one resident is present. The activities that are being
performed by the residents in parallel need to be recognized. even if the activity is
performed together by the residents in a group.

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

 

Optimrmtion space of
one. model parameter

Fig. 2. An rrrustratron of me degenerandn that may occur with batch learning of a new block or data suppdse that the dotted curve represents the cost
function associated with a system trained on block D], and that the pie!? curve represents the cost funcnon associated with a system trained on the
eumuranve data Dr um. Point (1 ) represents the optimum sdrunon of batch learning performed on D,, wrnre point (4) is the optlmum solution forhatch
reamrng performed on o, uni. If pornt[1)|s used as a startlng point for rncrementar training on D1[pornt[2)),thenrt w|ll become trapped in the local
uptlmum at pornt(3).

would also lower time complexity needed to learn new data. Finally, incremental leaming may provide a powerful tool in a
human-centric approach, where domain experts may be called upon to gradually design and update HMMs as the opera-
tional environment unfolds.

?ihis paper contains a survey of techniques that apply to incremental learning of HMM parameters.2 These techniques are
classi?ed according to objective function, optimization technique, and target application that involve block-wise and symbol?
wise learning of parameters. An analysis of their convergence properties and of their time and memory complexity is presented.
The applicabi ty of these techniques is assessed for incremental learning scenarios in which new data is either abundant or
limited. Finally, the advantages and shortcomings of these techniques ar'e outlined, providing the key issues and guidelines
for their application in different learning scenarios.

?ihis paper is structured according to ?ve sections. The next section briefly reviews the batch learning techniques
employed to estimate HMM parameters, and introduces the formalism needed to support subsequent sections. ?ection 3
provides a taxonomy of on-line learning techniques form literature that apply for incremental learning of HMM parameters.
An analysis of their convergence properties and resource requirements is provided in Section 4. ?lhen. an analysis of their
potential applicability in different incremental learning scenarios is presented in Section 5. ?(his paper concludes with a
discussion of the main challenges to be addressed for incremental learning of HMM parameters.

 

A survey of techniques for incremental learning of HMM parameters

Wael Khreich ?V?, Eric Granger ?, Ali Miri", Robert Sabourina

Online learning is the process of answering a sequence of questions given knowledge of the correct
answers to previous questions and possibly additional available information Answering questions
in an intelligent fashion and being able to make rational decisions as a result is a basic feature of
everyday life. Will it rain today (so should I take an umbrella)? Should I ?ght the wild animal that
is after me, or should 1 mn away?? Should I open an attachment in an email message or is it a virus?
The study of online learning algorithms is thus an important domain in machine learning, and one
that has interesting theoretical properties and practical applications.

This dissertation describes a novel framework for the design and analysis of online learning
algorithms. We show that various online learning algorithms can all be derived as special cases of
our algorithmic framework. This uni?ed view explains the properties of existing algorithms and
also enables us to derive several new interesting algorithms.

Online learning is perforated in a sequence of consecutive rounds. where at each round the
learner is given a question and is required to provide an answer to this question. After predicting an
answer. the correct answer is revealed and the learner suffers a loss ifthere is a discrepancy between
his answer and the correct one.

The algorithmic framework for online learning we propose in this dissertation stems from a
connection that we make between the notions of regret in online learning and wr'ak duality in convex
optimization. Regret bounds are the common thread in the analysis of online learning algorithms.
A regret bound measures the performance of an online algorithm relative to the performance of a
competing prediction mechanism, called a competing hypothesis. The competing hypothesis can
be chosen in hindsight from a class of hypotheses. after observing the entire sequence of question?
answer pairs. Over the years, competitive analysis techniques have been re?ned and extended to
numerous prediction problems by employing complex and varied notions of progress toward a good

competing hypothesis.

for an optimization problem. in which we search for the optimal competing hypothesis. While the

optimal competing hypothe.

 

can only be found in hindsight, after observing the entire sequence
of question~answer pairs. this viewpoint relates regret bounds to lower bounds of minimization
problems.

The notion of duality. commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem. By generalizing the
notion of Fenchel duality. we are able to derive a dual optimization problem. which can be opti~
mized incrementally, as the online learning progresses. The main idea behind our derivation is the
connection between regret bounds and Fenchel duality. This connection leads to a reduction from
the process of online learning to the task of incrementally ascending the dual objective function.

In order to derive explicit quantitative regret bounds we make use of the weak duality prop?
erty. which tells us that the dual objective lower bounds the primal objective. The analysis of our
algorithmic framework uses the increase in the dual for assessing the progress of the algorithm.
This contrasts most if not all previous works that have analyzed online algorithms by measuring the
progress of the algorithm based on the correlation or distance between the online hypotheses and a
competing hypothesis.

We illustrate the power of our framework by deriving various learning algorithms. Our frame?
work yields the tightest known bounds for several known online learning algorithms. Despite the
generality ofour framework, the resulting analysis is more distilled than earlier analyses. The frame?
work also serves as a vehicle for deriving various new algorithms. First. we obtain new algorithms
for classic prediction problems by utilizing different techniques for ascending the dual objective.
We further propose ef?cient optimization procedures for performing the resulting updates of the
online hypotheses. Second, we derive novel algorithms for complex prediction problems, such as

ranking and structured output prediction.

Section 5

Object Detection from video

The motion of object can be detected after the object is detected from Video. Trackmg the 301?le
or object from sequence Video frames this is the main goal of Video tracking. Blob tracking.
keinel-based tracking. Contour tracking are some common target representation and localization
algorithms. Ruolm Zhang [11] has proposed adaptive background subtraction about the Video
detecting and tracking moving object. He use median ?lter to achieve the backgroiuid subtraction,
This algorithm is used for both detecting and tracking moving objects in sequence of video. This
algorithm never suppoit for multi feature based object detection. Hong Lu and Hong Slieng Li
[12] were introduced a new approach to detect and track the moving object. The defme motion
model and the non-parameter distribution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kahnan filter estnnati g
its a?'me motion in next frame. The author shows Experimental results and proof the new method
can successfully track the object iuider such case as merging. splitting. scale Variation and scene
noise, The author Bayan [13] talks about adaptive mean shirt for automated multi tracking. The
bene?t of Gaussian mixture model is that it extracted Foreground image from video Er ine
sequence it also eliminate the shadow and noise from video sequence It is helpful in initiali 'ng
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from Video and hence we can track the object easily. The object can trap from
Video by changes in size and shape

In this paper section] gives introduction about HAR and gives motto of paper: over view of
traditional HMM classifier in section 2. Section 3 gives overview about HMM-based approach
that uses threshold aird voting and section 4 gives over view about HZMM-NN and NN-HMM.

Section 5 contaim the review of how object can be detected from other ways. The result of all
methods as conclusion in section 6 Sections ' contain references

Section 2

TRADITIONAL HMM (TLAssmIR

A hidden Markov model (HIVIM) is a statistical Markov model in which the system being
modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be
considered as the simplest dynamic Bayesian network. The logic behind the HMM was
developed by L. E. Baum and coworkers. It is nearly dependent on an earlier work on optimal
nonlinear ?ltering problem (stochastic processes) proposed by Ruslan L Stratonovich. who was
the ?rst to describe the forward-backward procedure [14]

In a regular Markov model. the state is directly visible to the observer. and therefore the state
transition probabilities are the only parameters. In a hidden Markov model. the state is not
directly visible. but output. dependent on the state. is visible. Each state has a probability
distribution over the possible output tokens. Therefore the sequence of tokens generated by an
HMM gives some infoiination about the sequence of states. Note that the word ?hidden' is refers
for the state sequence through which the model is passes. not for the parameters of the model.

Hidden Markov models are especially known for then" application in temporal pattern recognition
such as speech. handwriting. gesture recognition. pai?t-of-speech tagging. musical score following
partial discharges and bioinformatics

A hidden Markov model can be considered a generalization of a mixture model where the hidden
variables (or latent variables). which control the mixttu?e component to be selected for each
obseivation. are related through a Markov process rather than independent of each other.

. Index of the frequency component with largest magnitude.

. Weighted average of the frequency components to obtain a mean frequency.
. skewness and Kurtosis of the frequency domain signal.

. Energy of 6 equally spaced frequency bands within the 64 bins of the FFT,

Regarding the feature extraction complexity, we can comment that every feature is
represented by 4 bytes, so every frame needs around 16 kB for storing a feature vector
with all the features (4086 features) from all signals (117 signals from 9 measurement
units). The features extractor has been implemented using Octave v4.0.1. This module
needs 12 min. for extracting the features in a whole session (around 18 min of physical
exercise) using an lntel Core [74790 CPU at 3.6 GHZ With 16 GB of RAM.

Machine learning algorithm

The machine learning algorithm module acts as a classi?er. For this module, we have
tried two popular algorithms (148 decision tree and Random Forest) and found that the
Random Forest algorithm [28] works better in this circumstance. In our preliminary
experiments, it defeats the 148 decision tree algorithm by nearly 10% in accuracy. There?
fore, the Random Forest algorithm is used in our following experiments.

The Random Forest algorithm creates several decision trees during training. In our
experiments, the number of trees ranges from 40 to 95, and the number of nodes per
tree goes from 15 to 87. These numbers varies strongly with the number of features con?
sidered in the feature extractor. The algorithm for training the Random Forest model is:

This introduction presents an overview of the online learning model and the contributions of this
dissertation. The main concepts introduced here are covered in depth and more rigorously in later

chapters.

1.1 Online Learning

Online learning takes place in a sequence of consecutive rounds. On each round. the learner is
given a question and is required to provide an answer to this question. For example. a learner might
receive an encoding of an email message and the question is whether the email is spam or not.
To answer the question, the learner uses a prediction mechanism, termed a hypothesis. which is a
mapping from the set of questions to the set of admissible answers. After predicting an answer.
the learner gets the correct answer to the question. The quality of the leamer?s answer is assessed
by a loss function that measures the discrepancy between the predicted answer and the correct one.
The learner's ultimate goal is to minimize the cumulative loss suffered along its run. To achieve
this goal. the learner may update the hypothesis after each round so as to be more accurate in later
rounds.

As mentioned earlier, the perfomtance of an online learning algorithm is measured by the cu~
mulative loss suffered by the learning along his run on a sequence of question-answer pairs. We
also use the term example to denote a question?answer pair. The learner tries to deduce information
from previous examples so as to improve its predictions on present and future questions. Clearly,
learning is hopeless if there is no correlation between past and present examples. Classic statistical
theory of sequential prediction therefore enforces strong assumptions on the statistical properties of

the input sequence (for example, it must form a stationary stochastic process).

1.2.2 Problem Type

The Perceptron algorithm was originally designed for answering yes/no questions. In real?world
applications we are often interested in more complex answers. For example, in multiclass categov
rization tasks, the learner needs to choose the correct answer out of k possible answers.

Simple adaptations of the Perceptron for multiclass categorization tasks date back to Kessler's
construction [44]. Crammer and Singer [31] proposed more sophisticated variants of the Perceptron
for multiclass categorization. The usage of online learning for more complex prediction problems
has been further addressed by several authors. Some notable examples are multidimensional regres?
sion [76], discriminative training of Hidden Markov Models [23]. and ranking problems [28, 29].

1.2.3 Aggressiveness Level

The update procedure used by the Perceptron is extremely simple and is rather conservative. First,
no update is made if the predicted answer is correct. Second. all instances are added (subtracted)
from the weight vector with a unit weight. Finally. only the most recent example is used for updating
the weight vector. Older examples are ignored.

Krauth [78] proposed aggressive variants of the Perceptron in which updates are also perfonned
if the Perceptron's answer is correct but the input instance lies too close to the decision boundary.
The idea of twing to push instances away from the decision boundary is central to the Support
Vector Machines literature [1 17. 33, 100]. In addition. various authors [68. 57. 80, 74. 103. 31, 28]
suggested using more sophisticated learning rates. i.e., adding instances to the weight vector with
different weights.

Finally. early works in game theory derive strategies for playing repeated games in which all
past examples are used for updating the hypothesis. The most notable is followfthevleader ap-
proaches [63].

Our framework emerges from a new View on regret bounds, which are the common thread in
the analysis of online learning algorithms. As mentioned in Section 1.1, a regret bound measures
the performance of an online algorithm relative to the performance of a competing hypothesis. The
competing hypothesis can be chosen in retrospect from a class of hypotheses. after observing the
entire sequence of examples.

We propose an alternative View of regret bounds that is based on the notion of duality in con?
vex optimization. Regret bounds are universal in the sense that they hold for any possible ?xed
hypothesis in a given hypothesis class We therefore cast the universal bound as a lower bound for
an optimization problem. Speci?cally, the cumulative loss of the online learner should be bounded
above by the minimum value of an optimization problem in which we jointly minimize the cu?
mulative loss and a ?complexity" measure of a competing hypothesis. Note that the optimization
problem can only be solved in hindsight after observing the entire sequence of examples. Neverthe?
less. this viewpoint implies that the cumulative loss of the online learner fomts a lower bound for a
minimization problem.

The notion of duality, commonly used in convex optimization theory. plays an important role
in obtaining lower bounds for the minimal value of a minimization problem (see for example [89]).
By generalizing the notion of Fenchel duality, we are able to derive a dual optimization problem,
which can be optimized incrementally as the online learning progresses. In order to derive explicit
quantitative regret bounds we make immediate use of the weak duality property, which tells us that
the dual objective lower bounds the primal objective. We therefore reduce the process of online
learning to the task of incrementally increasing the dual objective function. The amount by which
the dual increases serves as a new and natural notion of progress. By doing so we are able to
associate the cumulative loss of the competing hypothesis (as re?ected by the primal objective

value) and the cumulative loss of the online algorithm, using the increase in the dual.

In most of this dissertation we make no statistical assumptions regarding the origin of the se?

quence of examples. We allow the sequence to be determini

 

stochastic. or even adversarially
adaptive to our own behavior (as in the case of spam email ?ltering). Naturally. an adversary can
make the cumulative loss of our online learning algorithm arbitrarily large. For example, the adver?
sary can ask the same question on each online round. wait for the learner?s answer, and provide the
opposite answer as the correct answer. To overcome this de?ciency. we restate the learner's goal
based on the notion of regret. To help understand this notion, note that the learners prediction on
each round is based on a hypothesis. The hypothesis is chosen from a prede?ned class of hypothe?
ses. In this class, we de?ne the optimal ?xed hypothesis to be the hypothesis that minimizes the
cumulative loss over the entire sequence of examples. The learner's regret is the difference between
his cumulative loss and the cumulative loss of the optimal ?xed hypothesis. This is termed "regret?
since it measures how 'sorry' the learner is. in retrospect, not to have followed the predictions of the
optimal hypothesis. In the example above, where the adversary makes the learners cumulative loss
arbitrarily large, any competing ?xed hypothesis would also suffer a large cumulative loss. Thus,
the learner?s regret in this case would not be large.

This dissertation presents an algorithmic framework for online learning that guarantees low
regret. Speci?cally. we derive several bounds on the regret of the proposed online algorithms. The
regret bounds we derive depend on certain properties of the loss functions, the hypothesis class, and

the number of rounds we run the online algorithm.

1.2 Taxonomy of Online Learning Algorithms

Before delving into the description of our algorithmic framework for online learning. we would like

to highlight connections to and put our work in context of some of the more recent work on online

PAC learning framework. For completeness, in Chapter B given in the appendix. we discuss the
applicability of our algorithmic framework to the PAC learning model. We start this chapter with
a short introduction to the PAC learning model. Next, we discuss the relative dif?culty of online
learning and PAC learning. Finally. we propose general conversion schemes from online learning
to the PAC setting.

1.4.2 Part II: Algorithms

The second part is (levoted to more speci?c algorithms and implementation details. We start in
Chapter 5 by deriving speci?c algorithms from our general algorithmic framework. In particular?,
we demonstrate that by varying the three components of the general framework we can design
algorithms with different update types, different aggressiveness levels. and for different problem
types.

Next. in Chapter 6 we show the applicability of our analysis for deriving boosting algorithms.
While boosting algorithms do not fall under the online learning model. our general analysis ?ts
naturally to general primal?dual incremental methods. As we discuss, the process of boosting can
be viewed as a primal~dual game between a weak learner and a booster.

Finally. in Chapter 7 we discuss the computational aspects of the different update schemes.
Depending on the loss function and update scheme at hand, we derive procedures for performing

the update with increasing computational complexity.

1.4.3 Part III: Applications

In the last part of the dissertation we demonstrate the applicability of our algorithms to real world

problems. We start with the problem of online email categorization. which is a natural online

Appendix B

Using Online Convex Programming for
PAC learning

In this chapter we outline the applicability of our algorithmic framework from the previous chapter
to Probably Approximately Correu (PAC) learning [116]. The PAC setting is also referred to as
hart/1 learning. We start this chapter with a short introduction to PAC learning. Next, in Section B2
we relate two important notions of learnahiliry: the notion of mistake bound in online learning
and the notion of VC dimension used in PAC learning. Based on this comparison. we demonstrate
that online learning is more dif?cult than PAC learning. Despite this disparity. as we have shown
in previous chapters. many classes of hypotheses can be ef?ciently learned in the online mode].
In Section B} we derive several simple conversions from the online setting to the batch setting.
The end result is that the existence of a good online learner implies the existence of good hatch
learning algorithms. These online?to?batch conversions are general and do not necessarily rely on
our speci?c algorithms for online learning.

B.1 Brief Overview of PAC Learning

hold as long as we have a suf?cient increment in the dual objective. By monitoring the increase in
the dual we are able to control the aggressiveness level of the resulting online learning algorithm.
To make this dissertation coherent and due to the lack of space, some of my research work was
omitted from this thesis. For example, I have also worked on boosting and online algorithms for
regression problems with smooth loss functions [SE]. 40]. online learning of pseudovmetrics [109],
online learning ofprediction suf?x trees [39], online learning with various notions of margin [103],
online learning with simultaneous projections [4], online learning with kernels on a budget [41],

and stochastic optimization using online learning techniques [1 10].

1.4 Outline

The dissertation is divided into three main parts. titled Theory. Algorithms, and Applications. In
each part, there are several chapters. The last section of each of the chapters includes a detailed

review of previous work relevant to the speci?c contents described in the chapter.

1.4.1 Part 1: Theory

In the theory part. we derive and analyze our algorithmic framework in its most general form We
start in Chapter 2 with a formal description of online learning and regret analysis. We then describe
a more abstract framework called online convex programming and cast online learning as a special
case of online convex programming. As its name indicates, online convex programming relies on
convexity assumptions. We describe a common construction used when the natural loss function for

an online learning task is not convex.

Abstract

Smartphones have become a global cormnunication tool and more recently a teclnrology for studying hrrrnan
behavior. Given their numerous built-in sensors. smartphones are able to capture detailed and continuous
observations on activities of daily living. However. translation of measurements from these consumer-grade
devices into research-grade physical activity patterns remains challenging. Over the years. researchers have
proposed various human activity recognition (HAR) systems which vary in algorithmic details and statistical
principles. In this paper, we summarize existing approaches to srnartphone-based HAR. We systematically
screened the literatru'e on Scopus. PubMed. and Web of Science in the areas of data acquisition. data
preprocessing, feature extraction. and activity classi?cation. We ultimately identi?ed 72 articles on
srnartphone-based HAR. To provide an understanding of the literature. we discuss each of these areas
separately, identify the most cormnon practices and their alternatives. and propose possible future research

directions for this interesting and important ?eld.

Keywords

Wearable computing; accelerometer; gyroscope: data acquisition: data processing: feature extraction:

activity classi?cation: digital plrenotyping machine learning: pattern recognition.

1.6 Bibliographic Notes

How to predict rationally is a key issue in various research areas such as game theory. machine
learning. and information theory. In this section we give a high level overview of related work in
different research ?elds. The last section of each of the chapters below includes a detailed review
of previous work relevant to the speci?c contents of each chapter.

ln game theory. the problem of sequential prediction has been addressed in the context of playing
repeated games with mixed strategies. A player who can achieve low regret (i.e. whose regret grows
sublinearly with the number of rounds) is called a Harman consistent player [63]. Hannan consistent
strategies have been obtained by Harman [63]. Blackwell [9] (in his proof of the approachability
theorem). Foster and Vohra [49, 50], Freund and Schapire [55]. and Hart and Mas?collel [64]. Von
Neumann?s classical minimax theorem has been recovered as a simple application of regret bounds
[55]. The importance of low regret strategies was further ampli?ed by showing that if all players
follow certain low regret strategies then the game converges to a correlated equilibrium (see for
example [65, 10]). Playing repeated games with mixed strategies is closely related to the expert
setting widely studied in the machine learning literature [41 82, 85, 119].

Prediction problems have also intrigued information theorists since the early days of the in?
formation theory ?eld. For example. Shannon estimated the entropy of the English language by
letting humans predict the next symbol in English texts [I l 1]. Motivated by applications of data
compression, Ziv and Lempel [124] proposed an online universal coding system for arbitrary in-
dividual sequences. In the compression setting. the learner is not committed to a single prediction
but rather assigns a probability over the set of possible outcomes. The success of the coding system
is measured by the total likelihood of the entire sequence of symbols. Feder, Merhav. and Gutman
[47] applied universal coding systems to prediction problems. where the goal is to minimize the

number of prediction errors. Their basic idea is to use an estimation of the conditional probabilities

ABSTRACT."

The rapid nnprovement m teahnologv Causes more attention towards to Remgnrzmg ofhuman aenvmes
?'om video. These new teehnolagreaz growth has made vision?based research much more mterestrng anal
e/?ctent than ever before. ihts paper present navel HMM (Hmaen Markov Model) based approach for
Human amwty r-eaagnraon from video. zhere are different approaches ofHMM to rerogn'ue ac?mi of
human from video. ere threshold and voting to automanmlly ana e?eetrvezy segment and reeogntze
sampler aanwtres, segment and recognwe eampten aatwmes and ?u srmple aammes we use Elmnn
Network (EN) and two hyanas ofNeuraINetwork (NN) andHll/lM, re. HMM?NN and NNI?VIM.

KEY Worms:

Human Activity reaagnmon, Hidden Markov Model, Hybrid model afHMM, Image mpmrmg from Video,
complex activity,

 

TRODUCTI

 

Automatically recognizing human activities from video is important for applications such as
automated suiyeillance systems and smart home applications. Several human activity recogn' 'on
methods [l][2][3][4][5][6] were proposed in the past few years to classify single human actitities
such as walking skipping. sitting down. etc. Human activity recognition (HAR) research has
been on the use because of the rapid technological development of the image-capturing software
and hardware. in addition to the omnipresence of reasonably low-cost high-performance personal
computers. The main goal of tlns recognition is used to develop the different application which
make human machine interaction is easy and interesting.

     

In the jouiney of developing algorithms for human activity recognition. some new developed
algorithms adds some new features in pre?ously developed algorithm. In this paper. we present a
novel HMM-based approach that uses threshold and wring to automatically and effectively
segment and recognize complex activities. And also survey on two hybrids of Neiu?al Network
(NN) and HMM. e. HMNI?NN and 3 ??]-]]\IM. This paper also compares their perfoimance
with that of the traditional HMM.

   

Experiments

This section describes the experiments conducted in this work. In ?rst and second sub?
section, the main dataset and the evaluation methods used in this work are introduced.
Third and fourth subsections shown the experiments carried out for the analysis of the
type of sensor and the type of feature. In ?fth subsection, the ?nal results on the main
dataset are given. At the end, sixth subsection includes an additional experiment on
another I?IAR datasetfoPPORTUNlTY dataset, using the same system.

REALDISP dataset
In this work, the HAR system has been mainly trained and tested using the REALDISP
Activity Recognition dataset, available at the UCI Machine Learning Repository [24].
This dataset includes recordings from 17 subjects, seven females and ten males, with
ages ranging from 22 to 37 years old. These recording include 13 inertial signals obtained
from 9 on?body inertial measurement units located on different body parts. Each unit
contains four sensors: an accelerometer, a gyroscope, a magnetometer and a quaternion
sensor. Using these sensors, a 3D (3?dimension) linear acceleration, a 3D angular veloc?
ity, a 3D magnetic ?eld orientation and a 4D quaternions are sampled every 20 ms (50 Hz
sample?rate). The experiment consisted in performing a complete set of exercises: 33
physical activities, including warm up, ?tness and cool down activities (walking, jogging,
cycling, jumping, etc). One run?through of the exercises lasted 15?20 min. Each session
was preceded by a preparation phase lasting around 30 min. This dataset also includes a
Null?activity, This label has been assigned to other activities (different from the 33 activi?
ties considered in this study), and also, the transitions between activities.

The most signi?cant characteristic of this dataset is the introduction of sensor dis?
placement, such as rotation and mis?positioning from the corresponding body part. The
dataset includes recordings in three placement scenarios:

approaches. The available studies both use existing methods and propose new methods for collection.
processing. and classi?cation of activities of daily limig. Authors commonly discuss data ?ltering and
feature selection techniques and compare the accuracy of various machine learning classi?ers either on
previously existing datasets or on datasets they have collected de ?um for the purposes of the specific study.

The results are typically siunmarized using classi?cation accuracy within different groups of activities
like ambulation. locomotion. and exercise. This paper aims to summarize recent efforts in smartphone-based
HAR research With the goal of providing an understanduig of the contextual complexity and

innltrdunensionality of the problem. the collected data. and the methods used to translate the digital

 

measurements into human activ

2. Methods

Oiu' systematic review was conducted by searching for articles published by June 30. 2019. on PubMed,
Scopus. and Web of Science databases. The databases were screened for titles. abstracts. and keywords
containing phrases ?acti\ity" AND ("recognitiorf OR ?estimation" OR "classi?cation") AND
(?srnai?tplione" OR ?cell phone" OR ?mobile phone?). The search was limited to full-length journal articles
written in English. A?er removing duplicates. we read titles and abstracts of the remaining publications.
References that did not uivestigate HAR approaches were excluded ?'oni further screening. In the following

step. we filtered out studies that employed auxiliary equipment. like wearable or ambient devices. and

algorithms. Therefore. only 292 references were assigned for full reading. of which 121 references that
employed additional hardware were excluded together with an additional 99 references that utilized built-in
microphones or' video cameras. Additionally, we excluded studies with sruai?rphones af?xed to the human
body. The remaining "2 references were read in detail.

Most HAR approaches consist of four stages. namely data acquisition. dam prepracasiing. feature
attraction, and activity classification. hi the follomirg. We provide an overview of these steps and brie?y
ponit to signi?cant methodological differences among the studies. Table 1 summarizes speci?c aspects of
each study: we have decomposed data acquisition process to sensor type. experimental envn'onruent.
mvestigated activities. and selected sinartphone location: We have also indicated which studies prepr'ocess
collected measurements using signal correction methods. sensor orientatioir-invar?iant transformations. and
noise filtering techniques. we have marked investigations due to types of sigma] features they extract. as
well as due to the feature selection approaches: fmally. we have indicated the adopted activity classi?cation
principles. utilized classi?ers. and practices for accuracy reporting. Figure 3 displays the most ?'equent

terms used in the included studies.

3.1. Data acquisition
We use the term data acquisition to refer to a process of collecting and storing raw sub-second level
srnartplioue measiu'ements for the purpose of HAR. The data are typically collected by a program or

application that runs on the device and samples data from built-m srnanphone sensors within predefined

1. Introduction

According to the GSM Association. there were roughly 2 billon srrrartplrones in use in 2019. and this number
is expected to dorrble in the next couple of years [1]. Such explosion iir worldwide snraitphone adoption
presents unprecedented opportunities for the study of hrunarr behavior. Smartphones now coirtairr rrrultiple
sensors to captru'e detailed. contnruous. and objective measurements of lrrurran behavior. including on
mobility and physical activity. Along with sufficient storage. powerful processors. and Wireless
transrrrission. such data cart be obtained Without additional hardware or instrumentation. which makes it
feasible to study large cohorts of subjects over extended tnne periods. Irrrportantly. smartphones are not a
niche product. as appears to be the case with most wearable activity trackers [2]. but instead they have
becorue a globally available technology. increasingly adopted by users of all ages both in advanced aird
ernergmg economies [3.4]. While these technological developments make the task of data collection easier.
analysis of the collected data is increasingly identified as the rrrarn bottleneck in research settings [5?"]. and
therefore it appears that the mam challenge iir human activity recognition (HAR) is now shi?ing from data
collection to statistical methodology and pattern recognition.

This proliferation of smartphones has riot goire unnoticed by the research comnrunity. At the time of
writing. there were nearly 300 articles published on HAR methods using srnartphones. a substantial increase
from just a handful of articles published a few years earlier (Figure 1). This growmg interest has takeir place

across various fields such as security aird surveillance [8]. personal navigation [9]. and health monitoring

individuals Less effort has been devoted to investigate populations with different demographic and disease
characteristics. such as elders [12] and subjects with Parkinson's disease [10]. As an example of a larger
study. Kelishomi et al. [13] analyzed data from 480 healthy individuals.

In the reviewed papers. data collection typically takes place in a research facility and or nearby outdoor
surroundings. In such environments. study participants are asked to perform a series of activities along
prede?ned routes and to interact Wllll prede?ned objects. The duration and order of performed activities are
usually determined by the study protocol and the subject is supervised by a research team member. A less
popular approach involves observation conducted in free-living emirouiuents. where participants perform
activities Without speci?c constraints. Such studies are likely to provide ruore insight into diverse activity
patterns due to individual habits and unpredictable real-life conditions. Compared to a single laboratory
visit. it also allows urvestigatois to monitor behavioral patterns over many weeks [14] or months [15].

Activity selection is one of the key aspects of HAR The studies considered here tend to focus on a
small set of activities. including sitting. standing. walking. running. and stair clunbing. The less common
activities involve various types of mobility. locomotion. fitness. and household routines. For instance. Wu
et al. [16] differentiate between slow. noirnal. and brisk walking: Guvensan et al. [1'] investigate multiple
transportation modes. like car. bus. train. train. metro. and ferry: Pei et al. [18] recognize sharp body-turns:
and Della Mea et al. [19] look into household activities. like sweeping a ?oor or walking with a shopping
bag. In Table 1. ?post\u?e" refers to lying. sitting. standing. or arty pair of these activities: ?mobility" refers

to Walking. stair climbnrg. body trims. riding elevator. or escalator. running. cycling. or airy pair of these

 

results. In the reviewed literatiu'e. subjects were o?en instructed to caiiy the device in pants pocket (either
front or back). although a number of studies also considered other placements. such as Jacket pocket [30].
bag or backpack [31]. and holding the sinaitphone in hand [32].

To establish the ground mith for physical activity in HAR studies. the data are usually annotated
manually by trained research personnel or by subjects themselves [33.34]. However. We also encoiuitered
several approaches that automate this process both in controlled and free-living conditions. For instance. in
[14] the data were labelled using a designated sniartphone application. A different approach was proposed
in [35]. where authors used a built-in step coiuiter to produce "Weak" labels. Also. the annotation can be
done usuig built-in microphone [36] and video camera [12].

Finally. the data acquisition process is carried out on purposely designed applications which captiu'e
and transmit data to the external server using cellular. Wi-Fi. Bluetooth. or wired connection. In online
activity classi?cation. the collected data do not leave the device but instead the entire HAR pipeline is

implemented on the smartphoue.

3.2. Data preprocessing
We use the term data prepi'ocessing to refer to a collection of procediu'es auned at repairmg. cleaning. and
transfoiining measiu'enients recorded for HAR. The need for such step is threefold: (1) nieasiu?ement

systems embedded in smartpliones are often less stable than research-grade data acquisition units. and the

which is possibly due to the temporally dense. high-resolution measurements they provide for distinguishing
among activity classes. The inertial sensors are o?en used synchronously providuig more insight into the
dynamic state of the device. Some stridies show that the use of a smgle sensor can yield similar activity
recognition results [20]. To alleviate the impact of sensor position. researchers collect data using built-m
barometer and GPS sensors to investigate changes in altitude and geographic location [21.22]. Certain
approaches bene?t from the broader set of capabilities of sinaitphones. and the researchers may additionally
exploit proximity and light sensors which allow the recognition of a nieasiu?ement?s context. e. g.. the

distance between siiiartphone and subject's body. and changes between in-pocket and out-of-pocket

 

locations based on chang' g illumination [23.24]. The selection of sensors is also affected by secondaiy
research goals. like simplicity of classi?cation and minimization of battery drain. In such approaches. data
collection is carried out on a single sensor (e.g. accelerometer [l4]). a small group of sensors (e.g..
accelerometer and GPS [25]). or with purposely modi?ed sampling frequency to reduce the amount of data
collected and processed [26].

The sampling frequency describes how many observations are collected by a particular sensor in one
second. The selection of sampling frequency is usually performed as a trade-off between measurement
accuracy and battery drain. In a typical data acquisition setting. the sampling frequency ranges between 20

to 30Hz for inertial sensors and l to lOHz for barometer and GPS. The most significant variations from this

description are required for inertial sensors if limited energy consumption is a pi'ioirty (e.g.. accelerometer

3.3. Feature extraction
We use the term feature extraction to refer to a process of selecting and computing meaningful summaries
of smartphone data for the goal of activity classification. A typical extraction scheme includes data
Visualization. data segmentation. featiu?e selection. and feature calculation. A careful featiu'e extraction step
allows urvestigators not only to understand the physical nature of activities and its manifestation in digital
measurements. but more importantly also helps uncover hidden structures and patterns in the data. The
identified differences are later quanti?ed through various statistical measures to distinguish among
activities. In an alternative approach, the entire process of feature extraction is automated rising deep
learning. which handles both segmentation and feature selection. On the other hand. and as with most
applications of deep leaniing. this often results in loss of irrteipr?etability and limited control over the process.
The conventional approach to feature extraction begins with data exploration. For this purpose.
researchers employ various graphical techniques. like scatter plots. lag plots. autocoirelations plots.
histograms. and power spectra [44]. The choice of tools is often dictated by the study objectives and
methods. For example. research on inertial sensors typically presents raw three-dimensional data from
accelerometers. gyroscopes. and magnetometers plotted for the corresponding activities of standing,
walking. stair climbing. etc. A similar approach is used for barometric pressiu'e data Acceleration data are
o?en inspected in the frequency-domain. particularly to observe periodic motions of walking. running. and

cycling [29]. and the impact of external environment. like natural vibration frequencies of a bus or a subway

 

frequency. Derawi and Bours [8] propose the use of linear mterpolation. while Gu et al. [3 7] utilize spline
interpolation. Such procedures are imposed on a range of affected sensors. typically including
accelerometer. gyroscope magnetometer. and barometer, Frnther time-domain preprocessing considers data
trimming. carried out to remove unwanted data components. For this purpose. the beginning and end of each
activity borrt are clipped as nonrepresentative for the given activity [30]. where a bout refers to a short period
of activity of a specified kind. During this stage the researchers also deal with dataset imbalance. The
imbalance occurs when observations of one activity signi?cantly dominate over others. Such situation
makes the classi?er susceptible to ovei?ttnig in favor of the larger class: however. the issue might be solved
by up- or downsamplmg of data [13.38]. Additionally. the rneasru'ements are processed for high-frequency
noise cancellation (m Table 1. see ?denoising?). The literature identifies several methods suitable for serving
this task. includnig the use of low-pass ?nite impulse response ?lters (with cut-off frequency typically equal
to 10Hz for inertial sensors and 0.1Hz for barometers) [39.40]. weighted monng average [8]. moving
median [29]. and singular value decomposition [41].

Another element of data preprocessmg considers device orientation (in Table 1. see ?tr"ansformation").
Sniartphone measurements are sensitive to device orientation. which may be due to personal choices.
clothing. body shape. and movement during dynamic activities [38]. One of the popular solutions is to
transform the three-dimensional signal into univarrate vector magnitude which is invariant to rotations and

more robust to translations. This procedure is often apphed to accelerometer. gyroscope. and magnetometer

select the optimal window size. which emphasizes the inipoitance of this parameter to the performance of
HAR systenis [46?48]. This calibration aims to closely match the window size with the duration of a single
mstance of the activity. Smiilar motivation leads researchers to seek more adaptable segmentation methods.
One idea is to segment data based on speci?c tune-domain events. like zero-cross points. peak points. or

valley points. which represent the start and end points of a particular? activity bout [8.38]. This allows for

 

segments to have different lengths corresponding to a single fundamental period of the acti ity in question
Such approach is typically used to recognize quasipeiiodic activities like walking. running. stair climbing.
and sitting and standing [41].

The literature offers a large variety of signal features used for HAR. Such features can be divided into
several categories based on the initial signal processing procedure. This enables one to distinguish between
activity templates (i.e.. raw signal). time-domain features. and frequency-doinaui features. The most popular
features in the reviewed papers are calculated from time-domain signals as descriptive statistics. such as
local mean. variance. minimum and maximum. interquartile range. energy. and higher order statistics. Other
time-domain features include mean absolute deviation. mean (or zero) crossing rate. regression coef?cients.
and autocorrelation. Some studies describe novel and customized time-domain features. like histogram of
gradients [49]. magnitude of standard deviations [50]. number of local maxima and minima. their airiphtiide
and temporal distance between them [26] The described time-domain features are typically calculated over

each axis of the three-dimensional measurement or oiieiitation-invariant vector magnitude. Studies that use

 

iiniltidirneusionahty of smartplione measurement and data analysis. and offer guidelines and direction to

anyone interested in this challenging but inipoitant topic.

Acknowledgements

We are grateful to C ipr'ian Craiuiceanu. J aroslavv Harezlak. Emily Huang. and Greyson Liu for their careful

reading of our manuscript and then" insightful feedback.

Funding sources

The authors were supported by NIH?NHLBI award U01HLl453 86.

Author biographies

Martin Straczkiewicz is a Postdoctoral Research Fellow in the Department of Biostatistics at Harvard
University. His research is focused on developing novel statistical methods for quanti?cation of human
movements using wearable devices. primarily accelerometers and smanphones.

J'P Onnela is Associate Professor of Biostatistics at Harvard University. He obtained his doctorate in
Finland and subsequently completed fellowships at the University of Oxford. Harvard Kennedy School. and
Harvard Medical School. His main interest is in developing quantitative methods in statistical network

science and digital phenotyping.

Activity templates function essentially as blueprints for different types of physical activity. In HAR
systems. these templates are compared to patterns of observed raw measurements using various distance
metrics [25], Given the heterogeneous nature of human activity. activity templates are often enhanced rising
techniques similar to dynamic time warping [38]. As an alternative to raw measurements. sortie studies use
signal symbolic approximations created by discretization functions that transform data segments into
symbols [5. .54],

In the reviewed articles. the number of extracted features typically varies from a few to a dozen.
However. some studies purposely calculate too many features (sometimes hundreds) and let the analytical

method identify those that are most relevant and informative to HAR, Support vector machines [52]. gam

 

ratio [55]. recursive feature elimination [ ]. correlation-based feature selection [33]. and principal
component analysis [56] are among popular feature selection dimension reduction methods. A comparison

of feature selection methods is prorided by Saeedi and El-Sheimy [57].

3.4. Activity classi?cation

We use the term activity classi?cation to refer to a process of associating extracted features with par1icular
activity classes based on the adopted classi?cation principle. The classification is typically performed by a
supervised learning algorithm that has been trained to recognize patterns between features and labeled

physical activities. The fitted model is then validated on separate observations. usually using data from the

conscientious feature selection [29.54]. Computation time is sometimes repoited for complex methods. such

 

as deep neural networks [66]. extreme learning machine [67]. or synnbolic representation [53 .68]. as Well
as in comparative analyses [30]. Nevertheless. a comprehensive coinpaiison of results is dif?cult or

impossible as discussed next.

4. Discussion

Over the past few years many studies have investigated HAR using smartphones. The reviewed literature
provides detailed descriptions of essential aspects of data collection. data processing. and activity
classification. The studies have been conducted with one or more objectives. e.g.. to limit technological
imperfections (e.g.. no GPS signal reception indoors). to minimize computational requirements (e.g.. online
systems). and to maximize classi?cation accuracy (all studies). Oiu' review summarizes most frequently
used methods and offers available alternatives. We do not however identify any one ultimate activity
recognition procedure. and we doubt that one even exists. This results in pan from the complexity of the
task. Different studies use different activities. signal processing techniques. and classifiers. and each likely
suffers from speci?c potential drawbacks.

Some of our concerns relate to the quality of the collected data. While datasets are usually collected in
laboratoiy settings. there is little evidence that algorithms trained usuig data ?'om these controlled settings

generalize to free-living conditions [69.'()]. In free-living settings. such aspects as duration. frequency. and

We observed that the majority of studies utilize srnaitphones positioned stationary at a single body position
(i.e.. a speci?c pants pocket). and sometimes even with ?xed orientation. Such scenarios are however rarely
observed in real-life settings. and these types of studies should therefore be considered more as proofs of
concept rather than HAR systems that generalize to free-living settings. Other data quality considerations
relate to the description of the experiment and study protocol. including demographic details of the enrolled
cohort. enviromnental context. and details of the performed activities. Such information should be reported
as fully and as accurately as possible.

Only a few papers consider classi?cation in a context that involves actinties outside the de?ned
research scope. i.e.. activities that the HAR system was not named on. The designed classi?ers were instead
tasked to associate every movement with one of the prespeci?ed set of activities. Real-life activities are
however not limited to any particular set of behaviors. i.e.. we do not only sit still. stand still. walk. and
climb stairs. These classi?ers. when applied to free-living conditions. will naturally miss the activities they
were not trained on but will also likely overestimate others. An improved recognition scheme could assume
that the observed activities are a sample from a broader spectrum of possible behaviors or assess the
uncertainty associated with the classi?cation of each event.

Despite meeting the technical and practical requirements for human activity monitoring. a lack of
standardized procedures makes an apples-to-apples comparison of the studies dif?cult. The research also

suffers from de?cits in publicly available datasets. soiu'ce code. and named classi?cation models. Although

Abstract As performance of dedicated facilities continually improved, massive pulsar
candidates are being received. which makes selecting valuable pulsar signals from candi?
dates challenging. In this paper, we designed a deep convolutional neural network (CNN)
with I 1 layers for classifying pulsar candidates. Compared to arti?cial designed features.
CNN chose sub~integrations plot artd sub~bands plot in each candidate as inputs without
carrying bias '. To address the imbalanced problem. data augmentation method based
on synthetic minority samples is proposed according to characteristics of pulsars. The
maximum pulses of pulsar candidates were ?rst translated to the same position. then new
samples were generaned by adding up multiple subplots of pulsars. The data augmentation
method is simple and effective for obtaining varied and representative samples which keep
pulsar characteristics. In the experiments on HTRU I dataset. it shows that this model can
achieve recall as 0.962 while precision as 0.963.

 

Key words: pulsars: general ? methods: statistical ? methods: data analysis

1 INTRODUCTION

Pulsar searching is an important frontier in radio astronomy. Scientists pay more attention to pulsars
because of broad impact across physi s. astronomy. astronautics (Cordes et al. 2004: Lorimer et al.
1998: Lyne et al. 2004: Hobbs et al. 2000: Sheikh et al. 2006), etc. Many dedicated surveys have been
used to search more pulsar signals. such as Parkes multi?beam survey (PMPS. Manchester et al. 2001).
High time resolution universe survey (HTRU. Keith et al. 2010). and so on. With the advent of the
large?scale search surveys. such as Five Hundred Meter Spherical Telescope (FAST, Nan et al. 2011).
the Square Kilometre Array (SKA, Smits et al. 2009), weaker pulsar signals can be received, while
coming with more and more noise or radio frequency interferences (RFIS). which makes it dif?~
cult to select valuable suspected pulsar signal from massive pulsar candidates. Researchers have ap?
plied many successful methods to select pulsar candidates. including manual selection (Stokes et al.
1986; Johnston etal. 1902). selection with graphical tools (Faulkner et al. 2004: Keith et al. 2009),
ranking & scoring approaches (Lee et al. 2013) and machine learning methods (Eatough et al. 2010;
Bates et al. 2012: Morello et al. 2014; Zhu et al. 2014: Lyon et al. 2016; Devine et al. 2016: Tan et al.
2018; Guo et al. 2017).

it All unlabeled instances in the data stream receive a prediction.

2A The prediction is given promptly after the instances" arrival time.

3 The prediction model is constantly improved (independent of the DS speed).
4 No external instance storage is needed

The framework can be applied whenever events have to be detected as soon
as possible, and no information is allowed to be missed to detect these events.
We believe that concepts for the embedding of machine learning into real?world
systems are required, taking into account the time it takes to make a prediction
as well as the time it takes to train or re?ne a model. In this paper, we discuss
one such framework and present evidence from experiments with varying loads.

This paper is organized as follows First, related work is presented. Then. the
problem setting is presented along with the proposed framework. Subsequently.
the evaluation of the framework is presented in Section 44 The paper closes with
a discussion.

2 Related Work

Data stream mining has developed considerably in the past decade and attracted
many researchers to adopt existing algorithms for the challenging task to process
and reason about instances received at a very high speed [5] One part addresses
the adaptation of batch algorithms to cope with the data stream setting [6] by.
eg incremental batch approaches [7] To provide a speci?c environment for ef-
?cient data stream processing data stream management systems (DSMS) have
been developed. Such systems are adaptations of database management systems

While the mass of data is steadily increasing and data streams constantly
gain in speed, online learning algorithms used to process the data are naturally
limited by their maximal instance processing speed. As the evolution of these
massive data streams is much faster than the improvement of CPU power after
Moore's law [4], the gap increases between available and processable data As a
consequence, not all provided instances in a stream can be used by the learning
algorithm and some have to be skipped This could be especially harmful if the
skipped instances would reveal important insights to the user. To avoid skip?
ping instances and to still enable (potential) insights, currently not processable
instances could, in principle, be externally stored for later processing. However,
as the data stream speed is higher than the processing speed. the algorithm is
constantly challenged by the amount of data, and the amount of stored instances
is constantly increasing Besides memory usage, the time span from the arrival
of the instances to their processing (response time) is steadily increasing as well.
This processing delay can result in outdated information, and important events
might be missed To address these issues this paper introduces PAFAS (Pre?
diction Assured Framework for Arbitrarily Fast Data Streams), a framework to
handle high-speed data streams that potentially go to or beyond the limits of the
processing speed of the online learning algorithm. The contributions of PAFAS
are:

1. All unlabeled instances in the data stream receive a prediction.

2. The prediction is given promptly after the instances" arrival time.

3 The prediction model is constantly improved (independent of the DS speed).
4. No external instance storage is needed

2 DATA SET

To train and test our model. we need labelled convictive datasets. At present. the public labelled datasets
are relatively few. The most common one is the HTRU 1 dataset '. produced by Morello et al. (20l4).

This dataset is a part of outputs of a new processing of HTRU intermediate galactic latitude data
(Morello et al. 2014). It contains 1196 pulsars from 521 distinct sources with varied spin periods. duty
cycles. and signal to noise ratios. Besides. it has 89.995 non-pulsar candidates. it has been used in sortie
recent works (Morello et al. 2014: Lyon et al. 2016: Guo et a]. 2017: Ford 2017). In this paper. we used
it to train and measure our model.

Figure I is an example of pulsar candidates (pulsar20023) in HTRU 1 dataset with its four most
important subplots. The ?rst one subplot is fold pro?le plot. It is obtained by summing signal in all
frequency and period. Pulse pro?le of typical pulsar would make up with one or several narrow peaks
above the noise ?oor. The lower left one is subvintegrations plot. it is obtained by summing data ofdifv
ferent frequency channels. It re?ects the intensity ofthe signal duri g the observation time. For the ideal
pulsar signal. signal would be observed throughout the observation period. so one or several vertical
stripes will form corresponding to the peak positions in pro?le curve. Sub~bands plot is at the upper
right. By summing data over all period. it re?ects the intensity of signal at different frequencies. Since
radio pulsars are broadband. there should be one or more vertical stripes in most frequencies. In DM~
SNR curve at lower right. signal to noise ratio (S/N) ratio as a function of DM is recorded. As the pulse

 

   

For example, A38 is a leader in industrial motion, power grids, electri?cation products and
industrial transport infrastructure. One of its biggest challenges is predicting plant emissions in
advance and taking proactive measures before violations occur.

ABB?s Predictive Emission Monitoring System (FEMS) powered by Al uses an empirical
model to predict emission concentrations based on processed data. The team successfully
implemented PEMS as part of a comprehensive Environmental Management System in one
of the largest gas processing plants ln the world. However, PEMS (inferential analyzer),
cannot measure emissions directly, So, the system uses an empirical model to predict
emission concentrations based on data like fuel ?ow, load, operating pressure and
ambient air temperature. These kinds of intelligence analytics applications are driving

Al adoption?and forcing companies to consider whether cloud?based or edge?based
analytics are right for their particular business case.

 

The power of actionable data

Data collected from one IoT device has limited value on its own Plus,
according to Forrester Research, 60% to 73% of data in an enterprise
that could be used for analysis goes untapped. Real value is derived by
combining data sets from multiple devices to uncover patterns that can
be used to predict future performance

For example, a manufacturing plant may have 10 IoT devices or sensors
monitoring processes on various production machines. Rather than
looking at the data from only one device or each device independently,
analyzing data from across the site can provide a holistic view of what is
happening at that location.

Al is the enabling technology that processes large amounts of data and
recognizes patterns in the data Using powerful algorithms, AI adjusts to
new inputs and makes decisions based on what it has learned over time
to provide automated, accurate feedback to guide decision-makings It?s
the tool that adds value to all the data collected by loT devices. Al takes
advantage of the aggregation of big data to do more than just discover
what happened in the past Rather, AI produces analyses about ways
processes can be more ef?cient and predicts what could happen based
on multiple scenarios.

In related works. supervised machine learning methods have become more signi?cant the major
methods in classifying pulsar candidates.

The ?rst published work which attempted to use a machine learning approach to select candidates
is Eatough et al. (2010). They implemented arti?cial neural networks (ANN) with 12 designed experv
imental features as input vectors. Then Bates et al. (2012) and Morello et al. (2014) have made some
work to improve the performance by optimizing designed features with ANN. In these methods, de?
signed features relied on humans experience. may carry unexpected biases against panicular types of
pulsar candidates (Morello et al. 2014: Lyon et al. 2016). To address these problems. Lyon et al. (2016)
and Tan et al. (2018) selected fundamental and statistical features which aimed to minimize biases and
selection effects (Moiello et al. 2014) to provide better generalization performance.

In addition to these approaches with arti?cial designed features. data-driven methods also play
an important role in this ?eld. Zhu et al. (2014) developed Pulsar Image?based Classi?cation System
(PICS) system by using a group of supervised machine learning approaches. It makes the classi?cation
based on image patterns. The inputs are four important diagnostic plots of candidates rather than ex-
tracted features. It avoids possible defects of arti?cial design features and relying on excessive informa~
tion. It has been validated superior ability of recognition in PALFA survey pipeline and has discovered
six new pulsars. To address class imbalance problem in pulsar candidates. Guo et al. (2017) used Deep
Convolution Generative Adversarial Network (DCGAN, Radford et al. 2015) to generate more candiv
dates and automatically extract deep features at the same time. Then they used deep features to classify
data. which help to makes the classi?er more accurate.

In this paper. we take a step towards improving performance by the data?driven method. We de?
signed a deep CNN with eight convolutional layers, one ?atten layer and two fully connected layers.
The inputs are sub?integrations plot and sub~bands plot in each candidate rather than arti?cial designed
features. To make class balanced. we designed a simple and useful approach to synthesize more diverse
pulsar candidates. New samples were synthesized by adding up multiple subplots of pulsars after max-
imum pulses of pulsars were shifted to the same position. We tested our model on HTRU 1 dataset.
The results show that our model can provide satisfactory results on both recall and precision. The fol~
lowing pans are organized as follows: Section 2. the dataset used for training is introduced. Section 3
describes the data augmentation method for pulsar candidates. Section 4 introduces the network archi-
lecture and training details. Section 5 presents the experimental results ot?our model and analyses of its
performance. Finally. Section 6 is conclusions of our work.

HTK is a toolkit for building Hidden Markov Models (HMMs). HMMs can be used to model
any time series and the core of HTK is similarly general-purpose. However, HTK is primarily
designed for building HMM~based speech processing tools, in particular recognisers. Thus, much of
the infrastructure support in HTK is dedicated to this task. As shown in the picture above, there
are two major processing stages involved. Firstly, the HTK training tools are used to estimate
the parameters of a set of HMMs using training utterances and their associated transcriptions.
Secondly, unknown utterances are transcribed using the HTK recognition tools.

The main body of this hook is mostly concerned with the mechanics of these two processes.
However, before launching into detail it is necessary to understand some of the basic principles of
HMMs. It is also helpful to have an overview of the toolkit and to have some appreciation of how
training and recognition in HTK is organised.

This ?rst part of the book attempts to provide this information. In this chapter, the basic ideas
of HMle and their use in speech recognition are introduced. The following chapter then presents a
brief overview of HTK and, for users of older versions, it highlights the main differences in version
2.0 and later. Finally in this tutorial part of the book, chapter 3 describes how a HMM?based
speech recogniser can be built using HTK. It does this by describing the construction of a simple
small vocabulary continuous speech recogniser.

The second part of the book then revisits the topics skimmed over here and discusses each in
detail. This can be read in conjunction with the third and ?nal part of the book which provides
a reference manual for HTK. This includes a description of each tool, summaries of the various
parameters used to con?gure HTK and a list of the error messages that it generates when things
go wrong.

Finally, note that this book is concerned only with HTK as a tool-kit. It does not provide
information for using the HTK libraries as a programming environment.

Fig. 1.1 hiessage
Encoding/Decoding

Speech recognition systems generally assume that the speech signal is a realisation of some mes
sage encoded as a sequence of one or more symbols (see Fig. 1.1). To e?ect the reverse operation of
recognising the underlying symbol sequence given a spoken utterance, the continuous speech wave?
form is ?rst converted to a sequence of equally spaced discrete parameter vectors. This sequence of
parameter vectors is assumed to form an exact representation of the speech waveform on the basis
that for the duration covered by a single Vector (typically lOms or so), the speech waveform can
be regarded as being stationary. Although this is not strictly true, it is a reasonable approxima~
tion. Typical parametric representations in common use are smoothed spectra or linear prediction
coei?cients plus various other representations derived from these.

The role of the recogniser is to effect a mapping between sequences of speech vectors and the
wanted underlying symbol sequences. Two problems make this very dif?cult. Firstly, the mapping
from symbols to speech is not oneto?one since different underlying symbols can give rise to similar
speech sounds. Furthermore, there are large variations in the realised speech Waveform due to
speaker variability, mood, environment, etc. Secondly, the boundaries between symbols cannot
be identi?ed explicitly from the speech waveform. Hence, it is not possible to treat the speech
waveform] as a sequence of concatenated static patterns.

The second problem of not knowing the word boundary locations can be avoided by restricting
the task to isolated Word recognition. As shown in Fig. 1.2, this implies that the speech waveform
corresponds to a single underlying symbol (erg. word) chosen from a ?xed vocabulary. Despite the
fact that this simpler problem is somewhat arti?cial, it nevertheless has a Wide range of practical
applications. Furthermore, it serves as a good basis for introducing the basic ideas of HMM?based
recognition before dealing with the more complex continuous speech case. Hence, isolated word
recognition using HMMs will be dealt with ?rst.

(DBMS) to query continuous, unbounded data streams possibly in combina?
tion with pre?stored, ?xed datasetsi Two well-known DSMS. AURORA [8] and
STREAM [9], use their own language to query data streams. Both systems also
address the problem of too fast data streams. i.ei, when the system is not capa?
ble of processing all of the instances provided by the data stream. They use load
shedding (also implemented in a system environment [IOU to select instances of
the data stream that should be processed Based on Quality?Of?Service (QOS)
speci?cations. the system decides which instances are useful for the system to
fetch and which instances can be discarded. The main idea is to select instances
that will most probably lead to a good prediction. Another possibility to cope
With too fast data streams is sampling. Sampling is a technique to represent a
larger dataset by a smaller selected subseti It was frequently applied to reduce
the overall processing time of data mining algorithms and to efficiently scan large
datasets [ll]i In the simplest case it selects a random subset from the Whole data
set as an input for the learner Frequently, the purpose of this is to estimate the
quality of the result [12] Another possibility to cope with very fast data streams
is to adapt the mining technique corresponding to the currently available re?
sources Such methods are summarized under the heading of granularity-based
techniques. While load shedding and sampling change the input granularity of
the data mining method, the output of the data mining method can also be re?
duced, e.gi the number of rules or clusters [13]4 Then. the model that is used for
classi?cation is smaller and thus also more time?e?icient i.e.< more instances can
be processed in less time This method termed Algorithm Output Granularity
(AOG) can also be applied on various data mining schemes like clustering, clas?
si?cation or frequent set mining Lasti, anytime algorithms are also often used
for altering data stream speeds, as they can be interrupted anytime to return
an intermediate result [14] The more time available, the better the result has

. Idtmlsplacement all sensors were placed by experts at their optimal place for classi??
cation A]! the subjects recorded a sesslon in these conditions (17 sessions),

. Self?placement every subject decides the positions of three sensors by themselves and
the remaining sensors were situated by experts, The number of three is considered a
reasonable estimate of the proportion of sensors that may be misplaced during the
normal wearing, All the subjects recorded a session in these conditions (17 sesslons).

. Mutual?placement where several displacements were intentionally introduced by
experts, Three out ofthe 17 volunteers were recorded for mutualsdlsplacemeut sce
nario (subjects 2, 5 and 15) These three subjects recorded one session for every sen?
sor con?guration: for the case in which four, ?ve, six or even seven out of the nine
sensors are misplaced,

Considering the size, me number of activities and the different placement scenarios,
we think that the REALDISP dahaset is an appropriate dataset to evaluate the perfor?
mance of the proposed HAR system.

Eva|ualion methods

in this work, we apply two methods to evaluate the system performance, The ?rst one is
a tenfold random?partitioning crosssvalidation evaluation. This method consists of split?
ting the whole database (with all subjects data) randomly into 10 equal parts (subsets).
For every experiment, one subset is used for testing and the other nine for training, cons
sideriug a round?robin strategy. The final crosswalida?on result is the average along the
io experiments. This is the method used in the original paper [24],

However, our hypothesis is that this mediod suti'ers the problem mat ?le data in both
training and testing could contain information from a same subject, so the machine
learning algorithm can learn not only physical activity characmristics but also some
subjectdependent ones. This aspect makes it hard to evaluam the system performance
when facing a new subject. In order to verify this hypothesis, sect. ?Evaluation methods"
includes some experiments comparing both evaluation methods,

Therefore, we propose the second method, a subjectswise crosssvalidation. in this
case, the same kind of cross?validation is done but on different subjects rather on auto?
matically split parts; all data from the same user is considered for testing and the data
from the remaining subjects for training. Since we have 17 subjects in the database, this

 

 

The basic principles of HMMehased recognition were outlined in the previous chapter and a
number of the key HTK tools have already been mentioned. This chapter describes the software
architecture of a HTK tool. It then gives a brief outline of all the HTK tools and the way that
they are used together to construct and test HMM?based recognisers. For the bene?t of existing
HTK users, the major changes in recent versions of HTK are listed. The following chapter will then
illustrate the use of the HTK toolkit by working through a practical example of building a simple
continuous speech recognition system.

2.1 HTK Software Architecture

Much of the functionality of HTK is built into the library modules. These modules ensure that
every tool interfaces to the outside world in exactly the same way. They also provide a central
resource of commonly used functions. Fig. 2.1 illustrates the software structure of a typical HTK
tool and shows its input / output interfaces.

User input /output and interaction with the operating system is controlled by the library module
HSHELL and all memory management is controlled by HMEM. Math support is provided by HMATH
and the signal processing operations needed for speech analysis are in HSIGP. Each of the ?le types
required by HTK has a dedicated interface module. HLABEL provides the interface for label ?les,
HLM for language model ?les, HNET for networks and lattices, HDICT for dictionaries, HVQ for
VQ codebooks and HMODEL for HMM de?nitions.

This concept of a path is extremely important and it is generalised below to deal with the
continuous speech case.

This completes the discussion of isolated word recognition using HMMs. There is no HTK tool
which implements the above Viterbi algorithm directlyr Instead, a tool called HVITE is provided
which along with its supporting libraries, HNET and HREC, is designed to handle continuous
speech. Since this recogniser is syntax directedT it can also perform isolated word recognition as a
special case. This is discussed in more detail below.

1.6 Continuous Speech Recognition

Returning now to the conceptual model of speech production and recognition exempli?ed by Fig. 1.1,
it should be clear that the extension to continuous speech simply involves connecting HMMs together
in sequence. Each model in the sequence corresponds directly to the assumed underlying symbol.
These could be either whole words for so?called connected speech recognition or sub-words such as
phonemes for continuous speech recognition. The reason for including the non-emitting entry and
exit states should now be evident, these states provide the glue needed to join models together.

There are, however, some practical dif?culties to overcome. The training data for continuous
speech must consist of continuous utterances and, in general, the boundaries dividing the segments
of speech corresponding to each underlying subword model in the sequence will not be known. In
practice, it is usually feasible to mark the boundaries of a small amount of data by hand. All of
the segments corresponding to a given model can then be extracted and the isolated word style
of training described above can be used. However, the amount of data obtainable in this way is
usually very limited and the resultant models will be poor estimates. Furthermore, even if there
was a large amount of dataT the boundaries imposed by handAmarking may not be optimal as far
as the HMMs are concerned. HenceT in HTK the use of HlNIT and HREST for initialising sub?word
models is regarded as a bootstrap operation??. The main training phase involves the use of a tool
called HEREST which does embedded training.

Embedded training uses the same Baum-Welch procedure as for the isolated case but rather
than training each model individually all models are trained in parallel. It works in the following
steps:

 

Figure 1.1: Diagram of an HMM showing the hidden Markov chain Xp, and the
conditional independence of the observation variables Yk given the states Xk. The arrows
indicate conditional dependence (e.g., Yo is dependent on X0 but Y1 is conditionally
independent of yo given X0 and X1).

3. Given the observations YD,Y1. . . . , yr, estimate the (unknown) parameters of the

HMM A that generated them.

Each of these three basic problems has a well known solution based on the HMM forward
backward procedure (see [14] and references therein for details). For example. the third
problem (i.e. the HMM parameter estimation problem) can be solved using the popular
Baum?VVelch algorithm (which uses the forward?backward procedure on the hatch of
observations YD,Y17. . . ya) [14].

over recent decades. a modi?ed HMM parameter estimation problem has been posed
by introducing the additional requirement that the observations Yn, y1, . . . in should be
processed sequentially (i.e. online) rather than stored and processed as a batch. This

online (or recursive) formulation of HMM parameter estimation has become a topic of

A robot is a machine?especially one programmable by a computer? capable of carrying out a complex series of actions automaticallylzl Robots can be
guided by an external control device or the control may be embedded within. Robots may be constructed on the lines of human form, but most robots are
machines designed to perform a task with no regard to their aesthetics.

Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY?s
TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed

swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating
movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the coming decade? with

home robotics and the autonomous car as some of the main drivers?)

The branch of technology that deals with the design, construction, operation, and application of robots,[5] as well as computer systems for their control, sensory
feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous
environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature
contributing to the ?eld of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.

From the time of ancient civilization there have been many accounts of user-con?gurable automated devices and even automata resembling animals and
humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications
such as automated machines, remotecontrol and wireless remote-control.

The term comes from a Czech word, robola, meaning "forced labor";[61 the word ?robot? was ?rst used to denote a ?ctional humanoid in a 1920 play R.U.R.
(Rossumovi Univerza?lrli Roboti - Rossum's Universal Robots) by the Czech writer, Karel Capek but it was Karel's brother Josef Capek who was the words true
inventorlms?g] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey
Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen. The
?rst commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where
it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New
Jerseylml

Robots have replaced humans?? in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations,
or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their
role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions.?21 The use of robots in
military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in ?ction and may be a realistic
concern in the future.

(CNN) ? Ricky Gervais? "After Life" was a bittersweet little gem, but the ?rst season basically
told a reasonably complete story. As a consequence, the second six?episode run feels as if it's
essentially retracing old tenitory ?? moving in places. out With less urgency, and more prone to
silly detours to ?esh out the run.

GeNais' formula of two seasons and out (plus a follow?up special) worked well enough for the
original "The Of?ce" and "Extras." But his ?lmography has been more uneven of late, With "After
Life" very much in keeping with the Writer?producer?star?s outspoken atheism and darker, if not
irredeemabie view of life.

As a brief recap. the ?rst season found Gervais? Tony sleepwalk?ing through his days after his
beloved Wife died of cancer, consoling himself by watching old videos and home movies, With a
faithful dog (Eho?s a good girl? Vou are) as a companion.

By the end, Tony's outlook had brightened, showtng traces of generosity toward coworkers at
the local newspaper Where he grudgingly churned out human?interest stories, and ?nding a
potential new romance in the nurse, Emma (Ashley Jensen, splendid as always), looking after his
dementia?stricken dad (David Bradley).

The new season, however. finds Tony backsliding, again wallowing in grief to the point of
endangering his relationship With Emma, who understandably struggles with his behavior. This
continues, notably, despite the advice that Tony seems intent on ignoring from his cemetery pal
Anne (Penelope Wilton), who chides him not to mex things up.

in the most unoomfortable real?life echo, the aforementioned newspaper is struggling ?nancially
?? at a moment when that industry is painfully unraveling ?? posing an additional challenge to
Tony's boss and brother?inelaw. Matt (Torn Basden), whose marriage is falling apart even as he
presides over the paper's Woes.

It the accuracy difference between two experiments is bigger than the con?dence
interval, this di?erence can be considered signi?cant with a 9596 of probability. in this
paper. for all the experiments, the con?dence interval is lower than 0.596, so any di?err
ence higher than this value, this difference can be considered as signi?cant with a 9596 a
probability.

Data analysis

lu this section, we ?rst conduct some experiments for l-lAR system con?guration tuning
in order to analyze how the evaluation, the sensor type, or the feature type in?uences the
system performance.

Evaluation method

This section includes the experiments considering the two different evaluation meth?
ods. In these experiments, are setup is: ideal?placement, Null?activity removed (as in the
original paper), and time?based features. The experimental results are shown in Table 1.
From the results, we can clearly see that the result given by the random?partitioning
method is signi?cantly better man the subjectrwise method: the accuracy (Acc%) differ?
ence is 3.496 (99.1?95596) higher than ?re con?dence interval, 0.596.

This result supports the hypothesis suited in me previous secn'on: in the random?part
evaluation, training and testing subsets could contain information from a same subiect
and this characteristic produces better classi?cation results, ln the rest of the paper, we
will only consider the subjectwise evaluation method (more challenging situation).

Type elsensor
This section includes the experiments on different sensor types. ln these experiments,
the setup is: ideals placement, Nullractivity removed (as in die original paper), and time
based features. The experimental results are shown in Table 2. From the results, we can
clearly see that the 3D magnetometer works best among are four types of sensor and the
quaternion sensor performs the worst. The accuracy (Acc?xi) differences are statistically
relevant because they are bigger than the con?dence interval (0.596).

 

TECHNEWSWORLD DEVELOPERS

(omD

 

Re ews Se

 

rig iternet rr MD

   

ty Technalngv TechEIDQ

sharpens the Developer 5 Edge

With more developers having an accessible, free place to create software, there
will be more opportunities to collaborate on projects without being limited by
prices, noted Netdata's Tsaousis. With more collaboration, open source startups
and other projects likely will grow and scale at a faster rate.

"Netdata's ?rsthand experience has proved how powerful these tools can be. As
an open source project, founded by a sole developer, Netdata's roots are ?in
using GitHub's open source repositories," he said.

With the help of the community's contributors, along with dedicated company
engineers, the Netdata Agent has grown into one of the most watched and
starred projects on Gitllub and is downloaded more than half a million times a
day, Tsaousis pointed out. The new free access expansion will make it even
easier for an organization to standardize on Gitllub, which is at the heart of the
pricing decision.

"Becoming a de facto standard is a huge advantage with regards to competitive
solutions," he noted. ?However, once an organization is set on GitHub, it can be
dif?cult to revisit that decision if Pro and Team features ever become cost?
prohibitive,"

Bril ant Growth Move

Gillab is a growmg threat to GitHub, and a key reason for GitHub?s change in
strategy, suggested Thomas Hatch, CTO of SaItStack.

"This move is cntical for Gitllub to stay relevant as GitLab continues to steal
users and customers from them. In a nutshell, this allows Gitllub to give open
source users and developers the same thing that G'ltLab has been delivering for
many years,? he told LinuxInsider.

As more big names open?source their software and participate in the
community, it's important to remember why open source existed in the first
place, and the value of the foundation from which you can build, noted Aiven

of ?ow meter based disaggregation. it also can determine the volume of water used during each
classified usage event. waterSense requires a sample every 2 s for flow and one every seven seconds
for the motion sensor. The latter is only processed when motion is detected.

4. Classi?cation of Water Even?s

As previously discussed, the approaches for water use disaggregation make use of a variety
of different sensing modalities. The sensed signals are then subject to pattern analysis by applying
classification techniques in order to identify the corresponding water usage events. Depending on
the nature of the sensing modalities, different classifiers are utilised. These classifiers can be further
divided into discriminative and generative ones. in the following we structure our discussion on the
classification techniques according to the utilised sensing modality.

4.1. Water Flow Based Methods

A key assumption of non?intrusive monitoring approaches based on flow meters is that the
use of a particular water fixture or fixture type causes a distinct flow pattern in the residential pipe
infrastructure, which can be observed by a single sensor point. By applying pattern recognition
algorithms to the recorded time series data, water usage events for the specific water fixture or fixture
type can be identified.

Water flow in households is typically modelled with Poisson rectangular pulse as described in [42].
Figure 2 shows a 24-11 dataset generated from the aforementioned model. The number of events in a
unit of time follows a Poisson distribution, while duration and intensity have their own mean and
variance. All the parameters should be adjusted by time of the day with diurnal multipliers. The
observed flows are directly related to the volumes. Reported figures about water volumes and flows
are shown in Table 1 [34,43].

Non~intrusive monitoring techniques based on flow meters face the following difficulties;

. lrregularities of flow patterns for some fixture types. Some mechanically driven water valves,
such as in a washing machine or dishwasher, usrurlly exhibit more regular water usage patterns,
unlike faucets or showers where human users have the ability to vary the amount of flow and
duration significantly. Even the more regular flow patterns of washers can show variation
based on a diversity of different water saving programs and washing cycles that are available in
modem~day devices.

. Similarity of flow patterns among instances of the same fixture type. Many homes have multiple
toilets and/or faucets, which may be located in different rooms. This makes it challenging to
identify an individual ?xture if multiple instances of the same type exist [34,33].

evaluation methods used in this work and the experimental results Obtained with the
proposed system. Sixth section summarizes the main conclusions.

Background
Human Activity Recognition systems can be categorized by machine learning algorithm
and the type of sensor they used. Human activity recognition can be seen as a machine
learning problem. To deal with this problem, the HAR system must extract features from
sensor signals, generate a model for each activity, and classify next activities based on
these models. In the literature, different machine learning solutions have been applied
to the recognition of activities including Naive Bayes [3], Decision Trees [4], Support
Vector Machines (SVMs) [5], Deep Neural Networks [6] and Hidden Markov Models
(HMMs) [7]. In many works, several approaches have been compared using the WEKA
learning toolkit [8] because it incorporates many machine learning algorithms, For
example, Yang [9] compares the performance of several machine learning approaches:
C45 Decision Trees, Naive Bayes, krNearest Neighbor, and Support Vector Machines,
Kwapisz [10] compares three learning algorithms: logistic regression, 148, and multilayer
perceptron. Not only supervised but also, unsupervised algorithms have been studied
[11], In many works [12], complex algorithms, like the Random Forest, have demon?
strated a very good performance compared to simple classi?cation algorithms. Because
of this, the Random Forest has been the algorithm selected in this work.

For HAR, there are two main types of sensors: ambient and on?body sensors. In
terms of ambient sensors, the most widely used sensors are video cameras [13]. Video
recording is one of the main strategies for supervising human behavior and activities

[14]. But, this behavior can also be studied by analyzing acoustic events, The human

I Wasn't the type of kid who got attention. Teachers always wrote ?needs to participate more" on my report cards (with a smiley face to make my parents feel better]. I never got into trouble
and barely ever stood out on purpose. A fewyears earlier, I accidentally peed my pants because my zipper had gotten stuck in the bathroom at the last moment. I tried to convince everyone
that I had fallen into a puddle at recess. The custodian, Mr. Salazar, charged outside with a mop and brought me to find the puddle. My guess is that we wasted an hour looking around at the
gravel. My mom dropped off some new clothes and nobody really noticed my wardrobe change (...or that it hadn't rained inweeks].

That's how it was. Whether I did something spectacular or sneezed myself out of a chair, nobody cared, and almost nobody said my name. As far as school has concerned, all those things had
happened to ?some kid". So, why would a frogwith glasses jump up on a windowsill to stare at ?some kid??

Teachers, on the other hand, were different. Once her story ended, it hadn't taken Miss Weaver long to realize that Imsn?t paying attention. She called me up to the blackboard to make an
example out of me.

?Since you don't feel the need to listen, why don't you solve a problem on the board instead?? she said, sitting down at her desk.

My stomach did a ?ip. The problem would take a minute or two to solve, and being in front of the class almys made me nervous. How could I be expected to do anything when there was a
spectacled frog staring me down!

I stood to the right of the equation on the board so that I could check on the frog with quick glances. Despite the distraction, I did my best to focus. Halfway through, I saw that the frog had
moved towards the front of the classroom. It stopped at the window by Miss Weaver's desk. It took me a moment to figure out what it ms doing. I couldn't believe what [was seeing. It was
trying to lift the window.

Focusing on the problem became almost impossible. I made a mistake and then quickly erased it. The next time I looked over, the window was open. Why should that surprise me? Of course
a frog with glasses would also be super strong. The window was only open an inch, but that was enough for it to slip through. I dropped the chalk, and some of my classmates laughed.
Bending down to pick it up, I tried convincing myself that when I stood back up again the frog would be gone. ?It?s not there, I just think it's there.?

When I straightened up, the frog was sitting on Miss Weaver's left shoulder. This was a brave frog. Her head blocked the class from seeing it, and I realized that [was still the only one who
could. Either the frogwas real or my imagination had outdone itself. It wastit all that surprising that Miss Weaver didn't feel it there, because the shoulder pads inside her jacket were large
and ?uffy. I had heard that she rested her head on them like pillows during her breaks. So, now there ms a frog sitting on Miss Weaver's shoulder and nobody else knew it. And [was
supposed to be doing math.

Now that it was closer, I could see the frog better. It didn't look like some new species of frog to me. It looked like every other frog I had seen (except for the glasses]. I wondered if they made
contacts small enough for a frog. But, it wasn't the right time to worry about frogvision. That's a job for a frog eye doctor, anyway.

I had daydreams all the time when I was drawing, and sometimes I got lost in them. I tried one last time to explain the frog away, by saying it had to be part of an elaborate daydream. I
concentrated hard, finished the problem, and put the chalk down. The frog couldn't be real. I shook my head confidently.

On a high level, disaggregation allows domestic water use to be broken down into fixture
categories, which identify the amount of water consumption of individual fixture types in a household.
Typical fixture categories for indoor use are shower, bathtub, toilet, and faucet, as well appliances,
such as washing machines and dishwashers. Typical outdoor fixture categories are exterior hose bib,
swirrrming pools and irrigation systems. Most current end use studies provide insights on residential
water use at the level of the fixture category. Sometimes, however, it is necessary to distinguish
between hot and cold water use, and the location (room, indoor or outdoor) where the water is being
used. Such water usage break down requires knowledge of water use at individual fixtures or even
valves (in the case of hot and cold water).

A further level of contextualisation is the attribution of water use to individual residents in a
multi-party home or the mapping of water use to individual activities (e.g., washing hands, cleaning
teeth, watering the garden, eta). The latter contextualisations are particularly hard to obtain

One of the most common methods for deriving a breakdown of domestic water end use
information is through manual data collection acquired by consumer surveys, diaries/self?reports
and in situ observations in domestic living environments. Such studies are able to capture a diversity
of information, even detailed information that is sometimes very difficult to capture, such as water
usage activities. However they tend to be very labour intensive and do not scale well for longitudinal
analysis (over longer periods of time for larger populations). 0nline survey tools, such as the Water
Energy Calculator [28], have made it easier to reach wider audiences [6], however, such studies rely on
the truthfulness of the persons participating in the studies. Despite their best attempts at being honest,
users often reflect perceptual bias or may accidentally misreport relevant information 129]. Furthermore,
self-reports and surveys are not able to capture the exact amount ot water use and represent only
estimates that have to be complemented by more detailed metered water use. Researchers in the
field have therefore looked into instnrmenting households and applying data analytics solutions to
measured data traces in order to gain a better insight into water usage patterns.

A very accurate but inefficient way to obtain such insights is through extensive instrumentation
of a household. Each individual fixture or even valve can be instrumented with a flow meter. Such
deployments are mainly limited to testbed settings [30732], in order to establish a ground truth tor
other experiments with less intrusive techniques. It is not difficult to see that such an approach is, not
only cumbersome and costly in terms of deployment, but highly intrusive. Such a case can be seen as
analogous to intrusive load monitoring in the energy domain [33].

ln order to overcome the above limitations, a variety of non~imrusive monitoring approaches have
been proposed, which are able to perform water disaggregation based on data obtained from a single
sensing point or from a limited set of sensing points deployed at strategic locations of a residential
water pipe infrastructure and /or rooms ot a residences.

 

USA
TODAV

NFL draft moves along after wild night

First round of the 2020 NFL draft is in the books and will move ahead to the second
and third rounds on Friday night (7 p.mi ET on ABC, ESPN and NFL Network), Joe
Burrow went ?rst overall to the Bengals as expected, and in the next several
selections, teams followed the chalk: Chase Young to Washington, Jeff Okudah to
Detroit. Offensive tackle Andrew Thomas went to the Giants, followed by
quarterbacks Tua Tagovailoa and Justin Herbert to the Dolphins and Chargers,
respectively The "virtual" NFL draft also made for interesting TV on Thursday
night, from Cardinals coach Kliff Kingsbury showing off his lavish pad to whatever
it was that was going on at Titans coach Mike Vrabel's housei USA TODAY

Sports will have live updates and analysis of all of the news from the event,
including an up?tofthe?minute tracker breaking down every pick in real time.

. NT]. drz?'s most intriguing ?rst?round pidm Packers selection of QB Jordan Low raises eyebrows

- SEC breaks its own record for ?rst-round NFL draft picks with 15, led by LSU
and Alabama

Virtual vigil to be held in Canada for mass shooting victims

Victims of one of Canada's deadliest mass killings will be remembered Friday night
in a virtual vigil. Twenty?two people were killed last weekend when a gunman
dressed as a Royal Canadian Mounted Police of?cer went on a 12?hour killing spree
that began in the rural town of Portapique, Nova Scotia, and included at least 16
crime scenes across the provinceThe suspect, 517year?old Gabriel Wortman, was
shot dead by police The vigil will be livestreamed on the Facebook group
Colchester ? Supporting our Communities at 7 pm, local time (6 p.m. ET) and will
also air on the CBC News Network.

Type offeature
We also make a comparison on the performance of different kinds of features, more spe?
ci?cally, temporal features and frequency features as described in ?Feature extraction?f
Same as the experiments on sensor types, here, we also consider the idealvplacement,
removing the Null?activitv. For the sake of con?dence, we repeat the experiments with
different types of sensor.

From the results shown in fig. 2, it is obvious that the temporal features always beat
the frequency features and their combination in the cases of all mree sensor types.
Therefore, we consider only the tjme?based features in the rest experiments.

As a conclusion, it is clear that using me signals from magnetometer and the time
based features is currently the best system con?guration. By including all sensors, we
obtain even higher system accuracy: 97.096.

 

When tr ning and testing with different subjects, it is important to deal with the
intenuser variability. In order to reduce this variability, we propose several normali?
zation strategies. In this work, we evaluate six normalization methods, considering
two different places where this normalization is applied: before and after the feature
extraction.

1. Mean removal: Subtract the lnean value from each value in a feature or signal vector.

2. ZsScore: Mean removal ?rst, and divide each value by its standard deviation.

3. Histogram equalization. Consider all the values in a graysscale, and equalize its his?
togram,

4. 0?1 mapping: Distribute all data to the (L1 range.

Vector normalization: Divide each value in a vector With the vector's magnitude

6. Vector normalization with mean normalization. Vector normalization followed by

9"

mean removal

3.1. Assessing Individual Water Consumption

An effective way of measuring household level water consumption is through metering the
water supply at the premises of a customer. Only about half of UK households currently have a
water meter installed [21]. The vast majority of these meters are not Internetfonnected, and require
a manual readout by the water company or the customer. Meter readouts often take place on an
annual or monthly basis, in order to estimate the domestic water hill. Non~metemd customers are
charged an amount that is proportionate to the rateable value of the property [22]. This results in
an annual ?at rate that does not take into consideration the size of the household [23]. Automated
meter reading (AMR) or smart meter reading provides the ability to automatically capture water
usage information at more regular intervals. In their most basic form, such meters do not require a
connectivity infrastructure. They act as standalone meters that can be read through some wireless
channel in a walk~by (e. g., handheld devices) or driveby fashion (e.g., utility service vehicle). A more
effective way is connecting AMR/smart meter devices via a dedicated metering infrastructure to the
utility company, or via existing communication networks available at the household (e.g., phone line,
lntemet router). While this comes at increased costs and complexity it removes the burden of relying
on physical proximity for retrieving the meter readout, theoretically allowing near maHime reporting
of metering information in practice, meters are typically monitored on a daily, hourly basis or 15 min
basis [24], as dictated by the costs for data communication and data storage, respectively

In contrast to AMR devices, which provide only simple reporting functionality, smart meters can
provide bidirectional communications. Depending on their extended capabilities, smart meters can
provide some configuration options to the utility company, such as the configuration of the reading
interval or other system settings Some smart meters can be even interfaced to in-home displays or
smart home platforms, providing residents with information on their current or historic water use [25].

Despite their advanced metering capabilities, current smart meters are only capable of answering
how much water is being used and when. Breaking down the residential water use to more finegrained
levels, e.g., fixture level use, requires higher resolution readings combined with external data analytics
and possible additional instnrrnentation. Recent work in the field of energy metering calls for an
evolution of smart meters to become ?cognitive meters" [26] that are able to disaggregate the water use
within the metering device. While basic features, such as household leak detection on smart meters, is
already feasible [27], this vision still requires some further advances in the field.

interestingly, the scientific community has been working since the 19905 on approaches for water
use disaggregation at the household level. In the following we will examine these works and identify
strengths and weaknesses of these and current gaps.

 

TECHNEWSWORLD DEVELOPERS

Commit d I ernet rr Mn leTecli Rev ws Secur Teclniulngy reclining

 

Right Timing

"This ?ls a huge investment we are making, and it is good for Github's business
long term because more developers globally will be able to use the platform," a
spokesperson said in a statement provided to Llnuxlnsider by company rep
Nicole Numrich.

The growth factor is the key reason for the change of plans in providing
expanded free access, according to the spokesperson. It has 40 million
developers now, and global development is not slowmg down.

The GitHub Enterprise produti is reaching more companies than ever with 29
Global Fortune 50 companies building the software behind their businesses on
Gitliub Enterprise, the spokesperson noted. This shift from a "pay?for?privacy"
model to a "pay?for?features" model IS a fundamental change to the business
architedure of Gitllub.

Plummeting Price Plan

Gitl?lub reduced the entry?level paid tier price to $4 per user per month instead
of $9. The company still offers a more expensive tier (521) with SAML sign?on
and greatly expanded storage and actions.

Also still available is the specialized GitHub One service. Account managers
with high?value customers can negotiate special subscription fees.

Under the previous cost structure, Gitl-lub offered a free tier for private
development. However, it limited the number of collaborators with access to a
private repository to three.

Teams interested in using GitHub for private development had to subscribe to
one of its paid plans. GitHub previously offered unlimited repositories for free
only to public projects or those with a small number of users. That precluded
use of the free tier by several different types of teams, organizations and
companies.

4.1.1. Discriminative Classifiers

Flow trace analysis is one of the first automated techniques to infer water usage from single
flow meter readings. It was initially proposed by Dziegielewski ct al. [45], and is currently the most
widely?used technique for identifying water usage events in the water industry due to its maturity
and the availability of commercial service offerings based on it. Flow trace analysis relies on the fact
that domestic water use exhibits Common patterns that are distinctive enough to discriminate water
usage events of different fixture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decisionstree based classi?ers, the current water source for these water usage events can be determined.

A first extensive study that utilised flow trace analysis was presented by De0reo el al. [34]. The
authors performed a collection of signature flow traces for each fixture inside of 16 homes at a rate
of one sample every 10 s using a flow meter The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each. Using the signatures, data~?ow traces were
determined based on visual analysis. When a type of flow was identified, it was isolated in a window
and the integral of the flow rate over this window provided the volume of water used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a slgnal~processing algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, flow rate
change over time and time of the day cues. The authors however did not provide any assessment of
the performance of their solution

The two market leading commercial tools, TraceWizard [46] and ldentiflow 147], are also based on
the principle of flow trace analysis. According to a previous review by Nguyen ct al. [43] for these two
systems, both use decision tree based classifiers and require a timeconsuming and labour-intensive
process to perform offline fixture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak flow rate, the most common flow rate, and how often this most common flow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
fixtures are used at the same time or 0% when three or more were used. Similarly ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.8% in terms of the correctlyaclassified volume. As it relies on
fixed physical features ofvarious water?using devices, such as volume and flow rate for disaggregation,

the ?nal classification accurag is greatly degendent on the existing Mes of water devices

In this chapter. we are focused on quickly determining if a manoeuvre has occurred.
rather than estimating the manoeuvre time T and postemanoeuvre velocity VG. we
therefore aim to design manoeuvre detection algorithms that minimise the time delay
hetween when the manoeuvre occurs and when the null hypothesis H0 is rejected (whilst
avoiding false alarms). we shall present our proposed manoeuvre detection algorithms
as (stopping) rules for selecting the time to declare that a manoeuvre has occurred (by

rejecting H0).

lmportantly. our aircraft manoeuvre detection problem can he viewed a non?Bayesian
quickest change detection problem. In this chapter, we will exploit our minimax ro
hustness results of Chapters 4 and 5 by proposing two classes of vision?based aircraft
manoeuvre detection algorithms: a heading?based class of approaches inspired by our
i.i.d. process results of Chapter 4; and a transition?based class of approaches inspired
by our dependent process results of Chapter 5. Here. in addition to proposing and
investigating robustness?inspired manoeuvre detection algorithms, we will also consider
adaptive algorithm that attempt to detect the unknown manoeuvre by estimating any

unknown postemanoeuvre imageplane velocity information.

We shall ?rst brie?y revise an HMM representation of the aircraft imageplane
dynamics (5.1) that underpins both our transition?based and heading?based aircraft mae
noenvre detection approaches. We will also introduce an intuitive method of estimating
the aircraft?s imageeplane heading app that we shall use to propose our heading?based

manoeuvre detectors.

a least favourable parameter approach that requires the uncertainty set of parameters
to satisfy an informationrtheoretic Pythagorean inequality condition. This information?
theoretic Pythagorean inequality condition is closely related to the partial stochastic
boundedness concept we introduced in Chapter 4 to identify least favourable distributions

for our i.i.d. process asymptotic rninimacr quickest change detection problems.

In the case of i.i.d. observations before and after the changetime, the Lorden and
Pollok results of this chapter are special cases of the results we established previously
in Chapter 4. speci?cally, the results of this chapter only apply in the i.i.d. case
when the prechange distribution is known. and when the uncertainty in the post?change
distribution is described by unknown parameters (i.e., when the uncertainty is parametric
rather than nonparametric). The results in this chapter are signi?cant because they
hold for a large variety of stochastic processes including Markov chains and linear state
space systems (whilst the results of Chapter 4 and [e] are limited to processes with i.i.d.

observations before and after the changetime).

This chapter is structured as follows: In section 5.1. we propose Lorden. Pollak, and
Bayesian minimar robust quickest change detection problems with polynomial delay
penalties and parametric uncertainty. In section 5.2. we introduce our information?
theoretic Pythagorean condition on the uncertainty set, and identify asymptotically
minimax robust Lorden. Pollak and Bayesian quickest change detection rules. In section
5.3, we present Markov chain and linear statespace system examples and simulation

results. Finally, we provide conclusions in section 5.4.

Abstract: Water monitoring in households is important to ensure the sustainability of fresh
water reserves on our planet. It provides stakeholders with the statistics required to formulate
optimal strategies in residential water management. However, this should not be prohibitive and
appliance?level water monitoring cannot practically be achieved by deploying sensors on every
faucet or water-consuming device of interest due to the higher hardware costs and complexity, not to
mention the risk of accidental leakages that can derive from the extra plumbing needed. Machine
learning and data mining techniques are promising techniques to analyse monitored data to obtain
non?intrusive water usage disaggregation. This is because they can discern water usage from the
aggregated data acquired from a single point of observation. This paper provides an overview of
water usage disaggregation systems and related techniques adopted for water event classification.
The state?of?the art of algorithms and testbeds used for fixture recognition are reviewed and a
discussion on the prominent challenges and future research are also included.

Keywords: water usage disaggregation; water monitoring; disaggregation algorithms; machine
learning; water management

 

1. Introduction

The global use of water is increasing at a rate faster than can be satisfied with current
usable water supplies [1]. While irrigation and electricity generation dominates water usage in
developed countries [2], household water conservation still represents an important factor in ensuring
sustainability of fresh water reserves on our planet. In the US, for example, nearly 10% of fresh
water consumption can be attributed to domestic use [3]. In the UK, each person uses about 142 L
of water each day with the average household using 349 L of water per/day [4]. Even though

 

2. Understanding Water Usage

For a domestic setting, there are a variety of questions that can be asked, which can lead to insights
about the water use of a household at different levels of granularity:

- How much water is being used?

- What water is being usedfis it hot or cold water, fresh water or grey water?

- When is water being usedfhow does the water use change over the day, week and seasons?

- Why is water being usedfwhat are the activities related to water use?

- Where is water being usedfat which fixture or fixture type is water being abstracted from? Are
these indoor or outdoor uses?

- Who is using the waterfin a multi?occupancy household or building, how can water use be
attributable to individuals?

When unravelling insights about domestic water use, it is obvious that not all questions may
be relevant for all stakeholders and that different exploitations may require a specific water usage
contextualisation to be obtained.

From a water utility perspective, there are different motivations as to why obtaining insights
into household level information is important. At the most basic level, water usage information on
individual households is important for billing purposes. Aggregate monthly or even yearly water
usage information may be sufficient for water utilities to obtain adequate compensation for their
domestic water supply. Moreover, in order to achieve water distribution network efficiencies and
to understand investment levels, the relative costs and benefits of consumption are required; these
can only be estimated by determining households' water use [10]. This, in turn, impacts on water
pricing mechanisms. This information is useful to manage resource constraints and future demands.
From end-use analysis, utilities can gain insights into how new generation appliances will positively
affect demand, and can thereby avoid oversupply before it is needed. Having information of water

Remark 6.10 We con?rmed that the e?ect at k z 120 taas due to imperfect image
stabilisation by inspecting the time?synchronised nuuiyational data in Figure mm) 54
(c) to con?rm that the target aircraft had not begun to manoeuvre. We also repeated the
erperiment raith an improued imaae stabilisation technique (that is not currently suitable

for real?time implementation) and there was less increase in the test statistics at this

frame.

6.6.4 Results Summary, Limitations and Alternatives

overall, our simulation and real data results suggest that our TRC and HRC deteo
tors can offer similar performance to our adaptive HGLR.. TGC. and TGLR detectors
(although we acknowledge that our HGLR and HRC detectors both perform poorly
in our third and fourth ground?based sequences since the manoeuvres involve a large
change in imageplane speed up). on the basis of our simulation and real data results.
it therefore seems reasonable to select manoeuvre detectors on secondary considerations
such as computational and memory complexity (see section 6.5). In this respect. our
proposed HRC and TRC detectors are particularly attractive. conversely, our proposed
TGLR and TGC detectors are extremely computationally expensive (although our TGC
is preferable to our TGLR), but do perform well across a range of manoeuvre and local

detectability scenarios.

Finally. as in radar?based aircraft manoeuvre detection (e.g. [13.40]; we acknowledge

that it seems possible to propose alternative vision?based aircraft manoeuvre detection

wm M. rated it kiwi

sl?lrlvcs classics mad-again 2017

 

iThe second paragraph contains spoilers, I'd steer clear oi it if you haven't
read the novelTable 6.9 suggests that the performance differences amongst our transition?based ma?
noeuvre detectors are marginal (although our proposed TGC detector performs slightly
better than the TRC and TGLR detector in videos 1. 2, and 3). Signi?cantly. our HRC
detector outperforms our TGC detector in videos 1 and 2 (although its performance
is poor in videos 3 and 4). Similarly, although our proposed HGLR detector performs
reasonably in videos 1 and 2, its performance in videos 3 and 4 is very poor. we believe
that the poor performance of our HGLR and HRC detectors in videos 3 and 4 is due
to our constant speed modelling assumption (Assumption 6.1) being violated. Indeed,
in videos 3 and 4, the aircraft manoeuvres first manifest themselves as changes in the
aircraft?s imageplane speed vk (before the aircraft reverses its imageplane heading).
overall. despite our ground?based real data results highlighting this shortcoming of
our headingebased approach to manoeuvre detection, they again illustrate (as in our
simulation study) that TRC and HRC detectors can offer similar (or better) performance
than the adaptive HGLR. TGC, and TGLR detectors.

6.6.3 Airborne Scenario

we ?nally examine the performance of our proposed vision?based aircraft manoeuvre
detectors on an airborne video sequence involving a tail chase of a manoeuvring Cessna
182 captured by a forward mounted camera on a Cessna 172 (at 15 frames per second,
8 bit grayscale. 1024 x 768 pixels). we captured time?synchronised navigational data

from CPS/INS navigation sensors tan?board the manoeuvring aircraft to demonstrate

Note that in Table 3, SIGNJ means normalization on signal data using method 1 and
FEATJ means normalization on feature data (i.e. after feature extraction) using me?iod
l. The results show that the vector normalization method (number 5) applied directly
on the signal data outperforms all other methods, Thus, for the rest of the report, we use
this normalization method before the feature extraction.

Final results and discussion
By applying the best experimental configuration described above, we conducted experi
ments using data from all signals and all sensors in all the three placement scenarios.

ldealplacemem
Table 4 shows our ?nal experimental results for the ideal placement scenario. After
introducing the signal normalization method and considering all sensors, the system
accuracy goes to 99.4%, a 2.496 improvement compared to the original paper (9796). This
improvement is higher than the con?dence interval (0.536) so the difference is statisti?
cally signi?cant with a 9596 of probability. it is important to notice that, in the original
paper, the evaluation method was random?partitioning and, based on the results pre
sented in "Evaluation memod?, the baseline accuracy would be even lower when using
the subject?wise crosssvalidation method.

Another aspect to comment is regarding the Nu??aCtIVity. In the ?rst two rows of
Table 4, the experiments are conducted without considering the Null?activity, in other
words, it is a 33?class classi?cation task. We truncated the Nulleactivity samples in order
to make a fair comparison with the original paper. In this work, we have also done expets
iments including the Null?acn?vity, which, in our opinion, is closer to a real situation. So,
are problem now becomes more challenging: a 34rclass classi?cation task. Regarding the
results shown in the mird row of Table 4, our system still maintains a high performance
when the Null?actlvity is included: the system only loses 0.396 accuracy (from 99.4 to
99,196) showing a significant improvement (2.196) respect to the baseline system (9796).

I. BOY MEETS FROG

The first time that I saw the frog, [was sitting in class. Its face was pressed up to the window next to me from the outside. I had been drawing in my notebook, but once I noticed it, I couldtit
stop looking. It couldn't stop looking at me either (if it had been a staring contest, Iwould have lost). Maybe frogs blinked, but with its big eyes smushed against the glass, this one didn't.
Stagwood Forest was just beyond the school yard and it was riddled with frogs, but they always avoided people. Iknew right away, in a way that I can think better than I can say, that this frog
was different.

Miss Weaver hadn't noticed. She'd been my teacher for a few months, and ms known for having a stack of black hair that rose a foot above her head. Before the school year started, I had
heard rumors about her, and within a weekl realized that they were all true. For one thing, she wore the same out?t every day; the colors changed, but she almys had on striped pants and a
striped jacket. For another thing, she was mind?numbingly boring. The kind of boring that makes your eyes shut without your permission. Part of the problem ms that she liked to tell
pointless stories instead of teaching. She was obsessed with telling stories about former students who had become famous. The ?rst couple of times weren't had, even kind of interesting, but
by the second week of school she had already started repeating herself, just like with her outfits.

I knew all the stories by heart. The professional football player who was good at math, the politician who was a teacher's pet; I knew every word. Instead of listening, I spent most of class
drawing. I drew imaginary places, and designed creatures to fill them. Every drawing had a story. But not that day. I had barely gotten started when the frog appeared, and changed my life
forever.

I tried to listen back in to Miss Weaver, just in time to hear the end of her story about Martin Shandals, the now?famous comedian. Martin had transferred schools half my through the year,
so I always felt like that one shouldrit count. We were supposed to be learning long division, but something had reminded her of Martin. I knew exactly what bad joke she would end the story
with, and much less about long division.

?Whenever he acted up in class I'd say, we?ve got a real comedian on our hands don't we?" And 1 was right!" she said with a giggle.

I was sure Miss Weaver would see the frog eventually, but she didnt. Nobody did. When I looked again to see if it was still there, I noticed something shiny. It made me forget all about class,
and Miss Weaver and Martin Shandals. There was no denying It: the frog had put on a tiny pair of glasses.

I wanted to lecture it, to explain that frogs don't wear glasses. It bothered me that it didn't already know that. On top of that, it had been staring at me for at least five minutes. It seemed like
it ms bordering on rude. Could a frog even be rude? I wasn't sure. But, the bigger question was why it was so interested in me.

3.3. Sensing

The sensing process for non~intrusive water use disaggregation approaches is determined by a
variety of factors. A first key discriminator is whether a single or multiple sensing points are required
for a residential setting.

While singlepoint sensing solutions are typically based on a single modality, multi-point
approaches can utilise one or more different sensing modalities. Approaches that utilise a single

 

Sensors 2016, 16, 738 6 of 20

sensing modality are referred to as mono?modal sensing approaches, while approaches that utilise
multiple sensing modalities for water use disaggregation are referred to as multi-modal.

Depending on the nature of sensing modality the sampling frequency may greatly vary. Low
frequency approaches typically operate in sub-Hz regions, while high frequency approaches can
require up to several kHz sampling of the sensing signal. The sampling frequency determines the data
rate, which has an impact on processing storage, and conununication requirements.

The sensing process also typically determines whether associated water volumes of water usage
events can be determined.

Figure 1 shows an overview of the different sensing modalities utilised in current approaches that
can be found in literature.

I?I

THE STELLAR ONE

by Daniel Errico

When the universe was young the sky was ?lled with planets, and stars, and stardust, and many many rocks.

One of these rocks was a bit more special than the rest. She was unlike any that came before her.

She was a kind and happy rock, who always ?oated near a big blue planet.

Sometimes when the light hit her surface, she would glow a brilliant green. At times like those, she almost didn't look like a rock at all.

As the sky moved from day to day, and week to week, the rock would see planets far off in the distance.

She would wonder what it would be like to go to them. Week after week and month after month she would wonder. Until one day she decided to go ?nd out.

The rock had never gone anywhere before and wasn?t sure how to go about it.
She started to rock back and forth.

Then she started to spin.

Soon enough, she was ?ying through the sky.

As she left, clouds swirled on the big blue planet. For it was sad to see her go, and when planets cry there is a rain storm.

At first the rock was not good at moving. She would spin too far to the right or too far to the left. Slowly she learned how to travel whichever direction she liked, and she enjoyed exploring

 

TECHNEWSWORLD DEVELOPERS

corn;

 

e Tech Rev

 

Internet rr Mu Technnlngy Teclialnn

   

 

Open Source gn cance

The inclusion of Wordpress and digital marketing features in GitHub's offering
allows plenty of room to experiment, observed Pavan Raheja, business head at
Flint Technology|Consulting.

"GitHub being the best?smooth version control system takes a burden off me
and allows my team to try new features comfortably," he told LinuxInsider. "It
just gives complete freedom to my developers to try new features without
having to worry about anything. Especially, as WordPress is evolving to
decoupled systems and we are constantly trying to evolve, Github gives us the
complete freedom to try and test at will and with complete ease.?

Github makes it easy to collaborate with remote developers, Raheja pointed
out. It makes the Work?ow so simple that he can keep a complete track of
where the new developments are happening Without being worried about his
master copy.

"As 1 have complete control over the master and commit requests, 1 do not
have to be worried," Raheja remarked. "Ijust have to have a quick look a the
pull request In any of the branches my developers make the changes on. It
further gives me complete freedom to focus on other activities, like digital
marketing, without worrying about my code management.?

No Perfect Plan

Github's new access strategy may have an unintended bad consequence,

according to Prabhu Subramanian, creator of AppThreat and lead architect at
shiftLeft.

"This is significant but not for the better, unfortunately. Having all of the open
source artifacts ~ code, packages, vulnerabil

   

es, discussions, road maps ??
being aggregated under a single commercial umbrella is kind of the opposite of
what open source stands for. Free in open source is about freedom,"
Subramanian told LinuxInSider.

 

 

 

mwmm.

 

 

(a) (b)

Figure 1.3; Example of an aircraft climb manoeuvre observed from behind through a
video camera mounted on another aircraft; (a) a grayscale image of the manoeuvring
aircraft (the white image feature in the black square) with a manually annotated
historical track (blue dots) of the observed aircraft manoeuvre in the video; and (b)
navigation data from sensors on.board the manoeuvring aircraft showing the observed
climb manoeuvre (the observing aircraft is approximately 2 km to the southwest). More
details of this example are provided in the conference paper [C2] and Chapter 6.

1.4 Research Problems and Objectives

In this section, we shall use the open problems discussed in sections 1.1, 1.2, and 1.3
to formulate the research problems and research objectives of this thesis. Throughout
this thesis, we will focus on discretetirne processes (although there are analogous open

problems in continuoustirne stochastic processes).

Environmental data are an excellent source of information for occupancy detection since the
presence of living beings affects the surroundings through heat or Carbon Dioxide (C02) emission
without jeopardising the privacy of the occupants in that particular location. Nevertheless, only with
data, it is almost impossible to gauge something. Machine Learning (ML) techniques look at the
data and try to find patterns; with these patterns, it is possible to affirm the occupancy with a certain
percentage of certainty. Although some contributions have been performed in this direction, there is
still room for improvement, and this research proposal is focused on that.

The main purpose of this research is the design and development of an affordable and
non-intrusive solution to improve occupants experience in Smart Environments with ML support.
The proposed solution monitors temperature, light intensity, noise, and CO; to estimate the presence of
occupants through these environmental features that can be integrated with other existent approaches.
First, the data are collected and analysed, before applying ML techniques to infer the occupancy of the
area under monitoring. In the first stage, our solution detects the presence or absence of occupants.
In the second stage, the number of occupants inside the area of interest is estimated.

This paper is structured as follows Section 2 discusses the related work. Section 3 presents the
solution developed to detect occupants, its architecture, and the key features that were considered, the
gathering system and the ML concerns. Section 4 shows the experimental implementation. Section 5
analyses and discusses the results obtained Finally, conclusions are presented in Section 6 as well as
suggestions for future works.

2. Related Work

Occupancy detection systems could be classified according to the need to use a terminal or
not [6,7]. In the case of the methods that require a terminal, it is necessary to attach a device to the
occupants to keep track of them (eg., a smartphone). In the non-terminal methods, the detection is
based on a passive approach that is focused on monitoring areas or spaces instead of the identification
of devices (eg, cameras monitoring a room). Figure 1 depicts a simple classification of the occupancy
detection methods following the terminal and non-terminal approaches, and their more specific
characterisations, which are used to organise the discussion of this section.

2.3 The Toolkit

The HTK tools are best introduced by going through the processing steps involved in building a
sub?Word based continuous speech recogniser. As shown in Fig. 2.2, there are 4 main phases: data
preparation, training, testing and analysis.

2.3.1 Data Preparation Tools

In order to build a set of HMMs, a set of speech data ?les and their associated transcriptions are
required. Very often speech data Will be obtained from database archives, typically on CD-ROMS.
Before it can be used in training, it must be converted into the appropriate parametric form and
any associated transcriptions must be converted to have the correct format and use the required
phone or word labels. If the speech needs to be recorded, then the tool HSLAB can be used both
to record the speech and to manually annotate it with any required transcriptions.

Although all HTK tools can parameterise waveforms on-the-?y, in practice it is usually better to
parameterise the data just once. The tool HCOPY is used for this. As the name suggests. HCOPY
is used to copy one or more source ?les to an output ?le. Normally, HCOPY copies the whole ?le,
but a variety of mechanisms are provided for extracting segments of ?les and concatenating ?les.
By setting the appropriate con?guration variables, all input ?les can be converted to parametric
form as they are read~in. Thus, simply copying each ?le in this manner performs the required
encoding. The tool HLIST can he used to check the contents of any speech ?le and since it can also
convert input on-the?y, it can he used to check the results of any conversions before processing
large quantities of data. Transcriptions will also need preparing. Typically the labels used in the
original source transcriptions will not be exactly as required, for example, because of di?erences in
the phone sets used. Also, HMM training might require the labels to be contextadependent. The
tool HLED is a scriptadriven label editor which is designed to make the required transformations
to label ?les. HLED can also output ?les to a single Master Label File MLF which is usually
more convenient for subsequent processing. Finally on data preparation, HLSTATS can gather and
display statistics on label ?les and where required, HQUANT can be used to build a VQ codebook
in preparation for building discrete probability HMM system.

Avemges are too colourless, mdeed too abstmct to may may,
to represent conmte ezpenemle.
?Frederic Manning, The Middle Parts of Fortune

As we discussed in Chapters 1 and 2, compared to mid?air collision avoidance sys
terns based on other technologies such as radar. vision?based mid?air collision avoidance
systems are likely to be smaller, lighter. cheaper, and more power e?icient. Whilst there
has been recent success in designing and testing vision?based aircraft detection systems
in [15,16,53757], vision?based aircraft manoeuvre detection during potential collision
scenarios is yet to be investigated. In this chapter, we investigate the use of parameter
estimation and quickest change detection techniques (similar to those we have developed

in Chapters 3, 4, and 5) in the application of vision?based aircraft manoeuvre detection.

The main contribution of this chapter is:

> The proposal and evaluation of adaptive and rohustnessinspired algorithms for

quickly detecting aircraft manoeuvres in video sequences.

Importantly, the adaptive aircraft manoeuvre detectors of this chapter are inspired by
generalised likelihood ratio (GLR) rules for quickest change detection (and the parameter

Preliminary versions of some results presented in this chapter are published in [C1] and [02].

 

Knowledge would be fatal, rt 15 aneemnnty that charms one.

A mist makes thtngs beautiful.
?Oscar Wild, The Prctmr of Damn Gmy

In this chapter, we build upon the independent and identically distributed (i.i.d.)
process results of Chapter 4 to investigate the problem of quickly detecting an unknown
abrupt change in a general dependent (noniid) stochastic process. The key contribu?

tions of this chapter are:

> The proposal and solution of Lorden, Pollak, and Bayesian asymptotic minimair ro
bust quickest change detection problems with polynomial delay penalties in general
dependent processes with unknown postrchzmge conditional density parameters;

and

> The derivation of new asymptotic bounds on the Lorden, Pollak and Bayesian
polynomial delay costs of asymptotically minimax robust rules compared to asyrnp

totically optimal rules.

Importantly, we identify asymptotic solutions to our minimax robust problems using

The results presented in this chapter appear in the submitted journal paper [JQ].

 

Such low degradation is made possible due to the large number of features extracted and
me suitable normalization method proposed in this paper.

Sellondmutualplucement
We repeat the previous experiments on the other two scenarios described in ?REALDs
ISP dataset": sel?placement and mutual?placement. The results are presenmd in Table 5.
In me ?rst two columns, this table shows ?te type of data (or scenario where the data
were recorded) used for training and testing the proposed system.

Regarding the sel?placement scenario, this table shows 8.696 accuracy drop compared
to the ideal?placement in the original paper (from 97.0 to 88.496), but with our system,
this reduction is lower than 0.596 (from 99.1 to 98.996). When comparing these results to
the original paper, there is a big improvement (more man 1096, from 88.4 to 98.996) in the
sel?placement scenario, The new feature extraction module shows a very good robusts
ness against different sensor placements.

For me mutual?placement scenario, the results are considerably low in both works
(baseline and this paper) but the degradation obtained with the system proposed in this
paper is considerably smaller compared to the baseline system: our system shows a bet?
mt robustness. This degradation is ditferent depending on the number of mis?displaces
ments (4, 5, 6, and 7).

In these experiments, we have used the data recorded in the mutual scenario for train?
ing and testing me system, As we commented in ?REALDISP dataset?; only three out of
the 17 volunteers were recorded for mutual?displacemem scenario so the amount of dam

Table 5 Flnal experimental result for sell-placement and mutual-placement scenarios

 

Train set Test set Baseline {24h This paper:
Evaluation method: randoms Evaluation method:
partitioning subjxrwiw
Nullsamvily: truncated Nullsattivity- included

Accuracy % Frmeasure Accuracy % Fsmeasure

 

{WV/UV " call Julran "
?>
" dial 332654 "

W

 

 

 

 

This ?nal chapter of the tutorial part of the book will describe the construction of a recogniser
for simple voice dialling apphcatious. This recogniser will be designed to recognise continuously
spoken digit strings and a hrnited set of names. It is sub?Word based so that adding a new name to
the vocabulary involves only modification to the pronouncing dictionary and task grammar. The
HMMs will be continuous density mixture Gaussian tied?state triphones with clustering performed
using phonetic decision trees. Although the voice dialling task itself is quite simple, the system
design is general?pulpose and would be useful for a range of apphcations.

The system will be built from scratch even to the extent of recording training and test data
using the HTK tool HSLAB. To make this tractable, the system will be speaker dependenl?, but
the same design would be followed to build a speaker independent system. The only ditference being
that data would be required from a large number of speakers and there would be a consequential
increase in model complexity.

Building a speech recogniser hem scratch involves a number of interrelated subtasks and ped?
agogicaily it is not obvious what the best order is to present them. In the presentation here, the
ordering is chronological so that in oifeet the text provides a recipe that could be followed to con?
struct a similar system. The entire process is described in considerable detail in order give a clear
view of the range of functions that HTK addresses and thereby to motivate the rest of the book.

The HTK software distribution also contains an example of constructing a recognition system
for the 1000 word ARPA Naval Resource Management Task. This is contained in the directory
llMl-I'X'K of the HTK distribution. Further demonstration of HTK?s capahihties can be found in the
directory HTKDemo. Some example scripts that may be of assistance during the tutorial are available
in the iiTKTutorial directory.

At each step of the tutorial presented in this chapter, the user is advised to thoroughly read
the entire section before executing the commands, and also to consult the reference section for
each HTK tool being introduced (chapter 17). so that all command line options and arguments are
clearly understood.

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

5.2.2. Data Fusion

In general, the classification of fused data can yield better results than the classification over
single data sources [59]. A promising direction of investigation is given by the nexus between energy
consumption, water consumption, and human presence in a house (also gas metering could be an
additional data source). In an extreme example, a 50% classification between laundry and gardening
could be better disambiguated by the analysis of instant energy consumption given that one of the
two activities uses energy and water at the same time. An example of the nexus between energy and
water is presented in Reference {60} In that paper, the authors leverage electricity non-intrusive load
monitoring (NILM) to acquire water disaggregation as a set of water/ energy correlated states.

5.2.3. Working at Scale

Applying standard rates derived from sample studies is misleading because of the high variability
in water use from one customer to another, even among customers with a similar infrastructure and
social-economic pro?le. The model extracted from a single house?s data is limited, and does not
leverage the information hidden in the broader population. What we consider parameters for a single
house (pipe size, extension of parcel, number of rooms, habits of tenants, ?le.) could be considered
as independent variables in a broader model, comprising a full set of properties in a city or region.
The collection of massive datasets for an entire city or region is, nowadays, technically feasible and
possible to maintain in the long ternL Hence, an interesting research question is to build and evaluate
large-scale models.

6. Conclusions

Non-intrusive water disaggregation is a valuable approach for estimating fixture-specific water
consumption, while keeping installation costs affordable, and, at the same time, the underlying
complexity of processing remains manageable. We have presented a review of water disaggregation
methods that make use of either mono?modal sensing or multi?modal sensing (e.g., combining different
variables, such as water flow, pressure, etc). The result of our review can be summarized in the
following conclusions:

5. Discussion on Issues and Future Challenges

Water usage disaggregation is the equivalent of non-intrusive electricity load monitoring, applied
in the water domain, but with an important difference: While electricity outlets can be monitored with
non-invasive, out-of-the-box meters, water fixtures are, in general, unpowered and more difficult to
wire to a data communication infrastructure. This entails battery-operated instrumentation and, in
turn, constrained communication capabilities. Moreover, when dealing with supervised classifiers, a
necessary step is fitting the model with labelled data In the case of water, this may require special
purpose sensors, plumbing, and battery-operated equipment to be installed. Unfortunately, in real
houses, it is not viable to install a flow switch in every fixture or a Closed Circuit Television (CCTV)
camera in every room just to fit the classification model because plumbing is expensive and invasive.
Any viable approach should then comply with the principle of minimal installation requirements, and,
further, any sensors or equipment installed should already be an off-the-shelf product with a high
degree of acceptance among the general public. In summary, we can identify three requirements for
instrumenting a house with sensors:

. High acceptance (design, shape, part of shopping trends, identification of a user need)
. Low cost to buy and install
. Minimal or zero maintenance

All the works described in this survey challenges the previous state-of?the-art against classification
accuracy and are thus built on some hi-tech lab-level setup that requires continuous manual
intervention to ensure a reliable collection and processing of water data and ground truth.
To summarize:

Flow traces analysis [34] requires a data logger to be installed and then data should be manually
collected every 14 days, the data collected are then manually analysed and added to a database. It
seems a feasible solution to analyse a given period of time, but is not practical to perform online
disaggregation. HydroSense [38] reaches an accuracy of 8 ?u, but needs at least two days of ground
truth collectiono Their current approach trains the language model using data from the home where it

unnecessary amount of radio pollution, which is, in turn, is blamed as a potential cancer cause. The
second one is with respect to violations of citizens? rights. Detractors accuse governments of being
driven by the interest of suppliers and that the smart metering roadmap has been laid out without
public consultation, in violation of the spirit of shared consensus and democracy [54]. The last concern,
and probably the one with a proven impact, is that of privacy. There are many examples of how
high-resolution metering could be used to identify personal habits and retrieve personal information.
Notable proof of this concept is shown in Reference [55], where TV programs actually watched by
home occupants is inferred by correlating features such as the luminosity of scenes to high-resolution
energy consumption data. An approach to increase acceptance of industrial-level smart and cognitive
meters a viable solution is twofold:

- Give control to end users (they must be able to switch on/ off the metering; to set up the resolution;
to control the amount of radio messaging inside the property, etc.)

. Locally process most of the data and locally reveal the insights needed by end users to monitor
and improve their water demand. Powerful insight, such as usage disaggregation, could occur
in-home rather than being inferred remotely. This allows to send, to the supplier, only the
strictly-necessary data for operation (for instance daily average consumption over a week).

However, the above-mentioned approach does not take into full account the detailed needs of
water suppliers, as the focus is mainly on user privacy. Thus, as explained in Reference [56], developing
a context-specific framework for assessing how the collection and processing of detailed water usage
impacts the users privacy, and identifying a set of best practices to mitigate the impact is of paramount
importance. We expect that this issue will be addressed as soon as smart metering and cognitive
metering become ubiquitously available for the adaptive management of urban water resources.

52, A Few Promising Research Directions towards Real World Adoption

In this section, some promising directions of further investigation are described. The general
rationale is not to encourage competition in classification techniques to achieve 100% accuracy, but
rather to bridge the gaps for water disaggregation to become a viable tool in real world environments.

samples the ?ow meter at a frequency of 2 Hz and draws presence events from PIR once every 7 5 if a
presence is detected. It employs a 3 tier unsupervised inference algorithm, where tier 1 detects water
?ow events based on edge detection of the ?ow meter signal, tier 2 performs clustering based on rooms
using a Bayes Network clustering approach, and tier 3 performs fixture determination based on event
duration and frequency. Fixture disaggregation is limited to sinks and toilets. It shows an accuracy of
86% for fixture event classification, and 80"/ir90% accuracy for individual fixtures. Unlike all previous
approaches, WaterSense is unsupervised and does not require any training data. However, it has
difficulties in classifying overlapping usage events if the same fixture type is used in different rooms
simultaneously. It also cannot discriminate between multiple fixtures of same type in the same room.
Unlike all of the previously-discussed work, Raniian ct HI, were the first to consider disaggregating
water use, not on the fixture level, but to attribute water use to individual users in a household [32]. In
their research, the authors explored whether room-level tracking of individuals is accurate enough for
user attribution of fixture use of both for water fixtures and energy. They instrumented a test home
with under?oor Radio Frequency Identification (RFID) readers embedded in each doorway for house?
and room-level tracking, and 15 RFID readers at individual fixtures for high accuracy, which were able
to track individuals that wore an RFID ankle bracelet. Fixture level use was directly inferred by ?ow
meters attached to each individual fixture. The home examined different house, room, and fixture
level tracking, and assessed the use of simple heuristics based on people history of fixture usage to
resolve ambiguous situations. Performance of house-level tracking could be improved to 60% with
heuristics, room-level up to 87?70, and coordinate-level up to 97?u. The work showed that room-level
tracking of users in homes can provide a good accuracy for user attribution of water usage activities in
the home. However, the approach using RFID tracking with ankle tags and under?oor readers is very
intrusive for everyday deployment contexts.

4.5. Summary aprpmachcs

To give a picture of the pros and cons for the different approaches, and starting from a
subset of selected works listed in the previous sections, Table 3 reports a summary in terms of
output/ performances, resilience and installation.

itallrvaluer imitate the best remit; in this expevlment

columns), ln conclusion, the proposed sysmm is also a competitive solution for home
care monitoring applications,

Conclusions
This paper has proposed a HAR system for classifying 33 di?'erent physical activities
composed of two main modules: feature extraction and activity recognition modules.

The first contribution has been an analysis of several feature extraction strategies:
timerbased and frequencysbased. The timerbased features have provided betmr results
compared to the frequencyrbased ones, This paper has also evaluated several normalis
zation methods for reducing the degradation produced when training and testing with
different users. Thanks to the new feature extraction module and the normalization
strategy, the system has shown strong robustness when facing me Nullractivity and dif
ferent placement scenarios, two vital aspects for real applications.

Regarding the type ofsensor, the magnetometer signals have provided better discrimir
nation capability, The best results have been obtained when combining the information
from all the sensors. ln this case, the improvement is significant. The main experiments
have been done on a public available dataset, REALDISP Activity Recognition dataset.
Final results have exhibited that the proposed system largely improves me performance
compared to previous works on the same damset [24]. Under the best configuration, the
accuracy reaches 99.196 and Frmeasule 0.991.

The proposed system luas been also evaluated with another public dataset (OPPORr
TUNITY dataset) demonstrating competitive results (compared to previous work [31])
in two main tasks for home care monitoring: highrlevel locomotion and midrlevel gess
ture classi?catlon,

 

OPPORTUNITY dataset

The OPPORTUNITY dataset contains data from four subjects, performing six di??erent
runs each of: ADLlfADLS and Drill. In the Drill run, subject must act in a predeterr
mined activity sequence and, as for ADLlfADLS, there is no restriction on the order
and number of activities, For each subject, there is information from three types of
sensors: bodyrworn sensors, object sensors and ambient sensors The onebody sensors
include 7 multiesensor inertial measurement units with another 12 3D acceleration sen?
sors: 145 signals in total, Since only body?worn sensors are concerned in the evaluation
section of the original paper [31], the data from object and ambient sensors are trun?
cated in the following experiments. In terms of activities or classes, this damsel has 3
ditterent sets: 4 types of locomotion (highrlevel activities); 17 types of gesture (midelevel
actions); and lowelevel actions to objects (which is ignored in this work),

Experiments on the OPPORTUNITY dataset

We retrain and evaluate our system using the same experimental setting as in the origi?
nal paper [31]: using ADLz and ADL3 from one subject as the testing set and use Drill,
ADLI, ADL4 and ADLs from the same subject as the training set. We conduct experi?
ments in this con?guration for all four subjects and in the two tasks: highelevel locos
motion (Table 7) and midelevel gestures (Table 8). The ?rst column shows the different
proposed systems, and the best systems are remarked with bold font.

For me highelevel locomotion task (Table 7), ?le system proposed in this paper obtains
?ue best results for all subjects when the Null class is not considered (the 4 last columns).
When including the Null class (the 4 ?rst columns), we obtain the best results for all
subjects except 83.

For the mid?level gesture task (Table 3), the system proposed in this paper obtains
the best results for all subiects except 54 when the Null class is included (the 4 ?rst

Table 7 Experimental results on the OPPORTUNITY dataset (hlgh-level locomntlnn classl-
?cauon)

for training the system is very small (2 out of the 3 subjects recorded in this scenario).
In order to analyze the in?uence of the amount of data, we repeat the same experiments
but using the ideal?p|acement data for training the system. Although there is a mismatch
in the conditions, the amount of available data for training would increase a lot (from 2
to 16 subjects). The experiments are shown in Table 6. In the ?Train Set" and ?Test Set"
columns, we have also included the number of subjects considered for training and mstr
ing the system.

The results show that when being tmined with ideal datasets and tested with mutual
damsets, the system reaches a very good accuracy though the training and testing sets
come from diti'erent placement scenarios. For example, for mutual4, the accuracy goes
from 87.9 to 99.096 (the ?rst row). These results support the hypothesis that the amount
of data for training is an important factor in the system performance.

with the idea of cross?dataset experiment, we go further on me idealrplacement and
sel?placement scenarios (the last row in Table 6). As Table 6 shows, there is not a signi??
cant di?'erence on the accuracy when testing with sel?placement dataset and training
with ideal or self placement (99.1 vs. 98,936, di?'erence lower than the con?dence interval
0.596). In this case, the amount of data available in ideal?placement and sel?placement
scenarios is me same.

System analysis in a new domain: home care monitoring

In the introduction, we commented two main applications of HAR: physical exercise
monitoring and home care monitoring. The REALDISP dataset is focused on the ?rst
one: physical exercise monitoring, In order to verify the viability of the proposed system
in a home care monitoring application, we have evaluated the best system con?guration
with another dataset: the OPPORTUNITY dataset for HAR from wearable, object, and
ambient sensors [30]. The recordings include daily morning activities: getting up from
the bed, preparing and having breakfast (a coffee and a salami sandwich) and clean?
ing the kitchen latter. This dataset is a very popular l-[AR dataset on thls research field.
There is no constraining on the location or body posture in any ofthe scripted activities.

event and the speed that the valve is opened or closed, but the shape of the signature does not change.
Non~intrusive monitoring techniques based on ?ow meters face the following challenges:

. Overlapping of flow valve events: The magnitude and shape of the transients are altered by
the overlapping water use events. This has an impact on the ability to perform accurate event
segmentation, especially for events that occur very close together.

. Generalisation: Due to the dependency of the sensing system on the piping infrastructure
topology, sensor placement and fixture types, a calibrations phase for each valve during
installation may be needed. This makes the deployment of the solution more dif?cult without
auto-calibration methods.

. Accuracy of ?ow estimation: The amount of water flow cannot be directly determined, and
requires and estimation to be performed based on changes to the pressure. For this to work,
additional calibration steps are required to approximate the behaviour of the piping infrastructure.

HydroSense [38] is the first approach to propose non-intrusive water use disaggregation based on
pressure sensors. HydroSense requires a pressure sensor to be installed on an available water hose bib,
utility sink faucet, or water heater drain valve. in their work, Froehlich et al. collected samples of valve
open and close events of all fixtures in the home at a 1 kHz sampling frequency with a pressure sensor
from 10 test homes, in order to extract signatures for these water usage events. Based on the collected
data set, they developed an approach that allows the classification of fixture open and close events in
a three-step approach: <1) valve event segmentation is based on a FIR low pass filter over a 1-5 time
window and determined based on a threshold over the derivate of the ?ltered signal; (2) valve event
segmentation determines valve open and valve close events using a hierarchical classi?er; (3) fixture
classification, which maps valve open/close events to an individual ?xture with a template based
hierarchical classifier with different distance metrics. Flow estimation rs based on an equation that can
approximate the flow with a change in pressure by measuring the difference between the pressures at
the onset of a detected valve open event to the stabilized pressure at the end of the segmented valve
open pressure wave impulse. In their experimental setup, they achieved 07.970 aggregate accuracy for
identification of individual fixtures, and ?ow rate estimation errors between 5% and 22%. Their work
only performed offline classification of isolated fixture usage events and considered valves thatwere
fully opened/closed.

In later work [37], HydroSense is further extended by Larson rt al. to be able to, not only to perform
valve fixture level classification, but also determine the valve at a fixture responsible for the water

Their algorithm used ground truth labels for event segmentation, and focused on a probabilistic
approach for valve event classi?cation using Bayesian estimation which is an approach that is inspired
by the dynamic Bayesian models used in speech recognition (where instead of recognizing words,
valve events are recognised). It consists of the following parts: (1) template matching using similarity
matching algorithms; (2) a language model to determine likelihood of a sequence of valve open/ close
events to identify event pairs,- (3) extract features from paired tuples and compare them to a probability
distribution; and (4) combine probabilities to select the most likely sequence. Using a single pressure
sensor per home, their algorithm was able to disaggregate valve, ?xture, and ?xture type at 70%,
90% and 9m percent accuracy with a single sensor, which rose to 82%, 93% and 97% if a second
sensor was deployed on the hot water piping infrastructure. The algorithm also showed acceptable
performance in the presence of two overlapping usage events. To be practically deployed and usable,
the algorithm requires some staged training data for each fixture and automatic segmentation of
events. While previous work in HydroSense showed that automatic segmentation was possible with
non-overlapping events, classification accuracy is likely to be worse due to segmentation errors in
overlapping situations.

4.3. Acoustic Based Methods

Acoustic event detection provides another alternative for disaggregating water use in a residential
environment (Figure 3). The assumption of such methods is thatwater usage events can be derived
from audio signals captured by microphones that are placed at strategic locations along the water
piping infrastructure of a home.

An approach based on acoustic sensing is presented by Fogarty ct III. [30]. Four acoustic sensing
units are placed in the basement of the home, one on the cold water pipe, one on the hot water
pipe from the heater, and two sensors on the waste water pipes. The sensing units were Mote class
devices equipped with microphones and performed intermittent high frequency audio sampling
(1000 samples in 0.25 s, every 2 s). From these 1000 sample windows, features are extracted in the form
of zero?crossing rate and the root mean square, These features are fed into a hand-crafted hierarchical
classifier that exploits knowledge of activity patterns and interdependencies across the two supply
and two drainage pipes for classifying water usage activities into the categories ofwashing machine,
dishwasher, shower, toilet, kitchen sink, and bathroom sink use. While not able to determine the water
volume or duration of the water usage events, the proposed classifrer was able to determine the usage
of a particular fixture with an accuracy between momma, depending in the fixture type. As only
performance data for isolated usage activities were presented, it is unclear how the classifier performs

activity can be characterized by de?ning the identity of sounds and their position in
time sequence [15]. The main two disadvantages of ambient sensors are the require?
ment of infrastructure (for example, installation of video cameras in the monitoring
areas) and also, people do not always stay all their time in the same environment.
These limitations can be overtaken by embody sensors [16, 17]. Body?worn sensors
add new possibilities to the human monitoring system [18]: they allow measuring
body signals (e.g. physiological, motion, location) and they are portable, allowing user
supervision at any location without the need of ?xed infrastructure. Because of these
bene?ts, several works have been developed using motion sensors in different body
parts (e.g. waist, wrist, chest and thighs) and achieved good classi?cation performance
[19722].

This work has been carried out using a public dataset: REALDISP Activity Recogni?
tion dataset. This dataset contains recordings from 17 subjects performing 33 different
gymnastic activities. This dataset has permitted several HAR works focused on different
aspects. One interesting aspect has been the degradation suffered on the HAR accuracy
depending on the sensor placement or the number of displacements (wrong placements)
[23?25]. Other analyzed aspects have been the window size [24, 26], and the detection

 

of acti ties transitions [27]. This paper contributes by analyzing several strategies for
feature extraction and proposing several normalization approaches for dealing with the
inter?user variability. As far as the authors know, this work reports the best HAR results

using this dataset.

System architecture
The proposed system architecture is shown in Fig. 1. It consists of two main modules:
feature extraction and machine learning algorithm for activity classi?cation. The inertial

The performance of Hidden Markov Models (HMMs) in real-world applications are often degraded because they face com?
plex environments that change during operations, and because they are designed a priori using limited training data and
prior knowledge. To sustain a high level of performance, a HMM should be capable of ef?ciently adapting its parameters,
in response to new data observations from the environment, through incremental learning. incremental learning allows
to update HMM parameters from new data without accessing the previously-learned data, however corrupting previ-
ously-acquired knowledge remains an issue. Standard techniques for estimating HMM parameters involve batch learning,
which assume a ?nite amount of training data available throughout the training process. ?lhe HMM parameters are estimated
over several training iterations, where each iteration requires processing the entire training data, until some objective func-
tion is maximized. To avoid knowledge corruption, learning new data with these techniques would require storing cumula-
tive data in memory and training from the start using all this data. '1 his may represent a very costly solution.

?lhis paper present a survey of techniques found in literature that are suitable for incremental learning of HMM param-
eters. These include on-line learning algorithms that have been initially proposed for learning from a long training sequence,
or block of sub?sequences. in this paper, these techniques are categorized according to the objective functions, optimization
techniques and target application. Some techniques are designed to update HMM parameters upon receiving a new symbol
(symbol-wise), while others update the parameters upon receiving a sequence (block-wise). Convergence properties of these
techniques are presented, along with an analysis of their time and memory complexity. In addition, the challenges faced
when these techniques are applied to incremental learning is assessed for scenarios in which the new training data is limited
and abundant.

When the new training data is abundant (scenario 1), the HMM parameters should be re-estimated to ?t the source gen-
erating the data through one pass over a sequence of observations. When new data corresponds to a long sequence or block
of sub-sequences generated from a stationary source (static environment), these techniques must employ a specialized strat-
egy to manage the learning rate such that new data and existing knowledge are integrated without compromising the HMM
performance. ?1 his includes resetting a monotonically decreasing learning rate when provided with new data or employing
an auto?adaptive learning rate. Rapid convergence with limited resource requirements are other major factors in such cases.
However few learning algorithms have been provided with a proof of convergence or other relevant statistical properties.
Another important issue is the ability to operate in dynamically-changing environments. in such cases, some novelty criteria
on the new data should be employed to detect changes and trigger adaptation by resting a monotonically decreasing learn-
ing rate or ?ne?tuning an auto-adaptive learning rate.

When the new training data is limited (scenario 2), HMM parameters should be re-estimated over several iterations due
to the limited view of phenomena. Therefore, HMM parameters should be able to escape local rnaxirna of the cost function
associated with the new data. Given the limited data, early stopping criteria through hold-out or cross-validation must be
considered when learning the new block of data to reduce the effects of over?tting. Accumulating and updating represen-
tative validation data set over time must be considered. However, this requires investigating some selection criteria for
maintaining the most informative sequences of observations and discarding less relevant ones. Another major challenge re-
sides in adapting the current operational HMM to the newly-acquired data without corrupting existing knowledge. One pos-
sible solution involves using an adaptive learning rate which, at each iteration, controls the weight given to the current HMM
with reference to the new information. ?lhis learning rate should preferably be inferred from the data.

algorithm is likely to perform well on water use patterns of household appliances, it is unclear
how well it will do on more irregular water usage patterns, such as tap use or showers.

4.1.2. Generative Classi?ers

The work of Fontdecaba rt ?11. [36] assumes flow meter data at a rate of a reading every 5 s. it
considers a common generative model for all households, which models water usage classes (toilets,
washing machine, kitchen sinks, bathroom sink, dish washer, shower) as probabilistic models with
multivariate Gaussian distribution. A maximum likelihood estimator is utilised to select the right water
usage class based on 10 indicators derived from the flow meter data. The algorithm achieved an overall
classi?cation accuracy of 70% for water usage classes and 689i. for water volume, considering sample
data obtained from eight households over a period of three months. The algorithm was assed only for
non-overlapping usage events and had dif?culties in accurately classifying water using appliances,
such washing machines and dishwashers.

in Reference [48], Nguyen rt 11]. investigated the use of a Hidden Markov Model (HMM) based
classi?er for water end-use event classi?cation They found that HMM alone did not provide suf?cient
classification accuracy and added extensive context information to ?ne?tune the classifiers, based
on time of day, likelihood of occurrences of events, and assumptions of event durations and ?ow
boundaries and volumes per event. The resulting approach was a hybrid analytical method employing
an HMM with over 100 states with a Dynamic Time Warping algorithm and event probability
techniques, resulting in a multilayer classi?er. The classifier was able to disaggregate water usage
events for tap, dishwasher, washing machine, shower, bathtub, toilet and irrigation. The classi?cation
for most events was nearly 90?/? for non-overlapping events, apart from irrigation and bathtub, which
the algorithm had difficulties recognising accurately. A bathtub was often confused with a long shower;
likewise, irrigation difficulties were due to irregular patterns.

The same team performed analysis of overlapping water events. In order to deal with concurrent
events, they proposed a new filtering method [52], which smoothens a combined event to any desired
level based on examination of gradient change along the sample, in order to make different dissection
decisions. The filtering determines a base samples and subsamples. Both base event and subsamples
are classified by an HMM, based on their likelihood. Subsamples require an additional threshold or
are broken into further subsamples using the same ?ltering technique. The base event is classified
based on likelihood without a threshold. The evaluation looked only at the fairly small number of
20 combined events (between two and three concurrent occurring events) and was able to perform

that domestic water use exhibits common patients that are distinctive enough to discriminate water
usage events of different ?xture types. Through analysis of aggregate data flows captured by a single
flow meter by visual comparison with a database of water event signatures, or by the use of simple
decision-tree based classi?ers, the current water source for these water usage events can be determined.

A ?rstextensive study that utilised ?ow trace analysis was presented by DeOreo 8! III. [34]. The
authors performed a collection of signature ?ow traces for each ?xture inside of 16 homes at a rate
of one sample every 10 s using a flow meter. The signatures encompassed nine distinct example
categories and where stored into a database as reference signatures for later analysis. Then the 16
houses were monitored over a period of three weeks each Using the signatures, data-flow traces were
determined based on visual analysis. When a type of ?ow was identi?ed, it was isolated in a window
and the integral of the ?ow rate over this window provided the volume ofwater used for the event.
Overall, 10,000 water usage events were identified.

In order to simplify the analysis, a signal-processmg algorithm was devised that utilised different
feature sets derived from the flow meter measurements. The algorithm parameters were derived from
the labelled empirical database and included features, such as peak flow, duration, volume, ?ow rate
change over time and time of the day cues. The authors however did notprovide any assessment of
the performance of their solution.

The two market leading commercial tools, TraceWizard [46] and ldentiflow [47], are also based on
the principle of ?ow trace analysis. According to a previous review by Nguyen et a]. [48] for these two
systems, both use decision tree based classi?ers and require a time-consuming and labour-intensive
process to perform of?ine ?xture disaggregation.

TraceWizard is reported to apply an algorithm that interprets data based on simple boundary
conditions. Examples of these boundary conditions include start time, stop time, duration, volume,
peak ?ow rate, the most common flow rate, and how often this most common ?ow rate occurs during
the duration of the event. However, the performance drops very quickly to 24% when two water
?xtures are used at the same time or 0% when three or more were used. Similarly, ldentiflow has the
same de?ciencies. It uses a decision tree algorithm to deconstruct a ?ow trace data series into water end
use events and achieves an accuracy of 74.3% in terms of the correctly-classi?ed volume. As it relies on
?xed physical features of various water-using devices, such as volume and ?ow rate for disaggregation,
the ?nal classi?cation accuracy is greatly dependent on the existing types of water devices.

in recent work, Dong ct al. [491 propose a Deep Sparse Coding based Recursive Disaggregation
Model (DSCRDM), which is particularly suited for low sample rate water consumption disaggregation.
Their algorithm is inspired by work in the energy disaggregation domain on Discriminative

Object Detection from video

The motion of object can be detected after the object is detected from tideo. Tracking the activity
or object from sequence video frames this is the main goal of Video tracking Blob tracking.
kernel-based tracking. Contour tracking are sortie coininon tar-get representation and localization
algorithnis Ruolin zhang [11] has proposed adaptive background subtraction about the tideo
detecting and tracking moving object. He use median filter to achieve the background subtraction.
This algorithrn is used for both detecting and tracking nioting objects in sequence of video This
algorithm never support for nrultr feature based object detection. Hong er and Hong Sheng Li
[12] were ulu'oduced a new approach to detect and track the moving Object The defnre motion
model and the non-parameter distnbution model are utilized to represent the object and then the
motion region of the object is detected by background difference while Kalnian ?lter estimating
its affine motion m next frame The author shous Experimental results and proof the new method
can successfully track the object under such case as nierging. splitting. scale Variation and scene
noise. The author Bayan [13] talks about adaptive mean shift for autoruated rnulti tracking The
bene?t of Gaussian mixture model is that it extracted Foreground image from Video frame
sequence it also eliminate the shadow and noise from video sequence. It is helpful in initializing
the object trackers. As a result of this filter it reduces the search area and the number of iterations
to meet for the new location of the object. The advantage of Gaussian mixture model as it reduces
the background from tideo and hence ue can track the object easily. The object cart trap from
Video by changes in size and shape.

Section 6

RESULT OF ALL METHODS AND CONCLFSION
Recognition using the traditional HMM

Since there is no simple theoretically correct way of choosing the number of states. 5. it was
taned from three to ten in the experiineiit.[lo] We ?xed the number of symbols M at 111 based
on the simpli?ed quantization process and used the Forward algoi-itlnns to coinpute the various
likelihood functions The best classi?cation result of 87% is obtained when 5:3. as shown in
Figure 8. the plot of HMM recognition rate versus number of states. 5. This reveals that in the
HMM classi?er. the higher number of states does not necessarily imply better perfoiinance. On
the contrary a mere three-state model is sufficient to classify our selected luuuan activities. rising
two one-cliniensional sequential features derived from tracking the estimated head centroid (x-
and y?coordrnates). Hence. the three-slate ergodic topology is chosen for the conventional HMM.
HMM-NN hybrid and the NN-HMM hybrid classifiers. for easy comparison.

 

3. On-Iine leaming of HMM parameters

Several on?line learning techniques from the literature may be applied to incremental learning of HMM parameters from
new training sequences. l?g. 4 presents a taxonomy of techniques for on-line learning of HMM parameters, according to
objective function, optimization technique, and target application. As shown in the ?gure, they fall in the categories of stan?
dard numerical optimization, expectation?maximization and recursive estimation, with the objective of either maximizing
the likelihood estimation (MLE) criterion, minimizing the model divergence (MMD) of parameters penalized with the MLE,
or minimizing the output or state prediction error (MPL).

The target application implies a scenario for data organization and learning. Some techniques have been designed for
black-wise estimation of HMM parameters, while others for symbol-wise estimation of parameters. Block-wise techniques
are designed for scenarios in which training symbols are organized into a block ofsub-sequences and the HMM re-estimates
its parameters after observing each sub-sequence. in contrast, symbol-wise techniques, also known as recursive or sequen?
tial techniques, are designed for scenarios in which training symbols are observed one at a time. from a stream of symbols,
and the HMM parameters are re-estimated upon observing each new symbol. The rest of this section provides a survey of
techniques for orHine learning of HMM parameters shown in Hg. 4.

3.1. Minimum Model Divergence (MMD)

The objective function now consists of maximization of the log-likelihood as well as minimization of parameter diver-
gence using some entropy measures. A dual cost function that maximizes the log-likelihood while minimizing the diver-
gence of HMM parameters, using an exponentiated gradient optimization framework [54], was ?rst proposed for the
block-wise case [86]. then extended to symbol-wise case [41 1. Both block- and symbol-wise algorithms described below,
have been evaluated on speech data.

Block-wise Based on the exponentiated gradient framework [54], Singer and Warmuth [86] proposed an objective function
for discrete HMMs that minimizes the divergence between old and new HMM parameters penalized by the negative log-like-
lihood of each sequence multiplied by a ?xed positive learning rate (1, >0):

learning procedure to one pass. Therefore. the convergence properties of an on-line algorithm must be known to determine
the speed and limits of convergence behavior. As described in Section 4.1. when the true model is contained in the set of
solutions, some algorithms are shown to be consistent and asymptotically normal by properly choosing a monotonically
decreasing learning rate r]! and by applying l?olyak-Ruppert averaging [78]. For on-line learning in static environment,
decreasing step-sizes are essential conditions of convergence. Fixed step-sizes may yield the convergence of algorithms to
oscillate around their limiting values with variances proportional to the step-size. However, some novelty criteria on the
new data should be employed to reset the monotonically decreased learning rate when provided with a new sequence of
observations.

in contrast, when learning is performed in dynamically-changing environments, the notion of optimality is no longer va?
lid because the on?line algorithm should forget past knowledge and adapt to the newly acquired information. Detecting
slow, systematic or abrupt changes would require specialized novelty detection strategies [59], and this remains an open
issue that is outside the scope of this paper. Tracking non-stationary environments and handling slow drift is typically
achieved by choosing a ?xed learning rate V]. For abrupt drift however, a data driven learning rate is required to detect
changes and adapt the step-sizes according to the incoming data [85,90].

in both static and dynamically-changing environments, algorithms with low time and memory complexity are favored to
learn the long sequence ofobservations. As discussed in Section 4.2, symbol-wise algorithms that requires more than N2 time
complexity upon receiving each symbol are less attractive, especially when the number of HMM states N is large. For block?
wise algorithms, smaller window sizes W are favored for reducing both time and memory complexity. However, the stability
of algorithms must also be considered when selecting the window size.

Some issues that require further investigation when adapting the HMM parameters from abundant data. Empirical
benchmarking studies of these techniques could offer further insight on trade-offs for selecting algorithms and user-de?ned
parameters for an application domain. For instance, the performance of techniques based on MLE (such as EM and gradient
based) should be compared to those based on MMD, and recursive techniques for MPE. An interesting comparison would also
involve symbol-wise ver's us block-wise and ?ltering versus ?xed?lag smoothing. Signi?cant improvement, accuracy, speed
and stability of convergence, computing resources, and amount oftraining data required to reach or maintain a given level of
performance should be among the evaluation criteria. To this end, it is recommended to conduct non-parametric tests for
statistical comparisons of multiple classi?ers over multiple data sets [27,40]. in addition to assessing the strength and weak?
ness of each algorithm, such comparison may lead to ef?cient hybrid on-line algorithms that require fewer resources (see
Section 2.2.3).

5.2. Limited data scenario

Under the limited data scenario, it is assumed that a short sequence of observations becomes available to update the
HMM parameters, providing therefore a limited view ofphenornena. For?block-wise algor?ithmsthe sequence is typically seg-
mented into shorter sub-sequences rising a sliding window with a user-de?ned window size W. in contrast to the abundant
data case, when provided with limited data. several iterations over the new sequence or block of observations are required to
r'e-estimate HMM parameters. This local optimization raises additional challenges. Specialized strategies are needed for
managing the learning rate which, at each iteration must balance integration of pre-existing knowledge of the HMM and

4.1. Convergence properties

The extension of the batch EM to an incremental EM version, using a partial E-step followed by a direct update ofHMM
parameters in the M-step, is a well known technique that overcomes the resource requirements when processing a large
?xed-size data set [44]. Neal and Hinton [75] showed that this EM variant provides non-increasing divergence, and that local
minima in divergence are local maxima in the likelihood function. However, as argued by Gunawardana and Byrne [43], this
is insuf?cient to conclude that it converges to local maxima in the likelihood function. They showed that this incremental EM
variant is not in fact an EM procedure and hence the standard GEM convergence properties [99] do not apply. indeed their E-
step is modi?ed to use summary statistics from the posterior distributions of previous estimates. By using the Generalized
Alternating Minimization (CAM) procedure in an information geometric framework, Gunawardana and Byrne [43] proved
that (he incremental EM converges to stationary points in likelihood, although not monotonically.

The authors could ?nd no proof of convergence for on-line EM-based algorithms when HMM parameters estimation is
performed from in?nite amount of data, for both block-wise [28,73] and symbol-wise [91,89,36,74,?!] techniques. Further-
more, statistical analysis such as consistency and asymptotic normality are not provided. Although the rate of convergence
is not studied analytically. applying stochastic techniques in literature such as averaging [78] has been shown to help im-
prove convergence properties [15]. Among the block-wise algorithms, the gradient-based algorithm proposed by Cappe
et al. [16] should achieve the fastest convergence rate since it is based on a quasi-Newton method (second order approxi-
mation), while others should have a slower convergence rate. Similarly, for the recursive EM symbol-wise algorithms
[56,88] since the complete likelihood is optimized by a second order method, although no proofofconvergence is provided.

Among the methods based on RMLE, Ryden [83,84] provided convergence analysis for his block-wise RMLE. He proved the
consistency by using the classical result of Kushner and Clark [60]. independently, LeGland and Mevel [64,65] proved the
convergence and asymptotic normality of their symbol-wise RMLE, under the assumption that the transition probability
matrix is primitive, yet without any stationary assumption. ?ihis is accomplished by using the geometric ergodicity and
the exponential forgetting of the predictive ?lter and its gradient. Krishnamurthy and Yin [57] extended the RMLE results
of LeGland and Mevel [65] to auto-regressive models with Markov regime, and added results on convergence, rate of

 

 

 

 

 

 

Symborwiso, Block? 50 Symbnlrwisc Block?wise
Stream of Sub?sequences of iwo scqucnccs of 1' Two blocks of 1c
observations observations observation symbols subsequenses

 

 

 

 

 

 

 

 

 

] i i i

 
      

 

           

so so 200 400 600
N w (window size)

(0) ?1 1000,000 symbols (d) ?1 1000,000 symbols

800 1000

 

40

Fig. 6. An example of me time and memory compleer of a block-wrse [731 Vs. symbol?wrse [3s] algorrthm far leamrng an abservatron sequence of length
r= momma with an output alphabetof SIZE M = 50 symbols.

5. Guidelines for incremental learning of HMM parameters

The target application implies a scenario for data organization and learning. Depending on the application, incremental
learning may be performed on an abundant or limited alnount of new data. in addi on, this data may be organized into a
block of observation sub-sequences or into one long observation sequence. Within the incremental learning scenario
(Fig. 1), HMM parameters should be re-estimated from newly-acquired training data. However, the corruption of previously
acquired knowledge remains a key issue. in fact, none ofthe algorithms described in Section 4 can fully overcome this issue
during incremental learning. Several factors such as the choice of the learning rate and sub-sequence length may help
alleviating this issue. by optimizing HMM parameters such that contributions of new data and pl'e-existing knowledge is
balanced. in contrast, ensemble of classi?ers trained independently on new training data and combined with previously-
trained classi?ers may provide an alternate solution [47,53]. The rest of this section provides guidelines and underscores

challenges faced when applying on-line techniques to supervised incremental learning of new training data, when the data
is either abundant or limited.

5.1. Abundant darn scenario

Under the abundant data scenario, it is assumed that a long sequence of training observations becomes available to up-
date HMM parameters through incremental learning. A more complete view of phenomena is therefore presented to the
learning algorithm. As stated previously, symbol-wise algorithms can be directly elnployed to learn such sequences, one
observation at a time, while block-wise algorithms require buffering some amount of data using for instance a sliding win-
dow according to a user-de?ned buffer size W.

When learning is perfonned in a static environment from a large sequence of observations, one view of the data is typ-
ically suf?cient to capture the underlying structure by exploiting patterns redundancy. This is because the abundant data
provide a mor'e complete view of phenomena. Resource constraints would be another reason behind restricting the iterative

Although incremental learning techniques ofsingle HMM parameters are unable to overcome knowledge corruption, typ?
ical procedures for unbiased performance estimation may still help alleviating the corruption. ?I his involves stopping the
training iterations when the log-likelihood of an HMM on independent validation data no longer improves. Using hold?
out or cross-validation procedures reduces the effects of over?tting and improves generalization performance during oper?
ations. When the situation allows, some proportion of a new tr ning data should be dedicated to validation, providing a
good stopping criterion and improving the generalization capabilities of the classi?cation system. in order to perform such
validation, a set of observations must be stored and updated over time in a ?xed-sized buffer. Managing this validation set
over time depends on the application environment and should be selected and maintained according to some relevant selec?
tion criteria. For instance, in dynamically-changing environments, older validation data should be discarded and replaced
with new observations data that is more representative of the underlying data distribution. In static or cyclically-stationary
environments, older data should be preserved or rotated in a ?rst-in-?rst-out manner. An information theoretic measure
called surprise has been recently proposed to capture uncertainty of a new observation with respect to the current knowl?
edge of the learning system [70]. The concept of surprise could be applied for maintaining the validation set, since it can
classify new data into abnormal, learnable or redundant for the current system [70]

A potential advantage of applying the on-line learning techniques over batch lear ing techniques to limited data scenario
resides in their added stochasticity, which stems from the rapid re-estimation of HMM parameters. This may aid escaping
local maxirna during the early adaptation to newly provided data. Managing the internal learning rate, associated with each
sub-sequence or observation symbol, is therefore different for limited data than for abundant data. in general, employing a
?xed learning rate along with the validation strategies described above would maintaining the level ofperformance in both
static and dynamically-changing environments. However, more insights are required in this regard to determine if applying
a monotonically decreasing or auto-adaptive learning rates at the iteration, sequence, or symbol levels are bene?cial for
escaping local rnaxirna and hence improving the performance.

As presented in Section 4.2, the resource requirements for the on-line algorithms when learning from limited data are
comparable to that of when learning from abundant data. However, the time complexity presented for both block- and sym?
bol-wise techniques must be multiplied by the number of training iterations, which varies according to the algorithm em?
ployed, validation strategy and stopping criteria, and training data. Learning from limited data is less restrictive to memory
and time complexity in the abundant data. Therefore, even the most compute-intensive symbol-wise algorithms ? requiring
0(N?) time complexity upon receiving each symbol , or block-wise algorithms 7 requiring O(N?T) time and own ineinoiy
complexity upon receiving each sub-sequence ? can be afforded as long as they improve or maintain the HMM performance.
in limited data scenario, escaping local maxima, managing learning rates and stopping criteria may be more critical factors
for selecting an on-line algorithm than minimizing the resource requirements.

Finally. as for the abundant data scenario. applying the on-line learning techniques from this survey to adapt HMM
parameters to limited new data requires comparative benchmarking and statistical testing at the objective functions. opti?
mization techniques and algorithms (symbol-wise versus block-wise) levels, and according to various user-de?ned param?
eters (learning rate and window size). Emp' ical evaluations of these techniques across various applications, such as those

presented in [20], would provide useful insight

 

 

Feature extraction

In the feature extraction module, the sample sequences from the inertial signals are
grouped together in frames: ?xed?width sliding windows of 3 s and 6696 overlap (150
samples per frame with an overlap of 100 samples). For each frame, the system calcu?
lates a feature vector, which makes it easier for the machine learning module to learn the
internal characteristics behind raw signals. These features are traditional measures like
the mean, correlation, Signal Magnitude Area (SMA) and auto regression coef?cients,
but also, advanced ones that will be described below, These features are computed from
117 signals obtained from nine measurement units. Taking the three accelerometer
signals (X, Y, Z) as an example (similar signals are also considered for gyroscope, mag?
netometer and quaternion sensor): in the time domain, the signals considered in this
work are:

. XYZ (3 signals): Original accelerometer signals.
. Mag (1 signal): Magnitude signal computed from the previous three signals. This
magnitude is computed as the square root of the sum of squared components (accel?

erometer signals).
. Ierk?XYZ (3 signals): Jerk signals (derivative of the accelerometer signals) obtained

from the original accelerometer signals,
. IerkMag (1 signal): Magnitude signal computed from the previous jerk signals
(square root of the sum of squared components).

And in the frequency domain, the signals from the accelerometer sensor are:

fXYZ (3 signals): Fast Fourier transforms (FFTs) from XYZ.
Mag (1 signal): FFT from Mag.

flerk?XYZ (3 sign. 5
flerkMag(lsignal a ?? 74 L G) E?

